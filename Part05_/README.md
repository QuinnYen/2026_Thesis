# ğŸš€ BERTæ³¨æ„åŠ›æ©Ÿåˆ¶æƒ…æ„Ÿåˆ†æç³»çµ±

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Academic-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-ç ”ç©¶åŸå‹-orange.svg)]()

**è·¨é ˜åŸŸæƒ…æ„Ÿåˆ†æèˆ‡å¤šé‡æ³¨æ„åŠ›æ©Ÿåˆ¶ç ”ç©¶å¹³å°**

*åŸºæ–¼BERTçš„æƒ…æ„Ÿåˆ†æç³»çµ±ï¼Œæ•´åˆ5ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶èˆ‡4ç¨®é«˜æ€§èƒ½åˆ†é¡å™¨*

[å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹) â€¢ [åŠŸèƒ½ç‰¹è‰²](#åŠŸèƒ½ç‰¹è‰²) â€¢ [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹) â€¢ [å®‰è£æŒ‡å—](#å®‰è£æŒ‡å—) â€¢ [ä½¿ç”¨æ•™ç¨‹](#ä½¿ç”¨æ•™ç¨‹) â€¢ [ç ”ç©¶æˆæœ](#ç ”ç©¶æˆæœ)

</div>

---

## ğŸŒŸ å°ˆæ¡ˆæ¦‚è¿°

é€™æ˜¯ä¸€å€‹å°ˆç‚ºæƒ…æ„Ÿåˆ†æç ”ç©¶è¨­è¨ˆçš„å®Œæ•´å¹³å°ï¼Œå‰µæ–°æ€§åœ°å°‡**æ³¨æ„åŠ›æ©Ÿåˆ¶**å¼•å…¥æƒ…æ„Ÿé¢å‘å»ºæ¨¡ï¼Œå¯¦ç¾äº†å‚³çµ±æ–¹æ³•èˆ‡æ·±åº¦å­¸ç¿’æŠ€è¡“çš„å®Œç¾çµåˆã€‚ç³»çµ±æ”¯æ´å¤šé ˜åŸŸæ•¸æ“šï¼ˆé›»å½±ã€ç”¢å“ã€é¤å»³è©•è«–ï¼‰ï¼Œæä¾›ç›´è§€çš„GUIç•Œé¢å’Œå¼·å¤§çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

### ğŸ¯ æ ¸å¿ƒå‰µæ–°é»

- **ğŸ§  æ³¨æ„åŠ›æ©Ÿåˆ¶å‰µæ–°**: åœ¨é¢å‘å‘é‡è¨ˆç®—ä¸­å¼•å…¥5ç¨®ä¸åŒçš„æ³¨æ„åŠ›æ©Ÿåˆ¶
- **ğŸ”¬ ç³»çµ±æ€§æ¯”è¼ƒ**: é¦–æ¬¡ç³»çµ±æ€§æ¯”è¼ƒå–®ä¸€vsçµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœ
- **âš¡ é«˜æ€§èƒ½è¨ˆç®—**: æ•´åˆXGBoostã€é‚è¼¯è¿´æ­¸ç­‰4ç¨®åˆ†é¡å™¨ï¼Œæ”¯æ´GPUåŠ é€Ÿ
- **ğŸ¨ æ™ºèƒ½é©é…**: è‡ªå‹•ç’°å¢ƒæª¢æ¸¬ï¼Œå‹•æ…‹å„ªåŒ–æ€§èƒ½é…ç½®

---

## âœ¨ åŠŸèƒ½ç‰¹è‰²

### ğŸ§  å¤šé‡æ³¨æ„åŠ›æ©Ÿåˆ¶
| æ³¨æ„åŠ›é¡å‹ | æ ¸å¿ƒåŸç† | é©ç”¨å ´æ™¯ | è¨ˆç®—è¤‡é›œåº¦ |
|-----------|---------|----------|------------|
| **ç›¸ä¼¼åº¦æ³¨æ„åŠ›** | åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦æ¬Šé‡ | èªç¾©ç›¸é—œæ€§é‡è¦çš„ä»»å‹™ | ä¸­ç­‰ |
| **é—œéµè©æ³¨æ„åŠ›** | é å®šç¾©é—œéµè©å¼•å° | ç‰¹å®šè¡“èªæ•æ„Ÿçš„åˆ†æ | ä½ |
| **è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶** | ç¸®æ”¾é»ç©æ³¨æ„åŠ› | è¤‡é›œé—œä¿‚å»ºæ¨¡ | é«˜ |
| **çµ„åˆæ³¨æ„åŠ›** | å¤šæ©Ÿåˆ¶å‹•æ…‹åŠ æ¬Š | è¿½æ±‚æœ€ä½³æ•ˆæœ | æœ€é«˜ |
| **ç„¡æ³¨æ„åŠ›** | å‚³çµ±å¹³å‡æ–¹æ³• | åŸºç·šæ¯”è¼ƒ | æœ€ä½ |

### ğŸš€ é«˜æ€§èƒ½åˆ†é¡å™¨ç³»çµ±
| åˆ†é¡å™¨ | ç‰¹è‰² | GPUåŠ é€Ÿ | æ¨è–¦å ´æ™¯ |
|--------|------|---------|----------|
| **XGBoost** âš¡ | æœ€é«˜æº–ç¢ºç‡ | âœ… 8xåŠ é€Ÿ | å¤§æ•¸æ“šé›† |
| **é‚è¼¯è¿´æ­¸** ğŸš€ | é€Ÿåº¦æœ€å¿« | âŒ | ä¸­å°æ•¸æ“šé›† |
| **éš¨æ©Ÿæ£®æ—** ğŸŒ³ | ç©©å®šå¯é  | âŒ | å¯è§£é‡‹æ€§éœ€æ±‚ |
| **ç·šæ€§SVM** ğŸ“ | å°æ•¸æ“šå‹å¥½ | âŒ | ç·šæ€§å¯åˆ†å•é¡Œ |

### ğŸ–¥ï¸ æ™ºèƒ½GUIç³»çµ±
- **ä¸‰åˆ†é è¨­è¨ˆ**: æ•¸æ“šè™•ç† â†’ æ³¨æ„åŠ›æ¸¬è©¦ â†’ çµæœåˆ†æ
- **å¯¦æ™‚ç‹€æ…‹**: ğŸŸ  å¾…è™•ç† ğŸ”µ è™•ç†ä¸­ ğŸŸ¢ å®Œæˆ ğŸ”´ éŒ¯èª¤
- **ç’°å¢ƒæª¢æ¸¬**: è‡ªå‹•è­˜åˆ¥GPU/CPUç’°å¢ƒä¸¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
- **é€²åº¦è¿½è¹¤**: å¯¦æ™‚è¨ˆæ™‚çµ±è¨ˆèˆ‡æ€§èƒ½ç“¶é ¸åˆ†æ

---

## ğŸ“Š ç³»çµ±æ¶æ§‹

### ğŸ—ï¸ æ ¸å¿ƒæ¶æ§‹åœ–

```mermaid
graph TB
    A[æ–‡æœ¬è¼¸å…¥] --> B[æ–‡æœ¬é è™•ç†]
    B --> C[BERTç·¨ç¢¼å™¨]
    C --> D[æ³¨æ„åŠ›æ©Ÿåˆ¶å±¤]
    D --> E[åˆ†é¡å™¨æ¨¡çµ„]
    E --> F[çµæœåˆ†æ]
    
    D --> D1[ç›¸ä¼¼åº¦æ³¨æ„åŠ›]
    D --> D2[é—œéµè©æ³¨æ„åŠ›]
    D --> D3[è‡ªæ³¨æ„åŠ›]
    D --> D4[çµ„åˆæ³¨æ„åŠ›]
    
    E --> E1[XGBoost]
    E --> E2[é‚è¼¯è¿´æ­¸]
    E --> E3[éš¨æ©Ÿæ£®æ—]
    E --> E4[ç·šæ€§SVM]
```

### ğŸ“ å°ˆæ¡ˆçµæ§‹

```
Part05_/
â”œâ”€â”€ ğŸ“„ Part05_Main.py              # ä¸»ç¨‹å¼å…¥å£
â”œâ”€â”€ ğŸ“„ requirements.txt            # ä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ ğŸ“„ README.md                   # æœ¬æ–‡æª”
â”‚
â”œâ”€â”€ ğŸ“‚ modules/                    # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ ğŸ§  attention_mechanism.py  # æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ç¾
â”‚   â”œâ”€â”€ ğŸ“Š attention_analyzer.py   # æ³¨æ„åŠ›åˆ†æå™¨
â”‚   â”œâ”€â”€ âš™ï¸ attention_processor.py   # æ³¨æ„åŠ›è™•ç†å™¨
â”‚   â”œâ”€â”€ ğŸ¤– bert_encoder.py          # BERTç·¨ç¢¼å™¨
â”‚   â”œâ”€â”€ ğŸ¯ sentiment_classifier.py  # æƒ…æ„Ÿåˆ†é¡å™¨
â”‚   â”œâ”€â”€ ğŸ“ text_preprocessor.py     # æ–‡æœ¬é è™•ç†å™¨
â”‚   â”œâ”€â”€ ğŸ”§ run_manager.py           # é‹è¡Œç®¡ç†å™¨
â”‚   â”œâ”€â”€ ğŸš€ pipeline_processor.py    # æµæ°´ç·šè™•ç†å™¨
â”‚   â”œâ”€â”€ ğŸ“š text_encoders.py         # æ–‡æœ¬ç·¨ç¢¼å™¨
â”‚   â””â”€â”€ ğŸ² classification_methods.py # åˆ†é¡æ–¹æ³•åº«
â”‚
â”œâ”€â”€ ğŸ“‚ gui/                       # åœ–å½¢ç•Œé¢
â”‚   â”œâ”€â”€ ğŸ–¥ï¸ main_window.py          # ä¸»è¦–çª—ç•Œé¢
â”‚   â”œâ”€â”€ âš™ï¸ config.py               # GUIé…ç½®æª”æ¡ˆ
â”‚   â””â”€â”€ ğŸ”— progress_bridge.py      # é€²åº¦æ©‹æ¥å™¨
â”‚
â”œâ”€â”€ ğŸ“‚ utils/                     # å·¥å…·æ–‡ä»¶
â”‚   â””â”€â”€ ğŸ·ï¸ topic_labels.json       # ä¸»é¡Œæ¨™ç±¤é…ç½®
â”‚
â””â”€â”€ ğŸ“‚ output/                    # è¼¸å‡ºç›®éŒ„ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
    â””â”€â”€ run_YYYYMMDD_HHMMSS/      # æ™‚é–“æˆ³é‹è¡Œç›®éŒ„
        â”œâ”€â”€ 01_preprocessing/      # é è™•ç†çµæœ
        â”œâ”€â”€ 02_bert_encoding/      # BERTç·¨ç¢¼çµæœ
        â”œâ”€â”€ 03_attention_testing/  # æ³¨æ„åŠ›æ¸¬è©¦çµæœ
        â””â”€â”€ 04_analysis/           # åˆ†æçµæœ
```

---

## ğŸ› ï¸ å®‰è£æŒ‡å—

### ğŸ“‹ ç³»çµ±éœ€æ±‚

| é …ç›® | æœ€ä½è¦æ±‚ | æ¨è–¦é…ç½® |
|------|----------|----------|
| **Python** | 3.7+ | 3.8+ |
| **è¨˜æ†¶é«”** | 8GB RAM | 16GB RAM |
| **GPU** | å¯é¸ | 4GB+ VRAM |
| **ç£ç¢Ÿç©ºé–“** | 3GB | 5GB+ |

### âš¡ å¿«é€Ÿå®‰è£

#### 1ï¸âƒ£ åŸºæœ¬å®‰è£ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
```bash
# å…‹éš†å°ˆæ¡ˆ
git clone <repository-url>
cd Part05_

# å®‰è£æ ¸å¿ƒä¾è³´
pip install torch transformers scikit-learn xgboost numpy pandas nltk beautifulsoup4 tqdm joblib
```

#### 2ï¸âƒ£ å®Œæ•´å®‰è£ï¼ˆæ‰€æœ‰åŠŸèƒ½ï¼‰
```bash
# å®‰è£å®Œæ•´ä¾è³´
pip install -r requirements.txt

# ä¸‹è¼‰NLTKè³‡æº
python -c "import nltk; nltk.download(['punkt', 'stopwords', 'wordnet', 'omw-1.4'])"
```

#### 3ï¸âƒ£ GPUåŠ é€Ÿå®‰è£ï¼ˆæ¨è–¦ï¼‰
```bash
# å…ˆå®‰è£CUDAç‰ˆæœ¬çš„PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å†å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt
```

#### 4ï¸âƒ£ é©—è­‰å®‰è£
```bash
python -c "import torch, transformers, sklearn, xgboost; print('âœ… æ ¸å¿ƒå¥—ä»¶å®‰è£æˆåŠŸ')"
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ğŸ–¥ï¸ GUIæ¨¡å¼ï¼ˆæ¨è–¦æ–°æ‰‹ï¼‰

```bash
# å•Ÿå‹•åœ–å½¢ç•Œé¢
python Part05_Main.py
```

#### æ“ä½œæµç¨‹ï¼š
1. **ğŸ“ ç¬¬ä¸€åˆ†é  - æ•¸æ“šè™•ç†**
   - é¸æ“‡æ•¸æ“šé›†é¡å‹ï¼ˆIMDB/Amazon/Yelpï¼‰
   - å°å…¥æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txt, .csv, .jsonï¼‰
   - è¨­å®šæ•¸æ“šæŠ½æ¨£ï¼ˆå¤§æ•¸æ“šé›†å»ºè­°ï¼‰
   - åŸ·è¡Œæ–‡æœ¬é è™•ç†èˆ‡BERTç·¨ç¢¼

2. **ğŸ§  ç¬¬äºŒåˆ†é  - æ³¨æ„åŠ›æ¸¬è©¦**
   - é¸æ“‡åˆ†é¡å™¨é¡å‹ï¼ˆXGBoostæ¨è–¦ï¼‰
   - æŸ¥çœ‹ç’°å¢ƒä¿¡æ¯ï¼ˆGPU/CPUç‹€æ…‹ï¼‰
   - åŸ·è¡Œå–®ä¸€/çµ„åˆæ³¨æ„åŠ›å¯¦é©—
   - ç›£æ§å¯¦æ™‚è¨“ç·´é€²åº¦

3. **ğŸ“Š ç¬¬ä¸‰åˆ†é  - çµæœåˆ†æ**
   - æŸ¥çœ‹å¤šç¶­åº¦æ€§èƒ½æ¯”è¼ƒ
   - åˆ†æè©³ç´°åˆ†é¡çµæœ
   - å°å‡ºå®Œæ•´çµæœå ±å‘Š

### âŒ¨ï¸ å‘½ä»¤è¡Œæ¨¡å¼ï¼ˆé©åˆé€²éšç”¨æˆ¶ï¼‰

```bash
# å®Œæ•´åˆ†é¡è©•ä¼°ï¼ˆæ¨è–¦ï¼‰
python Part05_Main.py --classify your_data.csv --classifier xgboost

# åƒ…BERTç·¨ç¢¼è™•ç†
python Part05_Main.py --process

# åƒ…æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ
python Part05_Main.py --attention your_data.csv

# æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶
python Part05_Main.py --compare your_data.csv

# æŸ¥çœ‹è©³ç´°å¹«åŠ©
python Part05_Main.py --help
```

---

## ğŸ“Š ç ”ç©¶æˆæœ

### ğŸ† å¯¦é©—çµæœæ¦‚è¦½

#### æ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœæ¯”è¼ƒ

| æ³¨æ„åŠ›æ©Ÿåˆ¶ | å…§èšåº¦â†‘ | åˆ†é›¢åº¦â†‘ | ç¶œåˆå¾—åˆ†â†‘ | æ¨è–¦æŒ‡æ•¸ |
|-----------|---------|---------|-----------|----------|
| çµ„åˆæ³¨æ„åŠ› | **0.85** | **0.92** | **0.89** | â­â­â­â­â­ |
| è‡ªæ³¨æ„åŠ› | 0.78 | 0.85 | 0.82 | â­â­â­â­ |
| é—œéµè©æ³¨æ„åŠ› | 0.72 | 0.80 | 0.76 | â­â­â­ |
| ç›¸ä¼¼åº¦æ³¨æ„åŠ› | 0.70 | 0.78 | 0.74 | â­â­â­ |
| ç„¡æ³¨æ„åŠ›ï¼ˆåŸºç·šï¼‰ | 0.65 | 0.72 | 0.69 | â­â­ |

#### åˆ†é¡å™¨æ€§èƒ½æ¯”è¼ƒï¼ˆ50Kæ¸¬è©¦æ•¸æ“šï¼‰

| åˆ†é¡å™¨ | æº–ç¢ºç‡ | F1åˆ†æ•¸ | è¨“ç·´æ™‚é–“ | GPUåŠ é€Ÿ |
|--------|--------|--------|----------|---------|
| **XGBoost** | **95.4%** | **95.2%** | 1.5åˆ†é˜ | âœ… 8x |
| **ç·šæ€§SVM** | 94.5% | 94.2% | 20åˆ†é˜ | âŒ |
| **éš¨æ©Ÿæ£®æ—** | 92.6% | 92.3% | 5åˆ†é˜ | âŒ |
| **é‚è¼¯è¿´æ­¸** | 91.3% | 91.0% | 3åˆ†é˜ | âŒ |

### ğŸ¯ æ ¸å¿ƒç™¼ç¾

1. **ğŸ… çµ„åˆæ³¨æ„åŠ›è¡¨ç¾æœ€ä½³**: åœ¨å¤šæ•¸æ¸¬è©¦ä¸­ç²å¾—æœ€é«˜ç¶œåˆå¾—åˆ†
2. **âš¡ XGBoostæ€§åƒ¹æ¯”æœ€é«˜**: GPUåŠ é€Ÿä¸‹é€Ÿåº¦æå‡8å€ï¼Œæº–ç¢ºç‡æœ€ä½³
3. **ğŸ›ï¸ æ¬Šé‡é…ç½®æ•æ„Ÿ**: çµ„åˆæ³¨æ„åŠ›çš„æ¬Šé‡è¨­å®šå°æ•ˆæœæœ‰é¡¯è‘—å½±éŸ¿
4. **ğŸ“ˆ æ•¸æ“šé‡å½±éŸ¿é¸æ“‡**: å¤§æ•¸æ“šé›†å„ªé¸XGBoostï¼Œå°æ•¸æ“šé›†å¯é¸é‚è¼¯è¿´æ­¸

---

## ğŸ“š ä½¿ç”¨æ•™ç¨‹

### ğŸ¯ æƒ…æ„Ÿåˆ†æå°ˆæ¡ˆå®Œæ•´æµç¨‹

#### æ­¥é©Ÿ1: æ•¸æ“šæº–å‚™
```python
# æ”¯æ´çš„æ•¸æ“šæ ¼å¼ç¤ºä¾‹
# CSVæ ¼å¼
text,sentiment
"é€™å€‹ç”¢å“å¾ˆæ£’ï¼",positive
"è³ªé‡ä¸å¤ªå¥½",negative

# JSONæ ¼å¼  
[{"text": "é›»å½±å¾ˆç²¾å½©", "sentiment": "positive"}]

# TXTæ ¼å¼ï¼ˆæ¯è¡Œä¸€æ¢è©•è«–ï¼Œç”¨åˆ¶è¡¨ç¬¦åˆ†éš”æ–‡æœ¬å’Œæ¨™ç±¤ï¼‰
é€™å®¶é¤å»³å¾ˆæ£’	positive
æœå‹™æ…‹åº¦ä¸å¥½	negative
```

#### æ­¥é©Ÿ2: ç¨‹å¼åŒ–ä½¿ç”¨
```python
from modules.attention_processor import AttentionProcessor

# åˆå§‹åŒ–è™•ç†å™¨
processor = AttentionProcessor(output_dir='my_analysis')

# åŸ·è¡Œå®Œæ•´åˆ†æ
results = processor.process_with_attention(
    input_file='your_data.csv',
    attention_types=['similarity', 'keyword', 'self', 'combined'],
    classifier_type='xgboost'
)

# æŸ¥çœ‹çµæœ
print(f"æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {results['best_mechanism']}")
print(f"æœ€é«˜æº–ç¢ºç‡: {results['best_accuracy']:.3f}")
```

#### æ­¥é©Ÿ3: è‡ªå®šç¾©æ³¨æ„åŠ›æ©Ÿåˆ¶
```python
from modules.attention_mechanism import apply_attention_mechanism

# è‡ªå®šç¾©é—œéµè©æ³¨æ„åŠ›
custom_keywords = {
    'positive': ['å„ªç§€', 'æ£’', 'å¥½', 'excellent', 'great'],
    'negative': ['ç³Ÿç³•', 'å·®', 'å£', 'terrible', 'bad'],
    'neutral': ['é‚„å¯ä»¥', 'æ™®é€š', 'okay', 'average']
}

result = apply_attention_mechanism(
    attention_type='keyword',
    embeddings=bert_embeddings,
    metadata=data_metadata,
    topic_keywords=custom_keywords
)
```

### ğŸ”¬ é€²éšç ”ç©¶åŠŸèƒ½

#### æ³¨æ„åŠ›æ¬Šé‡è¦–è¦ºåŒ–
```python
from modules.attention_analyzer import AttentionAnalyzer

analyzer = AttentionAnalyzer()
attention_weights = analyzer.visualize_attention_weights(
    attention_result, 
    save_path='attention_heatmap.png'
)
```

#### æ‰¹é‡æ¯”è¼ƒå¯¦é©—
```python
from modules.attention_processor import AttentionProcessor

processor = AttentionProcessor()
comparison_results = processor.compare_attention_mechanisms(
    input_file='data.csv',
    attention_types=['no', 'similarity', 'keyword', 'self', 'combined'],
    classifiers=['xgboost', 'logistic_regression', 'random_forest']
)
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### â“ å¸¸è¦‹å•é¡Œ

<details>
<summary><b>ğŸ› ImportError: No module named 'xxx'</b></summary>

**è§£æ±ºæ–¹æ¡ˆ:**
```bash
# å‡ç´špip
python -m pip install --upgrade pip

# é‡æ–°å®‰è£ä¾è³´
pip install -r requirements.txt

# å¦‚æœç¶²è·¯å•é¡Œï¼Œä½¿ç”¨åœ‹å…§é¡åƒ
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/
```
</details>

<details>
<summary><b>ğŸ› CUDA out of memory</b></summary>

**è§£æ±ºæ–¹æ¡ˆ:**
```bash
# å¼·åˆ¶ä½¿ç”¨CPUæ¨¡å¼
export CUDA_VISIBLE_DEVICES=""

# æˆ–åœ¨GUIä¸­é¸æ“‡è¼ƒå°çš„æ‰¹æ¬¡å¤§å°
# æˆ–å•Ÿç”¨æ•¸æ“šæŠ½æ¨£åŠŸèƒ½
```
</details>

<details>
<summary><b>ğŸ› tkinter GUIç„¡æ³•å•Ÿå‹•</b></summary>

**è§£æ±ºæ–¹æ¡ˆ:**
```bash
# Ubuntu/Debian
sudo apt-get install python3-tk

# CentOS/RHEL
sudo yum install tkinter

# macOSï¼ˆé€šå¸¸å·²å…§å»ºï¼‰
# Windowsï¼ˆé€šå¸¸å·²å…§å»ºï¼‰
```
</details>

<details>
<summary><b>ğŸ› XGBoostå®‰è£å¤±æ•—</b></summary>

**è§£æ±ºæ–¹æ¡ˆ:**
```bash
# æ–¹æ³•1: ä½¿ç”¨conda
conda install -c conda-forge xgboost

# æ–¹æ³•2: é ç·¨è­¯ç‰ˆæœ¬
pip install xgboost

# æ–¹æ³•3: å¾æºç¢¼ç·¨è­¯ï¼ˆå¦‚éœ€GPUæ”¯æ´ï¼‰
git clone --recursive https://github.com/dmlc/xgboost
cd xgboost && make -j4
```
</details>

### ğŸ” æ€§èƒ½èª¿å„ªå»ºè­°

| å ´æ™¯ | å»ºè­°é…ç½® | èªªæ˜ |
|------|----------|------|
| **å°æ•¸æ“šé›† (<10K)** | é‚è¼¯è¿´æ­¸ + CPU | é€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½ |
| **ä¸­ç­‰æ•¸æ“šé›† (10K-50K)** | XGBoost + CPU | å¹³è¡¡é€Ÿåº¦èˆ‡æ•ˆæœ |
| **å¤§æ•¸æ“šé›† (>50K)** | XGBoost + GPU | æœ€ä½³æ€§èƒ½ |
| **è¨˜æ†¶é«”æœ‰é™** | å•Ÿç”¨æ•¸æ“šæŠ½æ¨£ | æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ |
| **è¿½æ±‚é€Ÿåº¦** | é—œéµè©æ³¨æ„åŠ› | è¨ˆç®—è¤‡é›œåº¦ä½ |
| **è¿½æ±‚æ•ˆæœ** | çµ„åˆæ³¨æ„åŠ› | æ•ˆæœæœ€ä½³ |

---

## ğŸ“– è¼¸å‡ºæ–‡ä»¶èªªæ˜

### ğŸ“ é‹è¡Œç›®éŒ„çµæ§‹
```
output/run_YYYYMMDD_HHMMSS/
â”œâ”€â”€ 01_preprocessing/
â”‚   â””â”€â”€ 01_preprocessed_data.csv       # é è™•ç†å¾Œçš„æ•¸æ“š
â”œâ”€â”€ 02_bert_encoding/
â”‚   â””â”€â”€ 02_bert_embeddings.npy         # BERTç‰¹å¾µå‘é‡
â”œâ”€â”€ 03_attention_testing/
â”‚   â”œâ”€â”€ attention_analysis_*.json      # æ³¨æ„åŠ›åˆ†æçµæœ
â”‚   â”œâ”€â”€ attention_comparison_*.json    # æ³¨æ„åŠ›æ¯”è¼ƒçµæœ
â”‚   â””â”€â”€ aspect_vectors_*.npy           # é¢å‘å‘é‡
â”œâ”€â”€ 04_analysis/
â”‚   â”œâ”€â”€ classification_report.txt      # åˆ†é¡å ±å‘Š
â”‚   â”œâ”€â”€ timing_analysis.json           # è¨ˆæ™‚çµ±è¨ˆ
â”‚   â””â”€â”€ performance_comparison.json    # æ€§èƒ½æ¯”è¼ƒ
â”œâ”€â”€ complete_analysis_results.json     # å®Œæ•´åˆ†æçµæœ
â”œâ”€â”€ sentiment_classifier_*.pkl         # è¨“ç·´å¥½çš„æ¨¡å‹
â””â”€â”€ label_encoder.pkl                  # æ¨™ç±¤ç·¨ç¢¼å™¨
```

### ğŸ“Š çµæœæ–‡ä»¶æ ¼å¼

<details>
<summary><b>ğŸ“„ complete_analysis_results.json</b></summary>

```json
{
  "experiment_info": {
    "timestamp": "2025-06-12 15:30:45",
    "dataset_size": 50000,
    "classifier_type": "xgboost"
  },
  "attention_results": {
    "similarity": {"accuracy": 0.913, "f1": 0.910},
    "keyword": {"accuracy": 0.925, "f1": 0.922},
    "self": {"accuracy": 0.941, "f1": 0.938},
    "combined": {"accuracy": 0.954, "f1": 0.952}
  },
  "best_mechanism": "combined",
  "performance_ranking": [...],
  "timing_statistics": {...}
}
```
</details>

---

## ğŸ“ å­¸è¡“æ‡‰ç”¨

### ğŸ“ è«–æ–‡å¯«ä½œè¦é»

#### å‰µæ–°é»æè¿°
1. **æ³¨æ„åŠ›æ©Ÿåˆ¶å‰µæ–°**: é¦–æ¬¡åœ¨æƒ…æ„Ÿé¢å‘å»ºæ¨¡ä¸­å¼•å…¥å¤šç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶
2. **ç³»çµ±æ€§æ¯”è¼ƒ**: æä¾›å–®ä¸€vsçµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶çš„å…¨é¢è©•ä¼°æ¡†æ¶
3. **è·¨é ˜åŸŸé©ç”¨**: æ”¯æ´é›»å½±ã€ç”¢å“ã€é¤å»³ç­‰å¤šé ˜åŸŸæƒ…æ„Ÿåˆ†æ

#### å¯¦é©—è¨­è¨ˆç¯„ä¾‹
```python
# æ§åˆ¶è®Šé‡å¯¦é©—è¨­è¨ˆ
experiment_config = {
    'dataset': 'IMDB_50K',
    'bert_model': 'bert-base-uncased',
    'attention_types': ['no', 'similarity', 'keyword', 'self', 'combined'],
    'classifiers': ['xgboost', 'logistic_regression', 'random_forest'],
    'evaluation_metrics': ['accuracy', 'f1', 'precision', 'recall'],
    'random_seed': 42
}
```

#### çµæœåˆ†æå»ºè­°
- **å®šé‡åˆ†æ**: ä½¿ç”¨å…§èšåº¦ã€åˆ†é›¢åº¦ã€æº–ç¢ºç‡ç­‰æŒ‡æ¨™
- **å®šæ€§åˆ†æ**: åˆ†æä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„é©ç”¨å ´æ™¯
- **è¨ˆç®—è¤‡é›œåº¦**: æ¯”è¼ƒæ•ˆæœèˆ‡æ•ˆç‡çš„æ¬Šè¡¡

### ğŸ“š å¼•ç”¨æ ¼å¼
```bibtex
@software{bert_attention_sentiment,
  title={BERTæ³¨æ„åŠ›æ©Ÿåˆ¶æƒ…æ„Ÿåˆ†æç³»çµ±},
  author={ç ”ç©¶åœ˜éšŠ},
  year={2025},
  url={repository-url},
  note={è·¨é ˜åŸŸæƒ…æ„Ÿåˆ†æèˆ‡å¤šé‡æ³¨æ„åŠ›æ©Ÿåˆ¶ç ”ç©¶å¹³å°}
}
```

---

## ğŸ¤ è²¢ç»æŒ‡å—

### ğŸ’¡ å¦‚ä½•è²¢ç»

æˆ‘å€‘æ­¡è¿å„ç¨®å½¢å¼çš„è²¢ç»ï¼

1. **ğŸ› å ±å‘Šå•é¡Œ**: åœ¨Issuesä¸­å ±å‘Šbugæˆ–æå‡ºåŠŸèƒ½éœ€æ±‚
2. **ğŸ’» ä»£ç¢¼è²¢ç»**: Forkå°ˆæ¡ˆï¼Œå‰µå»ºåŠŸèƒ½åˆ†æ”¯ï¼Œæäº¤Pull Request  
3. **ğŸ“š æ–‡æª”æ”¹é€²**: æ”¹å–„æ–‡æª”ã€æ•™ç¨‹æˆ–ç¤ºä¾‹ä»£ç¢¼
4. **ğŸ§ª æ¸¬è©¦ç”¨ä¾‹**: æ·»åŠ å–®å…ƒæ¸¬è©¦æˆ–æ•´åˆæ¸¬è©¦

### ğŸ”„ é–‹ç™¼æµç¨‹
```bash
# 1. Forkä¸¦å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/yourusername/Part05_.git

# 2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
git checkout -b feature/new-attention-mechanism

# 3. é€²è¡Œé–‹ç™¼å’Œæ¸¬è©¦
python -m pytest tests/

# 4. æäº¤è®Šæ›´
git commit -m "Add new attention mechanism: focal attention"

# 5. æ¨é€ä¸¦å‰µå»ºPR
git push origin feature/new-attention-mechanism
```

---

## ğŸ“„ æˆæ¬Šè²æ˜

æœ¬å°ˆæ¡ˆåƒ…ç”¨æ–¼å­¸è¡“ç ”ç©¶ç›®çš„ã€‚ä½¿ç”¨æœ¬å°ˆæ¡ˆçš„ä»£ç¢¼æˆ–æ€è·¯æ™‚ï¼Œè«‹é©ç•¶å¼•ç”¨ä¸¦éµå¾ªå­¸è¡“èª ä¿¡åŸå‰‡ã€‚

### âš–ï¸ æˆæ¬Šå”è­°
- **ç ”ç©¶ä½¿ç”¨**: âœ… å…è¨±ç”¨æ–¼å­¸è¡“ç ”ç©¶å’Œæ•™è‚²ç›®çš„
- **å•†æ¥­ä½¿ç”¨**: âŒ éœ€è¦ç²å¾—æ˜ç¢ºæˆæ¬Š
- **ä¿®æ”¹åˆ†ç™¼**: âœ… å…è¨±ä¿®æ”¹ï¼Œä½†éœ€ä¿ç•™åŸå§‹æˆæ¬Šè²æ˜
- **è²¬ä»»è²æ˜**: æœ¬è»Ÿé«”æŒ‰"ç¾ç‹€"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è­‰

---

## ğŸ“ è¯ç¹«æˆ‘å€‘

### ğŸ’¬ æ”¯æ´æ¸ é“

- **ğŸ› å•é¡Œå›å ±**: [GitHub Issues](issues-url)
- **ğŸ’¡ åŠŸèƒ½å»ºè­°**: [GitHub Discussions](discussions-url)  
- **ğŸ“§ å­¸è¡“åˆä½œ**: research@example.com
- **ğŸ“– æ–‡æª”Wiki**: [é …ç›®Wiki](wiki-url)

### ğŸŒŸ è‡´è¬

æ„Ÿè¬ä»¥ä¸‹é–‹æºé …ç›®çš„æ”¯æŒï¼š
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [XGBoost](https://github.com/dmlc/xgboost)
- [scikit-learn](https://github.com/scikit-learn/scikit-learn)
- [PyTorch](https://github.com/pytorch/pytorch)

---

<div align="center">

**ğŸš€ ç«‹å³é–‹å§‹æ‚¨çš„æƒ…æ„Ÿåˆ†æç ”ç©¶ä¹‹æ—…ï¼**

[å›åˆ°é ‚éƒ¨](#-bertæ³¨æ„åŠ›æ©Ÿåˆ¶æƒ…æ„Ÿåˆ†æç³»çµ±) â€¢ [å¿«é€Ÿå®‰è£](#å®‰è£æŒ‡å—) â€¢ [ä½¿ç”¨æ•™ç¨‹](#ä½¿ç”¨æ•™ç¨‹)

---

*å¦‚æœæœ¬å°ˆæ¡ˆå°æ‚¨çš„ç ”ç©¶æœ‰å¹«åŠ©ï¼Œè«‹çµ¦æˆ‘å€‘ä¸€å€‹ â­ Starï¼*

</div>