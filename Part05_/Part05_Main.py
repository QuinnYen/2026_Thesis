#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BERTæƒ…æ„Ÿåˆ†æç³»çµ± - ä¸»ç¨‹å¼
æ•´åˆæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æåŠŸèƒ½
"""

import sys
import os
import logging
import pandas as pd
from datetime import datetime
from typing import Optional, List, Dict
import numpy as np

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°Pythonè·¯å¾‘ï¼Œä¸¦å®šç¾©ç‚ºå…¨åŸŸè®Šæ•¸
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, CURRENT_DIR)

# åŒ¯å…¥è·¯å¾‘é…ç½®
from config.paths import get_base_output_dir, setup_custom_output_dir

# åŒ¯å…¥éŒ¯èª¤è™•ç†å·¥å…·
from utils.error_handler import handle_error, handle_warning, handle_info, with_error_handling

# åŒ¯å…¥å„²å­˜ç®¡ç†å·¥å…·
from utils.storage_manager import StorageManager

from modules.run_manager import RunManager
from modules.attention_processor import AttentionProcessor
from modules.sentiment_classifier import SentimentClassifier
from modules.pipeline_processor import AnalysisPipeline, MultiPipelineComparison, create_simple_pipeline, run_quick_analysis
from modules.text_encoders import TextEncoderFactory
from modules.classification_methods import ClassificationMethodFactory
from modules.cross_validation import CrossValidationEvaluator

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ³¨æ„ï¼šç§»é™¤äº†å…¨åŸŸRunManagerï¼Œæ¯å€‹è™•ç†å™¨æœƒå‰µå»ºè‡ªå·±çš„RunManagerå¯¦ä¾‹

def process_bert_encoding(input_file: Optional[str] = None, output_dir: Optional[str] = None) -> str:
    """
    ä½¿ç”¨BERTæ¨¡å‹è™•ç†æ–‡æœ¬ä¸¦æå–ç‰¹å¾µå‘é‡
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨é è¨­è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘ï¼Œå¦‚æœç‚ºNoneå‰‡è‡ªå‹•ç”Ÿæˆ
        
    Returns:
        str: è¼¸å‡ºç›®éŒ„è·¯å¾‘
    """
    from modules.bert_encoder import BertEncoder
    
    try:
        # å¦‚æœæ²’æœ‰æŒ‡å®šè¼¸å‡ºç›®éŒ„ï¼Œä½¿ç”¨é…ç½®çš„é è¨­ç›®éŒ„
        if output_dir is None:
            output_dir = get_base_output_dir()
            
        # åˆå§‹åŒ–BERTç·¨ç¢¼å™¨ï¼Œå‚³å…¥è¼¸å‡ºç›®éŒ„
        encoder = BertEncoder(output_dir=output_dir)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è®€å–é è™•ç†å¾Œçš„æ•¸æ“š
        logger.info(f"è®€å–æ•¸æ“š: {input_file}")
        df = pd.read_csv(input_file)
        
        # æª¢æŸ¥å¿…è¦çš„æ¬„ä½ï¼ŒæŒ‰å„ªå…ˆé †åºæ’åˆ—
        required_columns = ['processed_text', 'clean_text', 'text', 'review']
        text_column = None
        for col in required_columns:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            available_columns = ', '.join(df.columns)
            raise ValueError(f"åœ¨è¼¸å…¥æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°æ–‡æœ¬æ¬„ä½ï¼ˆå„ªå…ˆé †åºï¼šprocessed_text > clean_text > text > reviewï¼‰ã€‚å¯ç”¨çš„æ¬„ä½æœ‰ï¼š{available_columns}")
        
        # å°æ–‡æœ¬é€²è¡Œç·¨ç¢¼
        logger.info(f"é–‹å§‹BERTç·¨ç¢¼...ä½¿ç”¨æ¬„ä½ï¼š{text_column}")
        embeddings = encoder.encode(df[text_column])
        
        # æ³¨æ„ï¼šembed.encode() æ–¹æ³•å·²ç¶“è‡ªå‹•ä¿å­˜äº†ç‰¹å¾µå‘é‡ï¼Œç„¡éœ€å†æ¬¡ä¿å­˜
        
        logger.info(f"è™•ç†å®Œæˆï¼çµæœä¿å­˜åœ¨: {encoder.output_dir}")
        return encoder.output_dir
        
    except Exception as e:
        handle_error(e, "BERTç·¨ç¢¼è™•ç†", show_traceback=True)
        raise

def process_attention_analysis(input_file: Optional[str] = None, 
                             output_dir: Optional[str] = None,
                             attention_types: Optional[List[str]] = None,
                             topics_path: Optional[str] = None,
                             attention_weights: Optional[Dict] = None,
                             encoder_type: str = 'bert') -> Dict:
    """
    åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        attention_types: è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
        attention_weights: çµ„åˆæ³¨æ„åŠ›æ¬Šé‡é…ç½®
        encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5, cnn, elmo)
        
    Returns:
        Dict: åˆ†æçµæœ
    """
    try:
        # åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è¨­å®šé è¨­çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
        
        logger.info(f"é–‹å§‹æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ...")
        logger.info(f"æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
        
        # åŸ·è¡Œæ³¨æ„åŠ›åˆ†æ
        results = processor.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            topics_path=topics_path,
            attention_weights=attention_weights,
            save_results=True
        )
        
        # è¼¸å‡ºçµæœæ‘˜è¦
        if 'comparison' in results and 'summary' in results['comparison']:
            summary = results['comparison']['summary']
            logger.info(f"åˆ†æå®Œæˆï¼æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {summary.get('best_mechanism', 'N/A')}")
            logger.info(f"æœ€ä½³ç¶œåˆå¾—åˆ†: {summary.get('best_score', 0):.4f}")
        
        logger.info(f"çµæœä¿å­˜åœ¨: {output_dir}")
        return results
        
    except Exception as e:
        handle_error(e, "æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ", show_traceback=True)
        raise

def process_attention_analysis_with_classification(input_file: Optional[str] = None, 
                                                 output_dir: Optional[str] = None,
                                                 attention_types: Optional[List[str]] = None,
                                                 topics_path: Optional[str] = None,
                                                 attention_weights: Optional[Dict] = None,
                                                 classifier_type: Optional[str] = None,
                                                 encoder_type: str = 'bert') -> Dict:
    """
    åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œåˆ†é¡è©•ä¼°
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        attention_types: è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
        attention_weights: çµ„åˆæ³¨æ„åŠ›æ¬Šé‡é…ç½®
        classifier_type: åˆ†é¡å™¨é¡å‹ (xgboost, logistic_regression, random_forest, svm_linear)
        encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5ç­‰ï¼Œé è¨­ç‚ºbert)
        
    Returns:
        Dict: å®Œæ•´çš„åˆ†æå’Œåˆ†é¡çµæœ
    """
    try:
        # GPUç’°å¢ƒæª¢æ¸¬å’Œé è™•ç†
        try:
            import torch
            if torch.cuda.is_available():
                logger.info(f"æª¢æ¸¬åˆ°GPUç’°å¢ƒ: {torch.cuda.get_device_name()}")
                # è¨­ç½®GPUè¨˜æ†¶é«”ç®¡ç†
                torch.cuda.empty_cache()
                # é˜²æ­¢GPUè¨˜æ†¶é«”ç¢ç‰‡åŒ–
                import gc
                gc.collect()
            else:
                logger.info("é‹è¡Œåœ¨CPUç’°å¢ƒ")
        except ImportError:
            logger.info("PyTorchæœªå®‰è£ï¼Œé‹è¡Œåœ¨CPUç’°å¢ƒ")
        except Exception as gpu_error:
            handle_warning(f"GPUç’°å¢ƒæª¢æ¸¬å¤±æ•—ï¼Œç¹¼çºŒä½¿ç”¨CPU: {str(gpu_error)}", "GPUæª¢æ¸¬")
        
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œåˆ†é¡è©•ä¼°")
        print("="*80)
        
        # åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è¨­å®šé è¨­çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
        
        print(f"\nğŸ“‹ åˆ†æé…ç½®:")
        print(f"   â€¢ è¼¸å…¥æ–‡ä»¶: {input_file}")
        print(f"   â€¢ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"   â€¢ æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
        
        logger.info(f"é–‹å§‹å®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œåˆ†é¡è©•ä¼°...")
        logger.info(f"æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
        
        # è®€å–å…ƒæ•¸æ“š
        df = pd.read_csv(input_file)
        
        # ç¬¬ä¸€éšæ®µï¼šåŸ·è¡Œæ³¨æ„åŠ›åˆ†æ
        print(f"\nğŸ”¬ éšæ®µ 1/3: æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ")
        print("-" * 50)
        attention_results = processor.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            topics_path=topics_path,
            attention_weights=attention_weights,
            save_results=False  # æš«ä¸ä¿å­˜ï¼Œç­‰åˆ†é¡è©•ä¼°å®Œæˆå¾Œä¸€èµ·ä¿å­˜
        )
        
        # ç¬¬äºŒéšæ®µï¼šåŸ·è¡Œåˆ†é¡è©•ä¼°
        print(f"\nğŸ¯ éšæ®µ 2/3: åˆ†é¡æ€§èƒ½è©•ä¼°")
        print("-" * 50)
        logger.info("é–‹å§‹åŸ·è¡Œåˆ†é¡è©•ä¼°...")
        classifier = SentimentClassifier(output_dir=output_dir, encoder_type=encoder_type)
        
        # ä¿®æ­£ï¼šç²å–æˆ–è¼‰å…¥ç·¨ç¢¼å™¨åµŒå…¥å‘é‡ï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
        print(f"   ğŸ” è¼‰å…¥ç·¨ç¢¼å™¨åµŒå…¥å‘é‡ç”¨æ–¼åˆ†é¡è©•ä¼°...")
        original_embeddings = None
        
        # ä½¿ç”¨é€šç”¨çš„æª”æ¡ˆæª¢æ¸¬é‚è¼¯
        temp_processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # å˜—è©¦æ‰¾åˆ°ç¾æœ‰çš„ç·¨ç¢¼å™¨æª”æ¡ˆ
        embeddings_file = temp_processor._find_existing_embeddings(encoder_type)
        
        if embeddings_file and os.path.exists(embeddings_file):
            # è¼‰å…¥å·²å­˜åœ¨çš„ç·¨ç¢¼å™¨åµŒå…¥å‘é‡
            original_embeddings = np.load(embeddings_file)
            print(f"   âœ… å·²è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡ï¼Œå½¢ç‹€: {original_embeddings.shape}")
            print(f"   ğŸ“ ä¾†æºæª”æ¡ˆ: {embeddings_file}")
            logger.info(f"è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡: {original_embeddings.shape}")
        
        if original_embeddings is None:
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œé‡æ–°ç”Ÿæˆï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            print(f"   ğŸ”„ æœªæ‰¾åˆ° {encoder_type.upper()} åµŒå…¥å‘é‡æ–‡ä»¶ï¼Œé–‹å§‹é‡æ–°ç”Ÿæˆ...")
            logger.info(f"æœªæ‰¾åˆ° {encoder_type.upper()} åµŒå…¥å‘é‡ï¼Œé–‹å§‹é‡æ–°ç”Ÿæˆ...")
            
            # æ ¹æ“šç·¨ç¢¼å™¨é¡å‹é¸æ“‡åˆé©çš„ç·¨ç¢¼å™¨
            if encoder_type == 'bert':
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            else:
                # å°æ–¼å…¶ä»–ç·¨ç¢¼å™¨é¡å‹ï¼Œä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹
                try:
                    from modules.encoder_factory import EncoderFactory
                    encoder = EncoderFactory.create_encoder(encoder_type, output_dir=output_dir)
                except:
                    # å¦‚æœæ¨¡çµ„åŒ–æ¶æ§‹ä¸å¯ç”¨ï¼Œå›é€€åˆ°BERT
                    print(f"   âš ï¸ {encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ï¼Œå›é€€ä½¿ç”¨BERT")
                    logger.warning(f"{encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ï¼Œå›é€€ä½¿ç”¨BERT")
                    from modules.bert_encoder import BertEncoder
                    encoder = BertEncoder(output_dir=output_dir)
            
            # æ‰¾åˆ°æ–‡æœ¬æ¬„ä½
            text_column = None
            for col in ['processed_text', 'clean_text', 'text', 'review']:
                if col in df.columns:
                    text_column = col
                    break
            
            if text_column:
                original_embeddings = encoder.encode(df[text_column])
                print(f"   âœ… {encoder_type.upper()} åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå½¢ç‹€: {original_embeddings.shape}")
                logger.info(f"ç”Ÿæˆçš„ {encoder_type.upper()} åµŒå…¥å‘é‡å½¢ç‹€: {original_embeddings.shape}")
            else:
                raise ValueError("ç„¡æ³•æ‰¾åˆ°æ–‡æœ¬æ¬„ä½ä¾†ç”ŸæˆåµŒå…¥å‘é‡")
        
        # è©•ä¼°ä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡æ€§èƒ½ï¼ˆä¿®æ­£ï¼šå‚³éåŸå§‹åµŒå…¥å‘é‡ï¼‰
        classification_results = classifier.evaluate_attention_mechanisms(
            attention_results, df, original_embeddings, model_type=classifier_type
        )
        
        # ç¬¬ä¸‰éšæ®µï¼šæ•´åˆçµæœ
        print(f"\nğŸ“Š éšæ®µ 3/3: æ•´åˆçµæœå’Œç”Ÿæˆå ±å‘Š")
        print("-" * 50)
        final_results = {
            'attention_analysis': attention_results,
            'classification_evaluation': classification_results,
            'processing_info': attention_results.get('processing_info', {}),
            'summary': {}
        }
        
        # ç”Ÿæˆç¶œåˆæ‘˜è¦
        if 'comparison' in classification_results:
            class_comparison = classification_results['comparison']
            # æ·»åŠ å®‰å…¨æª¢æŸ¥
            summary = class_comparison.get('summary', {})
            if summary:  # ç¢ºä¿summaryä¸ç‚ºç©º
                final_results['summary'] = {
                    'best_attention_mechanism': class_comparison.get('best_mechanism', 'N/A'),
                    'best_classification_accuracy': summary.get('best_accuracy', 0),
                    'best_f1_score': summary.get('best_f1', 0),
                    'mechanisms_tested': len(attention_types),
                    'evaluation_completed': True
                }
            else:
                # å¦‚æœæ²’æœ‰summaryï¼Œå‰µå»ºåŸºæœ¬æ‘˜è¦
                final_results['summary'] = {
                    'best_attention_mechanism': class_comparison.get('best_mechanism', 'N/A'),
                    'best_classification_accuracy': 0,
                    'best_f1_score': 0,
                    'mechanisms_tested': len(attention_types),
                    'evaluation_completed': False,
                    'error': 'Summary not available'
                }
            
            print(f"\nğŸ† æœ€çµ‚è©•ä¼°çµæœ:")
            print(f"   â€¢ æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {final_results['summary']['best_attention_mechanism']}")
            print(f"   â€¢ æœ€ä½³åˆ†é¡æº–ç¢ºç‡: {final_results['summary']['best_classification_accuracy']:.4f}")
            print(f"   â€¢ æœ€ä½³F1åˆ†æ•¸: {final_results['summary']['best_f1_score']:.4f}")
            
            logger.info(f"è©•ä¼°å®Œæˆï¼æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {final_results['summary']['best_attention_mechanism']}")
            logger.info(f"æœ€ä½³åˆ†é¡æº–ç¢ºç‡: {final_results['summary']['best_classification_accuracy']:.4f}")
            logger.info(f"æœ€ä½³F1åˆ†æ•¸: {final_results['summary']['best_f1_score']:.4f}")
        else:
            # å¦‚æœæ²’æœ‰comparisonçµæœï¼Œå‰µå»ºéŒ¯èª¤æ‘˜è¦
            final_results['summary'] = {
                'best_attention_mechanism': 'N/A',
                'best_classification_accuracy': 0,
                'best_f1_score': 0,
                'mechanisms_tested': len(attention_types),
                'evaluation_completed': False,
                'error': 'Classification comparison not available'
            }
        
        # ä¿å­˜å®Œæ•´çµæœåˆ°runç›®éŒ„æ ¹ç›®éŒ„
        if output_dir:
            print(f"\nğŸ’¾ ä¿å­˜å®Œæ•´åˆ†æçµæœ...")
            # ç¢ºä¿ä¿å­˜åˆ°runç›®éŒ„çš„æ ¹ç›®éŒ„
            from config.paths import get_path_config as get_config_for_results
            path_config = get_config_for_results()
            subdirs = [path_config.get_subdirectory_name(key) for key in ["preprocessing", "bert_encoding", "attention_testing", "analysis"]]
            
            if any(subdir in output_dir for subdir in subdirs):
                # å¦‚æœè¼¸å‡ºç›®éŒ„æ˜¯å­ç›®éŒ„ï¼Œæ”¹ç‚ºçˆ¶ç›®éŒ„ï¼ˆrunç›®éŒ„æ ¹ç›®éŒ„ï¼‰
                run_dir = os.path.dirname(output_dir)
            else:
                run_dir = output_dir
            
            filename = path_config.get_file_pattern("complete_analysis")
            results_file = os.path.join(run_dir, filename)
            with open(results_file, 'w', encoding='utf-8') as f:
                import json
                # è™•ç†ä¸å¯åºåˆ—åŒ–çš„å°è±¡
                serializable_results = _make_serializable(final_results)
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… å®Œæ•´çµæœå·²ä¿å­˜è‡³: {results_file}")
            logger.info(f"å®Œæ•´çµæœå·²ä¿å­˜è‡³: {results_file}")
        
        print(f"\nğŸ‰ å®Œæ•´åˆ†æè©•ä¼°å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰çµæœä¿å­˜åœ¨: {output_dir}")
        print("="*80)
        
        logger.info(f"å®Œæ•´åˆ†æçµæœä¿å­˜åœ¨: {output_dir}")
        return final_results
        
    except Exception as e:
        # ä½¿ç”¨æ–°çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
        handle_error(e, "å®Œæ•´åˆ†æéç¨‹", show_traceback=True)
        
        # å˜—è©¦GPUè¨˜æ†¶é«”æ¸…ç†
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                print("ğŸ§¹ å·²æ¸…ç†GPUè¨˜æ†¶é«”")
        except:
            pass
        
        # é‡æ–°æ‹‹å‡ºéŒ¯èª¤ï¼Œä½†æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
        raise RuntimeError(f"å®Œæ•´åˆ†æå¤±æ•—: {str(e)}ã€‚è«‹æª¢æŸ¥çµ‚ç«¯æ©Ÿè¼¸å‡ºä»¥ç²å–è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯ã€‚") from e

def _make_serializable(obj):
    """å°‡ç‰©ä»¶è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    if isinstance(obj, dict):
        return {key: _make_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [_make_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    elif isinstance(obj, pd.DataFrame):
        return obj.to_dict()
    else:
        return obj

def compare_attention_mechanisms(input_file: Optional[str] = None,
                               output_dir: Optional[str] = None,
                               attention_types: Optional[List[str]] = None,
                               encoder_type: str = 'bert') -> Dict:
    """
    å°ˆé–€ç”¨æ–¼æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœ
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        attention_types: è¦æ¯”è¼ƒçš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5ç­‰ï¼Œé è¨­ç‚ºbert)
        
    Returns:
        Dict: æ¯”è¼ƒçµæœ
    """
    try:
        # åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # åŸ·è¡Œæ¯”è¼ƒ
        results = processor.compare_attention_mechanisms(
            input_file=input_file,
            attention_types=attention_types
        )
        
        # ç”Ÿæˆå¯è®€çš„æ¯”è¼ƒå ±å‘Š
        if output_dir:
            report_file = os.path.join(output_dir, "attention_comparison_report.txt")
            processor.export_comparison_report(results, report_file)
        
        return results
        
    except Exception as e:
        handle_error(e, "æ³¨æ„åŠ›æ©Ÿåˆ¶æ¯”è¼ƒ", show_traceback=True)
        raise

def _generate_combination_name(combination_weights: Dict, index: int) -> str:
    """ç”Ÿæˆæœ‰æ„ç¾©çš„çµ„åˆæ³¨æ„åŠ›åç¨±
    
    Args:
        combination_weights: çµ„åˆæ¬Šé‡å­—å…¸ï¼Œå¦‚ {'similarity': 0.5, 'self': 0.5}
        index: çµ„åˆç´¢å¼•
        
    Returns:
        str: æœ‰æ„ç¾©çš„çµ„åˆåç¨±ï¼Œå¦‚ "ç›¸ä¼¼åº¦+è‡ªæ³¨æ„åŠ›çµ„åˆ"
    """
    # ä¸­æ–‡åç¨±æ˜ å°„
    name_mapping = {
        'similarity': 'ç›¸ä¼¼åº¦',
        'keyword': 'é—œéµè©', 
        'self': 'è‡ªæ³¨æ„åŠ›',
        'no': 'ç„¡æ³¨æ„åŠ›'
    }
    
    # æ‰¾å‡ºéé›¶æ¬Šé‡çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    active_mechanisms = []
    for mechanism, weight in combination_weights.items():
        if weight > 0:
            chinese_name = name_mapping.get(mechanism, mechanism)
            active_mechanisms.append(chinese_name)
    
    if len(active_mechanisms) == 0:
        return f"çµ„åˆ{index}"
    elif len(active_mechanisms) == 1:
        return f"{active_mechanisms[0]}çµ„åˆ"
    else:
        # å°‡æ©Ÿåˆ¶åç¨±ç”¨ "+" é€£æ¥
        combined_name = "+".join(active_mechanisms)
        return f"{combined_name}çµ„åˆ"


def process_attention_analysis_with_multiple_combinations(input_file: Optional[str] = None, 
                                                       output_dir: Optional[str] = None,
                                                       attention_types: Optional[List[str]] = None,
                                                       attention_combinations: Optional[List[Dict]] = None,
                                                       classifier_type: Optional[str] = None,
                                                       encoder_type: str = 'bert') -> Dict:
    """
    åŸ·è¡Œå¤šç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çµ„åˆåˆ†æï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        attention_types: è¦æ¸¬è©¦çš„åŸºæœ¬æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        attention_combinations: å¤šå€‹çµ„åˆæ³¨æ„åŠ›çš„æ¬Šé‡é…ç½®åˆ—è¡¨
        classifier_type: åˆ†é¡å™¨é¡å‹
        encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5, cnn, elmo)
        
    Returns:
        Dict: å®Œæ•´çš„åˆ†æå’Œåˆ†é¡çµæœ
    """
    try:
        # GPUç’°å¢ƒæª¢æ¸¬å’Œé è™•ç†
        try:
            import torch
            if torch.cuda.is_available():
                logger.info(f"æª¢æ¸¬åˆ°GPUç’°å¢ƒ: {torch.cuda.get_device_name()}")
                torch.cuda.empty_cache()
                import gc
                gc.collect()
            else:
                logger.info("é‹è¡Œåœ¨CPUç’°å¢ƒ")
        except ImportError:
            logger.info("PyTorchæœªå®‰è£ï¼Œé‹è¡Œåœ¨CPUç’°å¢ƒ")
        except Exception as gpu_error:
            handle_warning(f"GPUç’°å¢ƒæª¢æ¸¬å¤±æ•—ï¼Œç¹¼çºŒä½¿ç”¨CPU: {str(gpu_error)}", "GPUæª¢æ¸¬")
        
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹åŸ·è¡Œå¤šé‡æ³¨æ„åŠ›æ©Ÿåˆ¶çµ„åˆåˆ†æ")
        print("="*80)
        
        # åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è¨­å®šé è¨­çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self']
        
        # è¨­å®šé è¨­çš„çµ„åˆé…ç½®
        if attention_combinations is None:
            attention_combinations = []
        
        # çµ„åˆæ‰€æœ‰è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        all_attention_types = attention_types.copy()
        
        print(f"\nğŸ“‹ åˆ†æé…ç½®:")
        print(f"   â€¢ è¼¸å…¥æ–‡ä»¶: {input_file}")
        print(f"   â€¢ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"   â€¢ åŸºæœ¬æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
        print(f"   â€¢ çµ„åˆé…ç½®æ•¸é‡: {len(attention_combinations)}")
        
        # è®€å–å…ƒæ•¸æ“š
        df = pd.read_csv(input_file)
        
        # ç¬¬ä¸€éšæ®µï¼šåŸ·è¡ŒåŸºæœ¬æ³¨æ„åŠ›åˆ†æ
        print(f"\nğŸ”¬ éšæ®µ 1/3: åŸºæœ¬æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ")
        print("-" * 50)
        
        # å…ˆåŸ·è¡ŒåŸºæœ¬æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ
        basic_results = processor.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            save_results=False
        )
        
        # é‡è¦ï¼šæª¢æŸ¥å¯¦éš›ä½¿ç”¨çš„ç·¨ç¢¼å™¨é¡å‹ï¼ˆå¯èƒ½å› ç‚ºå›é€€è€Œæ”¹è®Šï¼‰
        actual_encoder_type = processor.encoder_type
        if actual_encoder_type != encoder_type:
            print(f"   âš ï¸ ç·¨ç¢¼å™¨å·²å¾ {encoder_type.upper()} å›é€€åˆ° {actual_encoder_type.upper()}")
            handle_warning(f"ç·¨ç¢¼å™¨å·²å¾ {encoder_type.upper()} å›é€€åˆ° {actual_encoder_type.upper()}", "ç·¨ç¢¼å™¨å›é€€")
            
            # æª¢æŸ¥æ˜¯å¦å­˜åœ¨èˆŠçš„æ³¨æ„åŠ›åˆ†æçµæœï¼ˆå¯èƒ½ä½¿ç”¨ä¸åŒç·¨ç¢¼å™¨ï¼‰
            print(f"   ğŸ” æª¢æŸ¥æ³¨æ„åŠ›åˆ†æçµæœçš„ç·¨ç¢¼å™¨ä¸€è‡´æ€§...")
            
            # æ›´æ–°ç·¨ç¢¼å™¨é¡å‹ä»¥ç¢ºä¿å¾ŒçºŒéšæ®µä¸€è‡´æ€§
            encoder_type = actual_encoder_type
            
            # å¦‚æœæœ‰èˆŠçµæœï¼Œéœ€è¦æ¸…ç†ä»¥ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„ç·¨ç¢¼å™¨é‡æ–°åˆ†æ
            attention_dir = os.path.join(output_dir, "03_attention_testing")
            if os.path.exists(attention_dir):
                print(f"   ğŸ§¹ æ¸…ç†å¯èƒ½ä¸ä¸€è‡´çš„èˆŠæ³¨æ„åŠ›åˆ†æçµæœ...")
                logger.info("æ¸…ç†å¯èƒ½ä½¿ç”¨ä¸åŒç·¨ç¢¼å™¨çš„èˆŠæ³¨æ„åŠ›åˆ†æçµæœ")
                import shutil
                try:
                    shutil.rmtree(attention_dir)
                    print(f"   âœ… å·²æ¸…ç†èˆŠçµæœï¼Œå°‡ä½¿ç”¨ {encoder_type.upper()} é‡æ–°åˆ†æ")
                except Exception as e:
                    logger.warning(f"æ¸…ç†èˆŠçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # é‡æ–°å‰µå»ºæ³¨æ„åŠ›è™•ç†å™¨ï¼Œç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„ç·¨ç¢¼å™¨é¡å‹
            processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
            
            # é‡æ–°åŸ·è¡Œæ³¨æ„åŠ›åˆ†æä»¥ç¢ºä¿ä¸€è‡´æ€§
            print(f"   ğŸ”„ ä½¿ç”¨ {encoder_type.upper()} é‡æ–°åŸ·è¡Œæ³¨æ„åŠ›åˆ†æ...")
            basic_results = processor.process_with_attention(
                input_file=input_file,
                attention_types=attention_types,
                save_results=False
            )
        
        # ç¬¬äºŒéšæ®µï¼šåŸ·è¡Œçµ„åˆæ³¨æ„åŠ›åˆ†æ
        if attention_combinations:
            print(f"\nğŸ”— éšæ®µ 2/3: çµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ")
            print("-" * 50)
            
            combination_results = {}
            for i, combination in enumerate(attention_combinations, 1):
                # ç”Ÿæˆæœ‰æ„ç¾©çš„çµ„åˆåç¨±
                combination_name = _generate_combination_name(combination, i)
                print(f"   ğŸ”„ è™•ç†çµ„åˆ {i}/{len(attention_combinations)}: {combination}")
                print(f"      çµ„åˆåç¨±: {combination_name}")
                
                # åŸ·è¡Œå–®å€‹çµ„åˆåˆ†æ
                combo_result = processor.process_with_attention(
                    input_file=input_file,
                    attention_types=['combined'],
                    attention_weights=combination,
                    save_results=False
                )
                
                # å°‡çµ„åˆçµæœæ·»åŠ åˆ°åŸºæœ¬çµæœä¸­ï¼ŒåŒæ™‚ä¿å­˜æ¬Šé‡é…ç½®
                combo_data = combo_result['combined'].copy()
                
                # æ¸…ç†ä¸¦ä¿å­˜æ¬Šé‡é…ç½®
                clean_weights = {k: v for k, v in combination.items() if not k.startswith('_')}
                combo_data['attention_weights'] = clean_weights
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæ™ºèƒ½å­¸ç¿’æ¬Šé‡
                if combination.get('_is_learned', False):
                    combo_data['learned_weights'] = clean_weights
                    combo_data['is_learned_weights'] = True
                    
                    # åœ¨çµ‚ç«¯æ©Ÿé¡¯è‘—æ‰“å°æ™ºèƒ½å­¸ç¿’åˆ°çš„æ¬Šé‡
                    print("\n" + "=" * 80)
                    print("ğŸ§  æ™ºèƒ½æ¬Šé‡å­¸ç¿’çµæœ")
                    print("=" * 80)
                    print(f"ğŸ“Š æ©Ÿåˆ¶åç¨±: {combination_name}")
                    print(f"ğŸ¯ å­¸ç¿’æ–¹æ³•: æ™ºèƒ½å‹•æ…‹æ¬Šé‡å­¸ç¿’")
                    print(f"ğŸ“ˆ å­¸ç¿’åˆ°çš„æœ€ä½³æ¬Šé‡é…ç½®:")
                    for mechanism, weight in clean_weights.items():
                        print(f"   â€¢ {mechanism}: {weight:.6f} ({weight*100:.2f}%)")
                    
                    # è¨ˆç®—æ¬Šé‡åˆ†å¸ƒçµ±è¨ˆ
                    weights_list = list(clean_weights.values())
                    max_weight = max(weights_list)
                    min_weight = min(weights_list)
                    weight_range = max_weight - min_weight
                    
                    print(f"\nğŸ“Š æ¬Šé‡åˆ†å¸ƒçµ±è¨ˆ:")
                    print(f"   â€¢ æœ€å¤§æ¬Šé‡: {max_weight:.6f}")
                    print(f"   â€¢ æœ€å°æ¬Šé‡: {min_weight:.6f}")
                    print(f"   â€¢ æ¬Šé‡ç¯„åœ: {weight_range:.6f}")
                    print(f"   â€¢ æ¬Šé‡ç¸½å’Œ: {sum(weights_list):.6f}")
                    
                    # é¡¯ç¤ºä¸»å°æ©Ÿåˆ¶
                    dominant_mechanism = max(clean_weights.items(), key=lambda x: x[1])
                    print(f"\nğŸ† ä¸»å°æ³¨æ„åŠ›æ©Ÿåˆ¶: {dominant_mechanism[0]} ({dominant_mechanism[1]*100:.2f}%)")
                    
                    print("=" * 80)
                    logger.info(f"æ™ºèƒ½å­¸ç¿’æ¬Šé‡ - {combination_name}: {clean_weights}")
                
                combination_results[combination_name] = combo_data
                # ç‚ºäº†çµ±ä¸€æ ¼å¼ï¼Œä¹Ÿæ·»åŠ åˆ°all_attention_typesä¸­
                all_attention_types.append(combination_name)
            
            # åˆä½µåŸºæœ¬çµæœå’Œçµ„åˆçµæœ
            final_attention_results = basic_results.copy()
            final_attention_results.update(combination_results)
        else:
            final_attention_results = basic_results
            print(f"\nâ­ï¸ è·³éçµ„åˆåˆ†æéšæ®µï¼ˆç„¡çµ„åˆé…ç½®ï¼‰")
        
        # ç¬¬ä¸‰éšæ®µï¼šåŸ·è¡Œåˆ†é¡è©•ä¼°
        print(f"\nğŸ¯ éšæ®µ 3/3: åˆ†é¡æ€§èƒ½è©•ä¼°")
        print("-" * 50)
        logger.info("é–‹å§‹åŸ·è¡Œåˆ†é¡è©•ä¼°...")
        classifier = SentimentClassifier(output_dir=output_dir, encoder_type=encoder_type)
        
        # ç²å–æˆ–è¼‰å…¥ç·¨ç¢¼å™¨åµŒå…¥å‘é‡ï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
        print(f"   ğŸ” è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡ç”¨æ–¼åˆ†é¡è©•ä¼°...")
        original_embeddings = None
        
        # ä½¿ç”¨é€šç”¨çš„æª”æ¡ˆæª¢æ¸¬é‚è¼¯
        temp_processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        
        # å˜—è©¦æ‰¾åˆ°ç¾æœ‰çš„ç·¨ç¢¼å™¨æª”æ¡ˆ
        embeddings_file = temp_processor._find_existing_embeddings(encoder_type)
        
        if embeddings_file and os.path.exists(embeddings_file):
            original_embeddings = np.load(embeddings_file)
            print(f"   âœ… å·²è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡ï¼Œå½¢ç‹€: {original_embeddings.shape}")
            print(f"   ğŸ“ ä¾†æºæª”æ¡ˆ: {embeddings_file}")
            logger.info(f"è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡: {original_embeddings.shape}")
        
        if original_embeddings is None:
            print(f"   ğŸ”„ æœªæ‰¾åˆ° {encoder_type.upper()} åµŒå…¥å‘é‡æ–‡ä»¶ï¼Œé–‹å§‹é‡æ–°ç”Ÿæˆ...")
            logger.info(f"æœªæ‰¾åˆ° {encoder_type.upper()} åµŒå…¥å‘é‡ï¼Œé–‹å§‹é‡æ–°ç”Ÿæˆ...")
            
            # æ ¹æ“šç·¨ç¢¼å™¨é¡å‹é¸æ“‡åˆé©çš„ç·¨ç¢¼å™¨
            if encoder_type == 'bert':
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            else:
                # å°æ–¼å…¶ä»–ç·¨ç¢¼å™¨é¡å‹ï¼Œä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹
                try:
                    from modules.encoder_factory import EncoderFactory
                    encoder = EncoderFactory.create_encoder(encoder_type, output_dir=output_dir)
                except:
                    # å¦‚æœæ¨¡çµ„åŒ–æ¶æ§‹ä¸å¯ç”¨ï¼Œå›é€€åˆ°BERT
                    print(f"   âš ï¸ {encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ï¼Œå›é€€ä½¿ç”¨BERT")
                    logger.warning(f"{encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ï¼Œå›é€€ä½¿ç”¨BERT")
                    from modules.bert_encoder import BertEncoder
                    encoder = BertEncoder(output_dir=output_dir)
            
            # æ‰¾åˆ°æ–‡æœ¬æ¬„ä½
            text_column = None
            for col in ['processed_text', 'clean_text', 'text', 'review']:
                if col in df.columns:
                    text_column = col
                    break
            
            if text_column:
                original_embeddings = encoder.encode(df[text_column])
                print(f"   âœ… {encoder_type.upper()} åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå½¢ç‹€: {original_embeddings.shape}")
                logger.info(f"ç”Ÿæˆçš„ {encoder_type.upper()} åµŒå…¥å‘é‡å½¢ç‹€: {original_embeddings.shape}")
            else:
                raise ValueError("ç„¡æ³•æ‰¾åˆ°æ–‡æœ¬æ¬„ä½ä¾†ç”ŸæˆåµŒå…¥å‘é‡")
        
        # è©•ä¼°æ‰€æœ‰æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡æ€§èƒ½
        classification_results = classifier.evaluate_attention_mechanisms(
            final_attention_results, df, original_embeddings, model_type=classifier_type
        )
        
        # æ•´åˆçµæœ
        print(f"\nğŸ“Š æ•´åˆåˆ†æçµæœ...")
        final_results = {
            'attention_analysis': final_attention_results,
            'classification_evaluation': classification_results,
            'processing_info': basic_results.get('processing_info', {}),
            'summary': {}
        }
        
        # ç”Ÿæˆç¶œåˆæ‘˜è¦
        if 'comparison' in classification_results:
            class_comparison = classification_results['comparison']
            summary = class_comparison.get('summary', {})
            if summary:
                final_results['summary'] = {
                    'best_attention_mechanism': class_comparison.get('best_mechanism', 'N/A'),
                    'best_classification_accuracy': summary.get('best_accuracy', 0),
                    'best_f1_score': summary.get('best_f1', 0),
                    'mechanisms_tested': len(all_attention_types),
                    'combinations_tested': len(attention_combinations),
                    'evaluation_completed': True
                }
            else:
                final_results['summary'] = {
                    'best_attention_mechanism': class_comparison.get('best_mechanism', 'N/A'),
                    'best_classification_accuracy': 0,
                    'best_f1_score': 0,
                    'mechanisms_tested': len(all_attention_types),
                    'combinations_tested': len(attention_combinations),
                    'evaluation_completed': False,
                    'error': 'Summary not available'
                }
            
            print(f"\nğŸ† æœ€çµ‚è©•ä¼°çµæœ:")
            print(f"   â€¢ æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {final_results['summary']['best_attention_mechanism']}")
            print(f"   â€¢ æœ€ä½³åˆ†é¡æº–ç¢ºç‡: {final_results['summary']['best_classification_accuracy']:.4f}")
            print(f"   â€¢ æœ€ä½³F1åˆ†æ•¸: {final_results['summary']['best_f1_score']:.4f}")
            print(f"   â€¢ æ¸¬è©¦æ©Ÿåˆ¶ç¸½æ•¸: {final_results['summary']['mechanisms_tested']}")
            print(f"   â€¢ çµ„åˆé…ç½®æ•¸é‡: {final_results['summary']['combinations_tested']}")
            
            logger.info(f"è©•ä¼°å®Œæˆï¼æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {final_results['summary']['best_attention_mechanism']}")
            logger.info(f"æœ€ä½³åˆ†é¡æº–ç¢ºç‡: {final_results['summary']['best_classification_accuracy']:.4f}")
            logger.info(f"æœ€ä½³F1åˆ†æ•¸: {final_results['summary']['best_f1_score']:.4f}")
        else:
            final_results['summary'] = {
                'best_attention_mechanism': 'N/A',
                'best_classification_accuracy': 0,
                'best_f1_score': 0,
                'mechanisms_tested': len(all_attention_types),
                'combinations_tested': len(attention_combinations),
                'evaluation_completed': False,
                'error': 'Classification comparison not available'
            }
        
        # ä¿å­˜å®Œæ•´çµæœåˆ°runç›®éŒ„æ ¹ç›®éŒ„
        if output_dir:
            print(f"\nğŸ’¾ ä¿å­˜å®Œæ•´åˆ†æçµæœ...")
            # ç¢ºä¿ä¿å­˜åˆ°runç›®éŒ„çš„æ ¹ç›®éŒ„
            from config.paths import get_path_config as get_config_for_multi_results
            path_config = get_config_for_multi_results()
            subdirs = [path_config.get_subdirectory_name(key) for key in ["preprocessing", "bert_encoding", "attention_testing", "analysis"]]
            
            if any(subdir in output_dir for subdir in subdirs):
                # å¦‚æœè¼¸å‡ºç›®éŒ„æ˜¯å­ç›®éŒ„ï¼Œæ”¹ç‚ºçˆ¶ç›®éŒ„ï¼ˆrunç›®éŒ„æ ¹ç›®éŒ„ï¼‰
                run_dir = os.path.dirname(output_dir)
            else:
                run_dir = output_dir
            
            filename = path_config.get_file_pattern("multiple_analysis")
            results_file = os.path.join(run_dir, filename)
            with open(results_file, 'w', encoding='utf-8') as f:
                import json
                serializable_results = _make_serializable(final_results)
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… å®Œæ•´çµæœå·²ä¿å­˜è‡³: {results_file}")
            logger.info(f"å®Œæ•´çµæœå·²ä¿å­˜è‡³: {results_file}")
        
        print(f"\nğŸ‰ å¤šé‡çµ„åˆåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰çµæœä¿å­˜åœ¨: {output_dir}")
        print("="*80)
        
        logger.info(f"å¤šé‡çµ„åˆåˆ†æçµæœä¿å­˜åœ¨: {output_dir}")
        return final_results
        
    except Exception as e:
        # è©³ç´°çš„éŒ¯èª¤è¿½è¹¤
        import traceback
        error_details = traceback.format_exc()
        
        logger.error(f"å¤šé‡çµ„åˆåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"è©³ç´°éŒ¯èª¤è¿½è¹¤:\n{error_details}")
        
        # å˜—è©¦GPUè¨˜æ†¶é«”æ¸…ç†
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                logger.info("å·²æ¸…ç†GPUè¨˜æ†¶é«”")
        except:
            pass
        
        raise RuntimeError(f"å¤šé‡çµ„åˆåˆ†æå¤±æ•—: {str(e)}ã€‚è«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ä»¥ç²å–è©³ç´°çš„éŒ¯èª¤è¿½è¹¤ä¿¡æ¯ã€‚") from e

def run_new_pipeline_analysis(input_file: Optional[str] = None, 
                             output_dir: Optional[str] = None,
                             encoder_type: str = 'bert',
                             classifier_type: str = 'sentiment',
                             encoder_config: Optional[Dict] = None,
                             classifier_config: Optional[Dict] = None) -> Dict:
    """
    åŸ·è¡Œæ–°çš„æµç¨‹åˆ†ææ¶æ§‹
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        encoder_type: ç·¨ç¢¼å™¨é¡å‹ ('bert', 'gpt', 't5', 'cnn', 'elmo')
        classifier_type: åˆ†é¡å™¨é¡å‹ ('sentiment', 'lda', 'bertopic', 'nmf', 'clustering')
        encoder_config: ç·¨ç¢¼å™¨é…ç½®åƒæ•¸
        classifier_config: åˆ†é¡å™¨é…ç½®åƒæ•¸
        
    Returns:
        Dict: å®Œæ•´çš„åˆ†æçµæœ
    """
    try:
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæ–°æ¶æ§‹æµç¨‹åˆ†æ")
        print("="*80)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è®€å–æ•¸æ“š
        logger.info(f"è®€å–æ•¸æ“š: {input_file}")
        df = pd.read_csv(input_file)
        
        # æª¢æŸ¥å¿…è¦çš„æ¬„ä½
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            available_columns = ', '.join(df.columns)
            raise ValueError(f"åœ¨è¼¸å…¥æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°æ–‡æœ¬æ¬„ä½ã€‚å¯ç”¨çš„æ¬„ä½æœ‰ï¼š{available_columns}")
        
        texts = df[text_column]
        
        # æª¢æŸ¥æ¨™ç±¤æ¬„ä½
        labels = None
        label_column = None
        for col in ['label', 'sentiment', 'category', 'class']:
            if col in df.columns:
                label_column = col
                labels = df[col]
                break
        
        print(f"\nğŸ“‹ åˆ†æé…ç½®:")
        print(f"   â€¢ è¼¸å…¥æ–‡ä»¶: {input_file}")
        print(f"   â€¢ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"   â€¢ ç·¨ç¢¼å™¨: {encoder_type}")
        print(f"   â€¢ åˆ†é¡å™¨: {classifier_type}")
        print(f"   â€¢ æ–‡æœ¬æ¬„ä½: {text_column}")
        print(f"   â€¢ æ¨™ç±¤æ¬„ä½: {label_column if label_column else 'ç„¡'}")
        print(f"   â€¢ æ•¸æ“šé‡: {len(texts)}")
        
        # å‰µå»ºæµç¨‹
        pipeline = AnalysisPipeline(output_dir=output_dir)
        
        # é…ç½®æµç¨‹
        config_info = pipeline.configure_pipeline(
            encoder_type=encoder_type,
            classifier_type=classifier_type,
            encoder_config=encoder_config or {},
            classifier_config=classifier_config or {}
        )
        
        # é¡¯ç¤ºé…ç½®ä¿¡æ¯
        print(f"\nğŸ”§ æµç¨‹é…ç½®:")
        print(f"   â€¢ ç·¨ç¢¼å™¨: {config_info['encoder']['name']}")
        print(f"   â€¢ åµŒå…¥ç¶­åº¦: {config_info['encoder']['embedding_dim']}")
        print(f"   â€¢ åˆ†é¡å™¨: {config_info['classifier']['method_name']}")
        
        # é¡¯ç¤ºå…¼å®¹æ€§ä¿¡æ¯
        compatibility = config_info['compatibility']
        if compatibility['warnings']:
            print(f"   âš ï¸  è­¦å‘Š: {'; '.join(compatibility['warnings'])}")
        if compatibility['recommendations']:
            print(f"   ğŸ’¡ å»ºè­°: {'; '.join(compatibility['recommendations'])}")
        
        # é‹è¡Œæµç¨‹
        results = pipeline.run_pipeline(texts=texts, labels=labels)
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        if 'summary' in results:
            summary = results['summary']
            print(f"\nğŸ“Š çµæœæ‘˜è¦:")
            print(f"   â€¢ åˆ†æé¡å‹: {summary.get('analysis_type', 'N/A')}")
            
            if summary['analysis_type'] == 'æƒ…æ„Ÿåˆ†æ':
                print(f"   â€¢ æº–ç¢ºç‡: {summary.get('accuracy', 0):.4f}")
                print(f"   â€¢ F1åˆ†æ•¸: {summary.get('f1_score', 0):.4f}")
                print(f"   â€¢ é¡åˆ¥æ•¸: {summary.get('n_classes', 0)}")
            elif summary['analysis_type'] == 'ä¸»é¡Œå»ºæ¨¡':
                print(f"   â€¢ æ–¹æ³•: {summary.get('method', 'N/A')}")
                print(f"   â€¢ ä¸»é¡Œæ•¸: {summary.get('n_topics', 0)}")
                if 'coherence_score' in summary:
                    print(f"   â€¢ ä¸€è‡´æ€§åˆ†æ•¸: {summary['coherence_score']:.4f}")
            elif summary['analysis_type'] == 'èšé¡åˆ†æ':
                print(f"   â€¢ èšé¡æ–¹æ³•: {summary.get('clustering_method', 'N/A')}")
                print(f"   â€¢ èšé¡æ•¸: {summary.get('n_clusters', 0)}")
                if summary.get('silhouette_score'):
                    print(f"   â€¢ è¼ªå»“åˆ†æ•¸: {summary['silhouette_score']:.4f}")
        
        print(f"\nğŸ‰ æ–°æ¶æ§‹æµç¨‹åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_dir}")
        print("="*80)
        
        logger.info(f"æ–°æ¶æ§‹æµç¨‹åˆ†æçµæœä¿å­˜åœ¨: {output_dir}")
        return results
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        
        logger.error(f"æ–°æ¶æ§‹æµç¨‹åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"è©³ç´°éŒ¯èª¤è¿½è¹¤:\n{error_details}")
        
        # å˜—è©¦GPUè¨˜æ†¶é«”æ¸…ç†
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                logger.info("å·²æ¸…ç†GPUè¨˜æ†¶é«”")
        except:
            pass
        
        raise RuntimeError(f"æ–°æ¶æ§‹æµç¨‹åˆ†æå¤±æ•—: {str(e)}ã€‚è«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ä»¥ç²å–è©³ç´°çš„éŒ¯èª¤è¿½è¹¤ä¿¡æ¯ã€‚") from e

def run_multi_pipeline_comparison(input_file: Optional[str] = None, 
                                output_dir: Optional[str] = None,
                                pipeline_configs: Optional[List[Dict]] = None) -> Dict:
    """
    åŸ·è¡Œå¤šæµç¨‹æ¯”è¼ƒåˆ†æ
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        pipeline_configs: æµç¨‹é…ç½®åˆ—è¡¨
        
    Returns:
        Dict: æ¯”è¼ƒåˆ†æçµæœ
    """
    try:
        print("\n" + "="*80)
        print("ğŸ”¬ é–‹å§‹åŸ·è¡Œå¤šæµç¨‹æ¯”è¼ƒåˆ†æ")
        print("="*80)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è®€å–æ•¸æ“š
        logger.info(f"è®€å–æ•¸æ“š: {input_file}")
        df = pd.read_csv(input_file)
        
        # æª¢æŸ¥æ–‡æœ¬å’Œæ¨™ç±¤æ¬„ä½
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            raise ValueError("æ‰¾ä¸åˆ°æ–‡æœ¬æ¬„ä½")
        
        texts = df[text_column]
        
        labels = None
        for col in ['label', 'sentiment', 'category', 'class']:
            if col in df.columns:
                labels = df[col]
                break
        
        # è¨­ç½®é è¨­çš„æµç¨‹é…ç½®
        if pipeline_configs is None:
            pipeline_configs = [
                {
                    'name': 'BERT+æƒ…æ„Ÿåˆ†æ',
                    'encoder_type': 'bert',
                    'classifier_type': 'sentiment'
                },
                {
                    'name': 'BERT+LDAä¸»é¡Œå»ºæ¨¡',
                    'encoder_type': 'bert',
                    'classifier_type': 'lda',
                    'classifier_config': {'n_topics': 5}
                },
                {
                    'name': 'CNN+èšé¡åˆ†æ',
                    'encoder_type': 'cnn',
                    'classifier_type': 'clustering',
                    'classifier_config': {'method': 'kmeans', 'n_clusters': 5}
                }
            ]
        
        print(f"\nğŸ“‹ æ¯”è¼ƒé…ç½®:")
        print(f"   â€¢ è¼¸å…¥æ–‡ä»¶: {input_file}")
        print(f"   â€¢ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"   â€¢ æµç¨‹æ•¸é‡: {len(pipeline_configs)}")
        print(f"   â€¢ æ•¸æ“šé‡: {len(texts)}")
        
        # å‰µå»ºå¤šæµç¨‹æ¯”è¼ƒå™¨
        comparator = MultiPipelineComparison(output_dir=output_dir)
        
        # æ·»åŠ æµç¨‹é…ç½®
        for config in pipeline_configs:
            comparator.add_pipeline_config(
                name=config['name'],
                encoder_type=config['encoder_type'],
                classifier_type=config['classifier_type'],
                encoder_config=config.get('encoder_config', {}),
                classifier_config=config.get('classifier_config', {})
            )
        
        # é‹è¡Œæ¯”è¼ƒ
        comparison_results = comparator.run_comparison(texts=texts, labels=labels)
        
        # é¡¯ç¤ºæ¯”è¼ƒçµæœ
        metrics = comparison_results.get('comparison_metrics', {})
        if 'error' not in metrics:
            print(f"\nğŸ“Š æ¯”è¼ƒçµæœ:")
            print(f"   â€¢ æˆåŠŸæµç¨‹: {metrics['successful_pipelines']}")
            print(f"   â€¢ å¤±æ•—æµç¨‹: {metrics['failed_pipelines']}")
            print(f"   â€¢ å¹³å‡è™•ç†æ™‚é–“: {metrics['performance_stats']['avg_processing_time']:.2f}ç§’")
            print(f"   â€¢ æœ€å¿«æµç¨‹: {metrics['fastest_pipeline']}")
            print(f"   â€¢ æœ€çœè¨˜æ†¶é«”æµç¨‹: {metrics['most_memory_efficient']}")
            
            recommendations = comparison_results.get('recommendations', {})
            if 'best_overall' in recommendations:
                print(f"   â€¢ ç¶œåˆæ¨è–¦: {recommendations['best_overall']}")
        
        print(f"\nğŸ‰ å¤šæµç¨‹æ¯”è¼ƒåˆ†æå®Œæˆï¼")
        print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_dir}")
        print("="*80)
        
        logger.info(f"å¤šæµç¨‹æ¯”è¼ƒåˆ†æçµæœä¿å­˜åœ¨: {output_dir}")
        return comparison_results
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        
        logger.error(f"å¤šæµç¨‹æ¯”è¼ƒåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"è©³ç´°éŒ¯èª¤è¿½è¹¤:\n{error_details}")
        
        raise RuntimeError(f"å¤šæµç¨‹æ¯”è¼ƒåˆ†æå¤±æ•—: {str(e)}ã€‚è«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ä»¥ç²å–è©³ç´°çš„éŒ¯èª¤è¿½è¹¤ä¿¡æ¯ã€‚") from e

def show_available_options():
    """é¡¯ç¤ºå¯ç”¨çš„ç·¨ç¢¼å™¨å’Œåˆ†é¡å™¨é¸é …"""
    print("\n" + "="*80)
    print("ğŸ“‹ å¯ç”¨çš„åˆ†æé¸é …")
    print("="*80)
    
    # é¡¯ç¤ºç·¨ç¢¼å™¨é¸é …
    print("\nğŸ”¤ æ–‡æœ¬ç·¨ç¢¼å™¨:")
    encoder_info = TextEncoderFactory.get_encoder_info()
    for encoder_type, info in encoder_info.items():
        status = "âœ…" if info.get('available', True) else "âŒ"
        print(f"   {status} {encoder_type.upper()}: {info['name']}")
        print(f"      - æè¿°: {info['description']}")
        print(f"      - åµŒå…¥ç¶­åº¦: {info['embedding_dim']}")
        print(f"      - å„ªå‹¢: {info['advantages']}")
        if not info.get('available', True) and 'note' in info:
            print(f"      - æ³¨æ„: {info['note']}")
        print()
    
    # é¡¯ç¤ºåˆ†é¡å™¨é¸é …
    print("ğŸ¯ åˆ†é¡/å»ºæ¨¡æ–¹æ³•:")
    classifier_info = ClassificationMethodFactory.get_method_info()
    for classifier_type, info in classifier_info.items():
        status = "âœ…" if info.get('available', True) else "âŒ"
        print(f"   {status} {classifier_type.upper()}: {info['name']}")
        print(f"      - æè¿°: {info['description']}")
        print(f"      - é¡å‹: {info['type']}")
        print(f"      - éœ€è¦æ¨™ç±¤: {'æ˜¯' if info['needs_labels'] else 'å¦'}")
        print(f"      - å„ªå‹¢: {info['advantages']}")
        if not info.get('available', True) and 'note' in info:
            print(f"      - æ³¨æ„: {info['note']}")
        print()
    
    print("="*80)

def process_cross_validation_analysis(input_file: Optional[str] = None,
                                    output_dir: Optional[str] = None,
                                    n_folds: int = 5,
                                    attention_types: Optional[List[str]] = None,
                                    model_types: Optional[List[str]] = None,
                                    encoder_type: str = 'bert') -> Dict:
    """
    åŸ·è¡Œ K æŠ˜äº¤å‰é©—è­‰åˆ†æ
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        n_folds: æŠ˜æ•¸ (5 æˆ– 10)
        attention_types: è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        model_types: è¦æ¸¬è©¦çš„æ¨¡å‹é¡å‹
        encoder_type: ç·¨ç¢¼å™¨é¡å‹
        
    Returns:
        Dict: äº¤å‰é©—è­‰çµæœ
    """
    try:
        print("\n" + "="*80)
        print(f"ğŸ”„ é–‹å§‹åŸ·è¡Œ {n_folds} æŠ˜äº¤å‰é©—è­‰åˆ†æ")
        print("="*80)
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è®€å–æ•¸æ“š
        logger.info(f"è®€å–æ•¸æ“š: {input_file}")
        df = pd.read_csv(input_file)
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰sentimentå‰‡å˜—è©¦æ ¹æ“šreview_starsç”Ÿæˆ
        if 'sentiment' not in df.columns:
            if 'review_stars' in df.columns:
                logger.info("æœªæ‰¾åˆ° 'sentiment' æ¬„ä½ï¼Œæ ¹æ“š 'review_stars' ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤...")
                # æ ¹æ“šè©•åˆ†ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤ï¼š1-2æ˜Ÿ=è² é¢, 3æ˜Ÿ=ä¸­æ€§, 4-5æ˜Ÿ=æ­£é¢
                def map_stars_to_sentiment(stars):
                    if stars <= 2:
                        return 'negative'
                    elif stars == 3:
                        return 'neutral'
                    else:
                        return 'positive'
                
                df['sentiment'] = df['review_stars'].apply(map_stars_to_sentiment)
                logger.info(f"ç”Ÿæˆçš„æƒ…æ„Ÿæ¨™ç±¤åˆ†ä½ˆï¼š{df['sentiment'].value_counts().to_dict()}")
            else:
                raise ValueError("æ•¸æ“šä¸­ç¼ºå°‘ 'sentiment' æ¬„ä½ï¼Œä¸”ç„¡æ³•æ‰¾åˆ° 'review_stars' æ¬„ä½ä¾†ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤")
        
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            raise ValueError("æ‰¾ä¸åˆ°æ–‡æœ¬æ¬„ä½")
        
        print(f"\nğŸ“‹ äº¤å‰é©—è­‰é…ç½®:")
        print(f"   â€¢ è¼¸å…¥æ–‡ä»¶: {input_file}")
        print(f"   â€¢ è¼¸å‡ºç›®éŒ„: {output_dir}")
        print(f"   â€¢ æŠ˜æ•¸: {n_folds}")
        print(f"   â€¢ ç·¨ç¢¼å™¨: {encoder_type.upper()}")
        print(f"   â€¢ æ•¸æ“šè¦æ¨¡: {len(df)} æ¨£æœ¬")
        print(f"   â€¢ é¡åˆ¥åˆ†ä½ˆ: {df['sentiment'].value_counts().to_dict()}")
        
        # è¨­å®šé è¨­çš„æ³¨æ„åŠ›æ©Ÿåˆ¶å’Œæ¨¡å‹é¡å‹
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
            
        if model_types is None:
            model_types = ['logistic_regression', 'random_forest', 'xgboost']
        
        # åˆå§‹åŒ–äº¤å‰é©—è­‰è©•ä¼°å™¨
        cv_evaluator = CrossValidationEvaluator(
            output_dir=output_dir, 
            n_folds=n_folds, 
            random_state=42
        )
        
        # ç¬¬ä¸€éšæ®µï¼šåŸ·è¡Œæ³¨æ„åŠ›åˆ†æ
        print(f"\nğŸ”¬ éšæ®µ 1/3: æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ")
        print("-" * 50)
        
        processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        attention_results = processor.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            save_results=False
        )
        
        # ç¬¬äºŒéšæ®µï¼šæº–å‚™æ¨¡å‹å’Œç‰¹å¾µ
        print(f"\nğŸ¯ éšæ®µ 2/3: æº–å‚™æ¨¡å‹å’Œç‰¹å¾µ")
        print("-" * 50)
        
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ä¾†ç²å–æ¨¡å‹
        classifier = SentimentClassifier(output_dir=output_dir, encoder_type=encoder_type)
        all_models = classifier.available_models
        
        # ç¯©é¸æŒ‡å®šçš„æ¨¡å‹
        models_dict = {name: model for name, model in all_models.items() 
                      if name in model_types}
        
        print(f"   â€¢ å¯ç”¨æ¨¡å‹: {list(models_dict.keys())}")
        
        # ç²å–åŸå§‹åµŒå…¥å‘é‡
        print(f"   ğŸ” è¼‰å…¥ {encoder_type.upper()} åµŒå…¥å‘é‡...")
        
        temp_processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        embeddings_file = temp_processor._find_existing_embeddings(encoder_type)
        
        if embeddings_file and os.path.exists(embeddings_file):
            original_embeddings = np.load(embeddings_file)
            print(f"   âœ… å·²è¼‰å…¥åµŒå…¥å‘é‡ï¼Œå½¢ç‹€: {original_embeddings.shape}")
        else:
            # é‡æ–°ç”ŸæˆåµŒå…¥å‘é‡
            print(f"   ğŸ”„ é‡æ–°ç”Ÿæˆ {encoder_type.upper()} åµŒå…¥å‘é‡...")
            if encoder_type == 'bert':
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            else:
                # å…¶ä»–ç·¨ç¢¼å™¨çš„è™•ç†
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            
            original_embeddings = encoder.encode(df[text_column])
            print(f"   âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œå½¢ç‹€: {original_embeddings.shape}")
        
        # æº–å‚™æ¨™ç±¤
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        # è¨“ç·´æ¨™ç±¤ç·¨ç¢¼å™¨
        label_encoder.fit(df['sentiment'].values)
        
        # ç¬¬ä¸‰éšæ®µï¼šåŸ·è¡Œäº¤å‰é©—è­‰
        print(f"\nğŸ“Š éšæ®µ 3/3: åŸ·è¡Œäº¤å‰é©—è­‰")
        print("-" * 50)
        
        # åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶çš„äº¤å‰é©—è­‰
        cv_results = cv_evaluator.evaluate_attention_mechanisms_cv(
            attention_results=attention_results,
            metadata=df,
            original_embeddings=original_embeddings,
            models_dict=models_dict,
            label_encoder=label_encoder
        )
        
        # ç”Ÿæˆæœ€çµ‚çµæœæ‘˜è¦
        print(f"\nğŸ† äº¤å‰é©—è­‰çµæœæ‘˜è¦:")
        
        if 'attention_comparison' in cv_results:
            comparison = cv_results['attention_comparison']
            if 'attention_ranking' in comparison and comparison['attention_ranking']:
                best_combo = comparison['attention_ranking'][0]
                print(f"   â€¢ æœ€ä½³çµ„åˆ: {best_combo['combination']}")
                print(f"   â€¢ å¹³å‡æº–ç¢ºç‡: {best_combo['accuracy_mean']:.4f}")
                print(f"   â€¢ å¹³å‡ F1 åˆ†æ•¸: {best_combo['f1_mean']:.4f}")
                print(f"   â€¢ ç©©å®šæ€§åˆ†æ•¸: {best_combo['stability_score']:.4f}")
                
                # é¡¯ç¤ºå‰ 3 å
                print(f"\n   ğŸ“ˆ å‰ 3 åçµ„åˆ:")
                for i, combo in enumerate(comparison['attention_ranking'][:3]):
                    print(f"      {i+1}. {combo['combination']}: "
                          f"æº–ç¢ºç‡ {combo['accuracy_mean']:.4f}, "
                          f"F1 {combo['f1_mean']:.4f}")
        
        print(f"\nğŸ‰ {n_folds} æŠ˜äº¤å‰é©—è­‰åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ çµæœä¿å­˜åœ¨: {output_dir}")
        print("="*80)
        
        logger.info(f"{n_folds} æŠ˜äº¤å‰é©—è­‰åˆ†æçµæœä¿å­˜åœ¨: {output_dir}")
        return cv_results
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        
        logger.error(f"{n_folds} æŠ˜äº¤å‰é©—è­‰åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"è©³ç´°éŒ¯èª¤è¿½è¹¤:\n{error_details}")
        
        raise RuntimeError(f"{n_folds} æŠ˜äº¤å‰é©—è­‰åˆ†æå¤±æ•—: {str(e)}") from e

def process_simple_cross_validation(input_file: Optional[str] = None,
                                  output_dir: Optional[str] = None,
                                  n_folds: int = 5,
                                  model_types: Optional[List[str]] = None,
                                  encoder_type: str = 'bert') -> Dict:
    """
    åŸ·è¡Œç°¡å–®çš„ K æŠ˜äº¤å‰é©—è­‰ï¼ˆåƒ…åŸºæœ¬åˆ†é¡ï¼Œä¸åŒ…å«æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰
    
    Args:
        input_file: è¼¸å…¥æ–‡ä»¶è·¯å¾‘
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        n_folds: æŠ˜æ•¸
        model_types: è¦æ¸¬è©¦çš„æ¨¡å‹é¡å‹
        encoder_type: ç·¨ç¢¼å™¨é¡å‹
        
    Returns:
        Dict: äº¤å‰é©—è­‰çµæœ
    """
    try:
        print(f"\nğŸ”„ é–‹å§‹åŸ·è¡Œç°¡å–® {n_folds} æŠ˜äº¤å‰é©—è­‰")
        
        # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
        if input_file is None or not os.path.exists(input_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶ï¼š{input_file}")
        
        # è®€å–æ•¸æ“š
        df = pd.read_csv(input_file)
        
        if 'sentiment' not in df.columns:
            raise ValueError("æ•¸æ“šä¸­ç¼ºå°‘ 'sentiment' æ¬„ä½")
        
        # ç²å–æ–‡æœ¬ç‰¹å¾µ
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in df.columns:
                text_column = col
                break
        
        # åˆå§‹åŒ–äº¤å‰é©—è­‰è©•ä¼°å™¨
        cv_evaluator = CrossValidationEvaluator(
            output_dir=output_dir, 
            n_folds=n_folds, 
            random_state=42
        )
        
        # ç²å–åµŒå…¥å‘é‡
        temp_processor = AttentionProcessor(output_dir=output_dir, encoder_type=encoder_type)
        embeddings_file = temp_processor._find_existing_embeddings(encoder_type)
        
        if embeddings_file and os.path.exists(embeddings_file):
            features = np.load(embeddings_file)
        else:
            # é‡æ–°ç”Ÿæˆ
            if encoder_type == 'bert':
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            else:
                from modules.bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=output_dir)
            
            features = encoder.encode(df[text_column])
        
        # æº–å‚™æ¨™ç±¤
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        labels = label_encoder.fit_transform(df['sentiment'].values)
        
        # æº–å‚™æ¨¡å‹
        classifier = SentimentClassifier(output_dir=output_dir, encoder_type=encoder_type)
        if model_types is None:
            model_types = ['logistic_regression', 'random_forest', 'xgboost']
        
        models_dict = {name: model for name, model in classifier.available_models.items() 
                      if name in model_types}
        
        # åŸ·è¡Œäº¤å‰é©—è­‰
        cv_results = cv_evaluator.evaluate_multiple_models(
            features=features,
            labels=labels,
            models_dict=models_dict,
            label_encoder=label_encoder
        )
        
        # é¡¯ç¤ºçµæœ
        if 'comparison' in cv_results and 'ranking' in cv_results['comparison']:
            ranking = cv_results['comparison']['ranking']
            if ranking:
                best = ranking[0]
                print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best['model_name']}")
                print(f"   â€¢ æº–ç¢ºç‡: {best['accuracy_mean']:.4f}")
                print(f"   â€¢ F1 åˆ†æ•¸: {best['f1_mean']:.4f}")
        
        return cv_results
        
    except Exception as e:
        logger.error(f"ç°¡å–®äº¤å‰é©—è­‰åˆ†æå¤±æ•—: {str(e)}")
        raise

def main():
    """
    ä¸»ç¨‹å¼å…¥å£é»
    """
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œåƒæ•¸
        if len(sys.argv) > 1:
            if sys.argv[1] == '--process':
                # åŸ·è¡ŒBERTç·¨ç¢¼è™•ç†
                process_bert_encoding()
            elif sys.argv[1] == '--attention':
                # åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ
                if len(sys.argv) > 2:
                    input_file = sys.argv[2]
                else:
                    input_file = None
                process_attention_analysis(input_file=input_file)
            elif sys.argv[1] == '--classify':
                # åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œåˆ†é¡è©•ä¼°
                if len(sys.argv) > 2:
                    input_file = sys.argv[2]
                else:
                    input_file = None
                process_attention_analysis_with_classification(input_file=input_file)
            elif sys.argv[1] == '--compare':
                # æ¯”è¼ƒæ³¨æ„åŠ›æ©Ÿåˆ¶
                if len(sys.argv) > 2:
                    input_file = sys.argv[2]
                else:
                    input_file = None
                compare_attention_mechanisms(input_file=input_file)
            elif sys.argv[1] == '--new-run':
                # æ¸…é™¤ä¸Šæ¬¡åŸ·è¡Œçš„è¨˜éŒ„ï¼Œå¼·åˆ¶å‰µå»ºæ–°çš„runç›®éŒ„
                pass  # å·²ç§»é™¤clear_last_runï¼Œä¿ç•™ä½”ä½
            elif sys.argv[1] == '--new-pipeline':
                # åŸ·è¡Œæ–°æ¶æ§‹æµç¨‹åˆ†æ
                input_file = sys.argv[2] if len(sys.argv) > 2 else None
                encoder_type = sys.argv[3] if len(sys.argv) > 3 else 'bert'
                classifier_type = sys.argv[4] if len(sys.argv) > 4 else 'sentiment'
                run_new_pipeline_analysis(input_file=input_file, 
                                         encoder_type=encoder_type,
                                         classifier_type=classifier_type)
            elif sys.argv[1] == '--multi-compare':
                # åŸ·è¡Œå¤šæµç¨‹æ¯”è¼ƒåˆ†æ
                input_file = sys.argv[2] if len(sys.argv) > 2 else None
                run_multi_pipeline_comparison(input_file=input_file)
            elif sys.argv[1] == '--show-options':
                # é¡¯ç¤ºå¯ç”¨é¸é …
                show_available_options()
            elif sys.argv[1] == '--cv':
                # åŸ·è¡Œ K æŠ˜äº¤å‰é©—è­‰
                input_file = sys.argv[2] if len(sys.argv) > 2 else None
                n_folds = int(sys.argv[3]) if len(sys.argv) > 3 else 5
                process_cross_validation_analysis(input_file=input_file, n_folds=n_folds)
            elif sys.argv[1] == '--simple-cv':
                # åŸ·è¡Œç°¡å–®äº¤å‰é©—è­‰
                input_file = sys.argv[2] if len(sys.argv) > 2 else None
                n_folds = int(sys.argv[3]) if len(sys.argv) > 3 else 5
                process_simple_cross_validation(input_file=input_file, n_folds=n_folds)
        else:
            # å˜—è©¦å•Ÿå‹•GUIï¼Œå¤±æ•—æ™‚é¡¯ç¤ºå¹«åŠ©ä¿¡æ¯
            try:
                from gui.main_window import main as gui_main
                print("æ­£åœ¨å•Ÿå‹•BERTæƒ…æ„Ÿåˆ†æç³»çµ±...")
                gui_main()
            except Exception as gui_error:
                if "display" in str(gui_error).lower() or "tkinter" in str(gui_error).lower():
                    print("ç„¡æ³•å•Ÿå‹•åœ–å½¢ä»‹é¢ï¼ˆé€™åœ¨ Docker å®¹å™¨ä¸­æ˜¯æ­£å¸¸çš„ï¼‰")
                    print("è«‹ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸é‹è¡Œç¨‹å¼ã€‚")
                    print("\nä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨é¸é …ï¼Œæˆ–å˜—è©¦ä»¥ä¸‹å‘½ä»¤ï¼š")
                    print("  --process              # åŸ·è¡ŒBERTç·¨ç¢¼è™•ç†")
                    print("  --attention data.csv   # åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ")
                    print("  --classify data.csv    # åŸ·è¡Œå®Œæ•´åˆ†é¡è©•ä¼°")
                    print("  --cv data.csv 5        # åŸ·è¡Œ5æŠ˜äº¤å‰é©—è­‰")
                    print("  --show-options         # é¡¯ç¤ºæ‰€æœ‰å¯ç”¨é¸é …")
                else:
                    raise gui_error
            
    except ImportError as e:
        handle_error(e, "æ¨¡çµ„å°å…¥", show_traceback=True)
        print("è«‹ç¢ºä¿æ‰€æœ‰å¿…è¦çš„æ¨¡çµ„éƒ½å·²å®‰è£")
    except Exception as e:
        handle_error(e, "ç¨‹å¼åŸ·è¡Œ", show_traceback=True)
        # åœ¨ Docker å®¹å™¨ä¸­é¿å…ç­‰å¾…è¼¸å…¥
        import os
        if os.getenv('DOCKER_CONTAINER'):
            print("ç¨‹å¼å·²çµæŸ")
        else:
            input("æŒ‰Enteréµé€€å‡º...")

def print_help():
    """è¼¸å‡ºå¹«åŠ©ä¿¡æ¯"""
    help_text = """
BERTæƒ…æ„Ÿåˆ†æç³»çµ± - ä½¿ç”¨èªªæ˜

åŸºæœ¬ç”¨æ³•:
    python Part05_Main.py                    # å•Ÿå‹•GUIä»‹é¢
    python Part05_Main.py --help            # é¡¯ç¤ºæ­¤å¹«åŠ©ä¿¡æ¯

å‘½ä»¤è¡Œé¸é …:
    --process                               # åŸ·è¡ŒBERTç·¨ç¢¼è™•ç†
    --attention [input_file]               # åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æï¼ˆåƒ…å¹¾ä½•è©•ä¼°ï¼‰
    --classify [input_file]                # åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œåˆ†é¡è©•ä¼°
    --compare [input_file]                 # æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœ
    --new-run                              # å‰µå»ºæ–°çš„åŸ·è¡Œç›®éŒ„
    
æ–°æ¶æ§‹é¸é …:
    --new-pipeline [input_file] [encoder] [classifier]  # åŸ·è¡Œæ–°æ¶æ§‹æµç¨‹åˆ†æ
    --multi-compare [input_file]           # åŸ·è¡Œå¤šæµç¨‹æ¯”è¼ƒåˆ†æ
    --show-options                         # é¡¯ç¤ºå¯ç”¨çš„ç·¨ç¢¼å™¨å’Œåˆ†é¡å™¨é¸é …

äº¤å‰é©—è­‰é¸é …:
    --cv [input_file] [n_folds]            # åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶ K æŠ˜äº¤å‰é©—è­‰
    --simple-cv [input_file] [n_folds]     # åŸ·è¡Œç°¡å–®æ¨¡å‹ K æŠ˜äº¤å‰é©—è­‰

åŠŸèƒ½èªªæ˜:
    --attention: åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æï¼Œè¨ˆç®—é¢å‘å‘é‡çš„å…§èšåº¦å’Œåˆ†é›¢åº¦
    --classify:  åŸ·è¡Œå®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›åˆ†æå’Œæƒ…æ„Ÿåˆ†é¡è©•ä¼°
                åŒ…å«è¨“ç·´åˆ†é¡å™¨ã€é æ¸¬æƒ…æ„Ÿæ¨™ç±¤ã€è¨ˆç®—æº–ç¢ºç‡ç­‰æŒ‡æ¨™

æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ:
    ç³»çµ±æ”¯æ´ä»¥ä¸‹æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹:
    - no/none: ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆå¹³å‡ï¼‰
    - similarity: åŸºæ–¼ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    - keyword: åŸºæ–¼é—œéµè©çš„æ³¨æ„åŠ›æ©Ÿåˆ¶  
    - self: è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶
    - combined: çµ„åˆå‹æ³¨æ„åŠ›æ©Ÿåˆ¶

é€²åº¦é¡¯ç¤ºåŠŸèƒ½:
    âœ¨ æ–°å¢åŠŸèƒ½ï¼šç³»çµ±ç¾åœ¨æœƒé¡¯ç¤ºè©³ç´°çš„åŸ·è¡Œé€²åº¦ä¿¡æ¯
    ğŸ“Š BERTç·¨ç¢¼é€²åº¦æ¢ï¼šé¡¯ç¤ºæ‰¹é‡è™•ç†çš„é€²åº¦
    ğŸ”¬ æ³¨æ„åŠ›åˆ†æé€²åº¦ï¼šé¡¯ç¤ºå„å€‹éšæ®µçš„å®Œæˆç‹€æ…‹
    ğŸ¯ åˆ†é¡è©•ä¼°é€²åº¦ï¼šé¡¯ç¤ºæ¯å€‹æ³¨æ„åŠ›æ©Ÿåˆ¶çš„è©•ä¼°é€²åº¦
    ğŸ“ˆ å¯¦æ™‚çµæœé¡¯ç¤ºï¼šå³æ™‚é¡¯ç¤ºå„é …æŒ‡æ¨™çš„è¨ˆç®—çµæœ

ç¯„ä¾‹:
    python Part05_Main.py --attention data.csv        # åƒ…åˆ†ææ³¨æ„åŠ›æ©Ÿåˆ¶
    python Part05_Main.py --classify data.csv         # å®Œæ•´åˆ†é¡è©•ä¼°
    python Part05_Main.py --compare processed_data.csv
    
æ–°æ¶æ§‹ç¯„ä¾‹:
    python Part05_Main.py --show-options              # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é¸é …
    python Part05_Main.py --new-pipeline data.csv bert sentiment  # BERT+æƒ…æ„Ÿåˆ†æ
    python Part05_Main.py --new-pipeline data.csv bert lda        # BERT+LDAä¸»é¡Œå»ºæ¨¡
    python Part05_Main.py --new-pipeline data.csv cnn clustering  # CNN+èšé¡åˆ†æ
    python Part05_Main.py --new-pipeline data.csv t5 bertopic     # T5+BERTopicä¸»é¡Œå»ºæ¨¡
    python Part05_Main.py --multi-compare data.csv               # å¤šæµç¨‹è‡ªå‹•æ¯”è¼ƒ

äº¤å‰é©—è­‰ç¯„ä¾‹:
    python Part05_Main.py --cv data.csv 5                        # 5æŠ˜æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰
    python Part05_Main.py --cv data.csv 10                       # 10æŠ˜æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰
    python Part05_Main.py --simple-cv data.csv 5                 # 5æŠ˜ç°¡å–®æ¨¡å‹äº¤å‰é©—è­‰

æ¸¬è©¦é€²åº¦åŠŸèƒ½:
    python test_progress.py                           # æ¸¬è©¦é€²åº¦é¡¯ç¤ºåŠŸèƒ½

æ³¨æ„ï¼š
    - input_file æ‡‰è©²æ˜¯ç¶“éé è™•ç†çš„CSVæ–‡ä»¶
    - ç³»çµ±æœƒè‡ªå‹•æª¢æ¸¬æ–‡æœ¬æ¬„ä½ï¼ˆprocessed_text, clean_text, text, reviewï¼‰
    - --classifyé¸é …æœƒåŸ·è¡Œå®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’æµç¨‹ï¼ŒåŒ…æ‹¬åˆ†é¡å™¨è¨“ç·´å’Œè©•ä¼°
    - çµæœæœƒä¿å­˜åœ¨è‡ªå‹•ç”Ÿæˆçš„è¼¸å‡ºç›®éŒ„ä¸­
    - é€²åº¦ä¿¡æ¯æœƒåŒæ™‚é¡¯ç¤ºåœ¨çµ‚ç«¯æ©Ÿå’Œæ—¥èªŒæ–‡ä»¶ä¸­
    """
    print(help_text)

if __name__ == "__main__":
    main() 