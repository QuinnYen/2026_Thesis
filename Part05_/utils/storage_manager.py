#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å„²å­˜ç®¡ç†å·¥å…·
çµ±ä¸€ç®¡ç†runè³‡æ–™å¤¾å…§çš„æª”æ¡ˆå„²å­˜ï¼Œé¿å…é‡è¤‡å„²å­˜åŸå§‹æ•¸æ“šé›†
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import shutil
import logging

logger = logging.getLogger(__name__)

class StorageManager:
    """
    å„²å­˜ç®¡ç†å™¨
    ç¢ºä¿æ‰€æœ‰æ“ä½œçµæœéƒ½æ­£ç¢ºå„²å­˜åˆ°runè³‡æ–™å¤¾å…§ï¼Œä¸é‡è¤‡å„²å­˜åŸå§‹æ•¸æ“šé›†
    """
    
    def __init__(self, run_dir: str):
        """
        åˆå§‹åŒ–å„²å­˜ç®¡ç†å™¨
        
        Args:
            run_dir: runè³‡æ–™å¤¾è·¯å¾‘
        """
        self.run_dir = Path(run_dir)
        self.subdirs = {
            'preprocessing': self.run_dir / '01_preprocessing',
            'encoding': self.run_dir / '02_bert_encoding',
            'attention': self.run_dir / '03_attention_testing',
            'analysis': self.run_dir / '04_analysis'
        }
        
        # å‰µå»ºæ‰€æœ‰å­ç›®éŒ„
        self._ensure_directories()
        
        # åˆå§‹åŒ–æ–‡ä»¶è¨˜éŒ„
        self.file_registry = {}
        self._load_file_registry()
    
    def _ensure_directories(self):
        """ç¢ºä¿æ‰€æœ‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨"""
        for subdir in self.subdirs.values():
            subdir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ å·²ç¢ºä¿runç›®éŒ„çµæ§‹å­˜åœ¨: {self.run_dir}")
    
    def _load_file_registry(self):
        """è¼‰å…¥æ–‡ä»¶è¨˜éŒ„"""
        registry_file = self.run_dir / 'file_registry.json'
        if registry_file.exists():
            try:
                with open(registry_file, 'r', encoding='utf-8') as f:
                    self.file_registry = json.load(f)
            except Exception as e:
                logger.warning(f"è¼‰å…¥æ–‡ä»¶è¨˜éŒ„å¤±æ•—: {e}")
                self.file_registry = {}
        else:
            self.file_registry = {}
    
    def _save_file_registry(self):
        """å„²å­˜æ–‡ä»¶è¨˜éŒ„"""
        registry_file = self.run_dir / 'file_registry.json'
        try:
            with open(registry_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_registry, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å„²å­˜æ–‡ä»¶è¨˜éŒ„å¤±æ•—: {e}")
    
    def save_processed_data(self, data: pd.DataFrame, stage: str, 
                          filename: str, metadata: Optional[Dict] = None) -> str:
        """
        å„²å­˜è™•ç†éçš„æ•¸æ“šï¼ˆè€ŒéåŸå§‹æ•¸æ“šé›†ï¼‰
        
        Args:
            data: è™•ç†éçš„æ•¸æ“š
            stage: è™•ç†éšæ®µ ('preprocessing', 'encoding', 'attention', 'analysis')
            filename: æª”æ¡ˆåç¨±
            metadata: é¡å¤–çš„å…ƒæ•¸æ“šä¿¡æ¯
            
        Returns:
            str: å„²å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        if stage not in self.subdirs:
            raise ValueError(f"ç„¡æ•ˆçš„è™•ç†éšæ®µ: {stage}")
        
        # åªå„²å­˜è™•ç†å¾Œçš„çµæœï¼Œä¸å„²å­˜åŸå§‹æ•¸æ“š
        output_path = self.subdirs[stage] / filename
        
        # å„²å­˜æ•¸æ“š
        data.to_csv(output_path, index=False, encoding='utf-8')
        
        # è¨˜éŒ„æ–‡ä»¶ä¿¡æ¯
        file_info = {
            'path': str(output_path),
            'stage': stage,
            'timestamp': datetime.now().isoformat(),
            'rows': len(data),
            'columns': list(data.columns),
            'file_size': output_path.stat().st_size,
            'metadata': metadata or {}
        }
        
        self.file_registry[filename] = file_info
        self._save_file_registry()
        
        logger.info(f"âœ… å·²å„²å­˜è™•ç†æ•¸æ“šåˆ°: {output_path} ({len(data)} è¡Œ)")
        return str(output_path)
    
    def save_embeddings(self, embeddings: np.ndarray, encoder_type: str, 
                       metadata: Optional[Dict] = None) -> str:
        """
        å„²å­˜ç‰¹å¾µå‘é‡
        
        Args:
            embeddings: ç‰¹å¾µå‘é‡çŸ©é™£
            encoder_type: ç·¨ç¢¼å™¨é¡å‹
            metadata: å…ƒæ•¸æ“š
            
        Returns:
            str: å„²å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        filename = f"02_{encoder_type}_embeddings.npy"
        output_path = self.subdirs['encoding'] / filename
        
        # å„²å­˜ç‰¹å¾µå‘é‡
        np.save(output_path, embeddings)
        
        # è¨˜éŒ„æ–‡ä»¶ä¿¡æ¯
        file_info = {
            'path': str(output_path),
            'stage': 'encoding',
            'encoder_type': encoder_type,
            'timestamp': datetime.now().isoformat(),
            'shape': embeddings.shape,
            'dtype': str(embeddings.dtype),
            'file_size': output_path.stat().st_size,
            'metadata': metadata or {}
        }
        
        self.file_registry[filename] = file_info
        self._save_file_registry()
        
        logger.info(f"âœ… å·²å„²å­˜{encoder_type.upper()}ç‰¹å¾µå‘é‡: {output_path} {embeddings.shape}")
        return str(output_path)
    
    def save_analysis_results(self, results: Dict[str, Any], 
                            analysis_type: str, filename: str) -> str:
        """
        å„²å­˜åˆ†æçµæœ
        
        Args:
            results: åˆ†æçµæœ
            analysis_type: åˆ†æé¡å‹
            filename: æª”æ¡ˆåç¨±
            
        Returns:
            str: å„²å­˜çš„æª”æ¡ˆè·¯å¾‘
        """
        # æ ¹æ“šåˆ†æé¡å‹é¸æ“‡ç›®éŒ„
        if 'attention' in analysis_type.lower():
            output_path = self.subdirs['attention'] / filename
            stage = 'attention'
        else:
            output_path = self.subdirs['analysis'] / filename
            stage = 'analysis'
        
        # å„²å­˜JSONçµæœ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        # è¨˜éŒ„æ–‡ä»¶ä¿¡æ¯
        file_info = {
            'path': str(output_path),
            'stage': stage,
            'analysis_type': analysis_type,
            'timestamp': datetime.now().isoformat(),
            'file_size': output_path.stat().st_size,
            'keys': list(results.keys()) if isinstance(results, dict) else None
        }
        
        self.file_registry[filename] = file_info
        self._save_file_registry()
        
        logger.info(f"âœ… å·²å„²å­˜{analysis_type}åˆ†æçµæœ: {output_path}")
        return str(output_path)
    
    def create_input_reference(self, original_file_path: str) -> str:
        """
        å‰µå»ºåŸå§‹è¼¸å…¥æ–‡ä»¶çš„åƒè€ƒè¨˜éŒ„ï¼ˆä¸è¤‡è£½æ–‡ä»¶ï¼‰
        
        Args:
            original_file_path: åŸå§‹æ–‡ä»¶è·¯å¾‘
            
        Returns:
            str: åƒè€ƒè¨˜éŒ„è·¯å¾‘
        """
        original_path = Path(original_file_path)
        
        # å‰µå»ºåƒè€ƒæ–‡ä»¶è€Œä¸æ˜¯è¤‡è£½åŸå§‹æ–‡ä»¶
        reference_info = {
            'original_path': str(original_path.absolute()),
            'filename': original_path.name,
            'file_size': original_path.stat().st_size if original_path.exists() else 0,
            'timestamp': datetime.now().isoformat(),
            'note': 'æ­¤ç‚ºåŸå§‹æ•¸æ“šé›†çš„åƒè€ƒè¨˜éŒ„ï¼Œæœªè¤‡è£½å¯¦éš›æ–‡ä»¶ä»¥ç¯€çœç©ºé–“'
        }
        
        reference_file = self.run_dir / 'input_reference.json'
        with open(reference_file, 'w', encoding='utf-8') as f:
            json.dump(reference_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ å·²å‰µå»ºè¼¸å…¥æ–‡ä»¶åƒè€ƒ: {original_file_path}")
        return str(reference_file)
    
    def check_existing_embeddings(self, encoder_type: str) -> Optional[str]:
        """
        æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒç·¨ç¢¼å™¨çš„ç‰¹å¾µå‘é‡
        
        Args:
            encoder_type: ç·¨ç¢¼å™¨é¡å‹
            
        Returns:
            Optional[str]: å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡æ–‡ä»¶è·¯å¾‘ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å›None
        """
        filename = f"02_{encoder_type}_embeddings.npy"
        
        # å…ˆæª¢æŸ¥ç•¶å‰runç›®éŒ„
        current_path = self.subdirs['encoding'] / filename
        if current_path.exists():
            logger.info(f"ğŸ” ç™¼ç¾ç•¶å‰runä¸­çš„{encoder_type.upper()}ç‰¹å¾µå‘é‡: {current_path}")
            return str(current_path)
        
        # æª¢æŸ¥å…¶ä»–runç›®éŒ„ï¼ˆå¦‚æœéœ€è¦çš„è©±ï¼‰
        parent_dir = self.run_dir.parent
        if parent_dir.exists():
            for run_folder in parent_dir.glob('run_*'):
                if run_folder.is_dir() and run_folder != self.run_dir:
                    other_embedding = run_folder / '02_bert_encoding' / filename
                    if other_embedding.exists():
                        logger.info(f"ğŸ” åœ¨å…¶ä»–runä¸­ç™¼ç¾{encoder_type.upper()}ç‰¹å¾µå‘é‡: {other_embedding}")
                        # å¯ä»¥é¸æ“‡è¤‡è£½åˆ°ç•¶å‰runæˆ–ç›´æ¥ä½¿ç”¨
                        return str(other_embedding)
        
        return None
    
    def get_file_info(self, filename: str) -> Optional[Dict]:
        """
        ç²å–æ–‡ä»¶ä¿¡æ¯
        
        Args:
            filename: æª”æ¡ˆåç¨±
            
        Returns:
            Optional[Dict]: æ–‡ä»¶ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å›None
        """
        return self.file_registry.get(filename)
    
    def list_files_by_stage(self, stage: str) -> List[Dict]:
        """
        æŒ‰éšæ®µåˆ—å‡ºæ–‡ä»¶
        
        Args:
            stage: è™•ç†éšæ®µ
            
        Returns:
            List[Dict]: è©²éšæ®µçš„æ–‡ä»¶åˆ—è¡¨
        """
        return [info for info in self.file_registry.values() 
                if info.get('stage') == stage]
    
    def cleanup_temp_files(self):
        """æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        temp_patterns = ['temp_*', '*.tmp', '*_temp.*']
        cleaned_files = []
        
        for pattern in temp_patterns:
            for temp_file in self.run_dir.rglob(pattern):
                if temp_file.is_file():
                    try:
                        temp_file.unlink()
                        cleaned_files.append(str(temp_file))
                    except Exception as e:
                        logger.warning(f"æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•— {temp_file}: {e}")
        
        if cleaned_files:
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {len(cleaned_files)} å€‹è‡¨æ™‚æ–‡ä»¶")
        
        return cleaned_files
    
    def generate_summary_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆrunè³‡æ–™å¤¾çš„æ‘˜è¦å ±å‘Š
        
        Returns:
            Dict[str, Any]: æ‘˜è¦å ±å‘Š
        """
        summary = {
            'run_directory': str(self.run_dir),
            'created_time': datetime.now().isoformat(),
            'total_files': len(self.file_registry),
            'stages': {},
            'total_size_mb': 0,
            'file_details': []
        }
        
        # è¨ˆç®—å„éšæ®µçš„çµ±è¨ˆä¿¡æ¯
        for stage in self.subdirs.keys():
            stage_files = self.list_files_by_stage(stage)
            summary['stages'][stage] = {
                'file_count': len(stage_files),
                'files': [f['path'] for f in stage_files]
            }
        
        # è¨ˆç®—ç¸½å¤§å°
        total_size = 0
        for file_info in self.file_registry.values():
            file_size = file_info.get('file_size', 0)
            total_size += file_size
            summary['file_details'].append({
                'filename': Path(file_info['path']).name,
                'stage': file_info.get('stage'),
                'size_mb': round(file_size / (1024 * 1024), 2),
                'timestamp': file_info.get('timestamp')
            })
        
        summary['total_size_mb'] = round(total_size / (1024 * 1024), 2)
        
        # å„²å­˜æ‘˜è¦å ±å‘Š
        summary_file = self.run_dir / 'storage_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š å·²ç”Ÿæˆå„²å­˜æ‘˜è¦å ±å‘Š: {summary_file}")
        return summary
    
    def verify_storage_integrity(self) -> Dict[str, Any]:
        """
        é©—è­‰å„²å­˜å®Œæ•´æ€§
        
        Returns:
            Dict[str, Any]: é©—è­‰çµæœ
        """
        verification = {
            'verified_files': [],
            'missing_files': [],
            'corrupted_files': [],
            'total_verified': 0,
            'verification_time': datetime.now().isoformat()
        }
        
        for filename, file_info in self.file_registry.items():
            file_path = Path(file_info['path'])
            
            if not file_path.exists():
                verification['missing_files'].append({
                    'filename': filename,
                    'expected_path': str(file_path),
                    'stage': file_info.get('stage')
                })
                continue
            
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            try:
                current_size = file_path.stat().st_size
                expected_size = file_info.get('file_size', 0)
                
                if current_size != expected_size:
                    verification['corrupted_files'].append({
                        'filename': filename,
                        'path': str(file_path),
                        'expected_size': expected_size,
                        'current_size': current_size
                    })
                else:
                    verification['verified_files'].append({
                        'filename': filename,
                        'path': str(file_path),
                        'stage': file_info.get('stage')
                    })
                    verification['total_verified'] += 1
                    
            except Exception as e:
                verification['corrupted_files'].append({
                    'filename': filename,
                    'path': str(file_path),
                    'error': str(e)
                })
        
        # å„²å­˜é©—è­‰çµæœ
        verification_file = self.run_dir / 'storage_verification.json'
        with open(verification_file, 'w', encoding='utf-8') as f:
            json.dump(verification, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ” å„²å­˜é©—è­‰å®Œæˆ: {verification['total_verified']} å€‹æ–‡ä»¶æ­£å¸¸")
        if verification['missing_files']:
            logger.warning(f"âš ï¸ ç™¼ç¾ {len(verification['missing_files'])} å€‹éºå¤±æ–‡ä»¶")
        if verification['corrupted_files']:
            logger.warning(f"âš ï¸ ç™¼ç¾ {len(verification['corrupted_files'])} å€‹æå£æ–‡ä»¶")
        
        return verification 