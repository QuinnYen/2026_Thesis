"""
æ³¨æ„åŠ›è™•ç†å™¨ - è² è²¬åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œæ¯”è¼ƒ
"""

import os
import numpy as np
import pandas as pd
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from tqdm import tqdm

from .attention_analyzer import AttentionAnalyzer
from .bert_encoder import BertEncoder
from .run_manager import RunManager

# åŒ¯å…¥éŒ¯èª¤è™•ç†å·¥å…·
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.error_handler import handle_error, handle_warning, handle_info
from utils.storage_manager import StorageManager

logger = logging.getLogger(__name__)

class AttentionProcessor:
    """æ³¨æ„åŠ›æ©Ÿåˆ¶è™•ç†å™¨ï¼Œç”¨æ–¼åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†ææµç¨‹"""
    
    def __init__(self, output_dir: Optional[str] = None, config: Optional[Dict] = None, progress_callback=None, encoder_type: str = 'bert'):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            config: é…ç½®åƒæ•¸
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
            encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5, cnn, elmo)
        """
        self.output_dir = output_dir
        self.config = config or {}
        self.progress_callback = progress_callback
        self.encoder_type = encoder_type
        self.run_manager = RunManager(output_dir) if output_dir else None
        
        # åˆå§‹åŒ–å„²å­˜ç®¡ç†å™¨
        self.storage_manager = StorageManager(output_dir) if output_dir else None
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.bert_encoder = None
        self.attention_analyzer = None
        
        logger.info(f"æ³¨æ„åŠ›è™•ç†å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ {encoder_type.upper()} ç·¨ç¢¼å™¨")
        if self.storage_manager:
            logger.info("ğŸ“ å„²å­˜ç®¡ç†å™¨å·²å•Ÿç”¨ï¼Œå°‡å„ªåŒ–æª”æ¡ˆå„²å­˜")
    
    def process_with_attention(self, 
                             input_file: str,
                             attention_types: List[str] = None,
                             topics_path: Optional[str] = None,
                             attention_weights: Optional[Dict] = None,
                             save_results: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†ææµç¨‹
        
        Args:
            input_file: è¼¸å…¥çš„é è™•ç†æ•¸æ“šæ–‡ä»¶
            attention_types: è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
            attention_weights: çµ„åˆæ³¨æ„åŠ›æ¬Šé‡é…ç½®
            save_results: æ˜¯å¦ä¿å­˜çµæœ
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æçµæœ
        """
        try:
            start_time = datetime.now()
            print("ğŸ”„ é–‹å§‹æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†ææµç¨‹")
            
            # è¨­å®šé»˜èªçš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            if attention_types is None:
                attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
            
            total_steps = 6  # ç¸½å…±6å€‹ä¸»è¦æ­¥é©Ÿ
            
            # 1. è®€å–é è™•ç†æ•¸æ“š
            print(f"\nğŸ“Š æ­¥é©Ÿ 1/{total_steps}: è®€å–æ•¸æ“š...")
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': 'è®€å–é è™•ç†æ•¸æ“š',
                    'current_phase': 1,
                    'total_phases': total_steps
                })
                self.progress_callback('status', f'è®€å–æ•¸æ“š: {input_file}')
            
            logger.info(f"è®€å–æ•¸æ“š: {input_file}")
            if not os.path.exists(input_file):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶: {input_file}")
            
            df = pd.read_csv(input_file)
            print(f"âœ… æˆåŠŸè®€å– {len(df)} æ¢æ•¸æ“š")
            logger.info(f"æˆåŠŸè®€å– {len(df)} æ¢æ•¸æ“š")
            
            # å‰µå»ºè¼¸å…¥æ–‡ä»¶åƒè€ƒè¨˜éŒ„ï¼ˆä¸è¤‡è£½åŸå§‹æ•¸æ“šé›†ï¼‰
            if self.storage_manager:
                self.storage_manager.create_input_reference(input_file)
            
            if self.progress_callback:
                self.progress_callback('status', f'âœ… æˆåŠŸè®€å– {len(df)} æ¢æ•¸æ“š')
            
            # 2. æª¢æŸ¥å¿…è¦æ¬„ä½
            print(f"\nğŸ” æ­¥é©Ÿ 2/{total_steps}: æª¢æŸ¥æ•¸æ“šæ¬„ä½...")
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': 'æª¢æŸ¥æ•¸æ“šæ¬„ä½',
                    'current_phase': 2,
                    'total_phases': total_steps
                })
            
            text_column = self._find_text_column(df)
            if text_column is None:
                raise ValueError("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ¬„ä½")
            print(f"âœ… ä½¿ç”¨æ–‡æœ¬æ¬„ä½: {text_column}")
            
            if self.progress_callback:
                self.progress_callback('status', f'âœ… ä½¿ç”¨æ–‡æœ¬æ¬„ä½: {text_column}')
            
            # 3. åˆå§‹åŒ–ç·¨ç¢¼å™¨å’Œç²å–ç‰¹å¾µå‘é‡
            print(f"\nğŸ¤– æ­¥é©Ÿ 3/{total_steps}: è™•ç†{self.encoder_type.upper()}ç‰¹å¾µå‘é‡...")
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': f'è™•ç†{self.encoder_type.upper()}ç‰¹å¾µå‘é‡',
                    'current_phase': 3,
                    'total_phases': total_steps
                })
            
            embeddings = self._get_embeddings(df, text_column, self.encoder_type)
            print(f"âœ… ç‰¹å¾µå‘é‡æº–å‚™å®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            
            if self.progress_callback:
                self.progress_callback('status', f'âœ… ç‰¹å¾µå‘é‡æº–å‚™å®Œæˆ (å½¢ç‹€: {embeddings.shape})')
            
            # 4. æº–å‚™å…ƒæ•¸æ“š
            print(f"\nğŸ“‹ æ­¥é©Ÿ 4/{total_steps}: æº–å‚™å…ƒæ•¸æ“š...")
            metadata = self._prepare_metadata(df)
            print(f"âœ… å…ƒæ•¸æ“šæº–å‚™å®Œæˆ")
            
            # 5. åˆå§‹åŒ–æ³¨æ„åŠ›åˆ†æå™¨
            print(f"\nâš™ï¸ æ­¥é©Ÿ 5/{total_steps}: åˆå§‹åŒ–æ³¨æ„åŠ›åˆ†æå™¨...")
            if self.attention_analyzer is None:
                topic_labels_path = self._find_topic_labels_path()
                self.attention_analyzer = AttentionAnalyzer(
                    topic_labels_path=topic_labels_path,
                    config=self.config
                )
            print(f"âœ… æ³¨æ„åŠ›åˆ†æå™¨å·²åˆå§‹åŒ–")
            
            # 6. åŸ·è¡Œæ³¨æ„åŠ›åˆ†æ
            print(f"\nğŸ”¬ æ­¥é©Ÿ 6/{total_steps}: åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ...")
            print(f"å°‡æ¸¬è©¦ä»¥ä¸‹æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
            
            results = self.attention_analyzer.analyze_with_attention(
                embeddings=embeddings,
                metadata=metadata,
                attention_types=attention_types,
                topics_path=topics_path,
                attention_weights=attention_weights
            )
            
            # 7. æ·»åŠ è™•ç†ä¿¡æ¯
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            results['processing_info'] = {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'processing_time_seconds': processing_time,
                'input_file': input_file,
                'data_samples': len(df),
                'text_column': text_column,
                'embeddings_shape': embeddings.shape,
                'attention_types_tested': attention_types or ['no', 'similarity', 'keyword', 'self', 'combined']
            }
            
            # 8. ä¿å­˜çµæœ
            if save_results and self.output_dir:
                print(f"\nğŸ’¾ ä¿å­˜åˆ†æçµæœ...")
                if self.storage_manager:
                    # ä½¿ç”¨å„²å­˜ç®¡ç†å™¨ä¿å­˜çµæœ
                    self.storage_manager.save_analysis_results(
                        results, 
                        "attention_analysis", 
                        "attention_analysis_results.json"
                    )
                    # ç”Ÿæˆæ‘˜è¦å ±å‘Š
                    summary = self.storage_manager.generate_summary_report()
                    print(f"ğŸ“Š å·²ç”Ÿæˆå„²å­˜æ‘˜è¦å ±å‘Šï¼Œå…± {summary['total_files']} å€‹æ–‡ä»¶")
                else:
                    self._save_analysis_results(results)
                print(f"âœ… çµæœå·²ä¿å­˜è‡³: {self.output_dir}")
            
            print(f"\nğŸ‰ æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š ç¸½è€—æ™‚: {processing_time:.2f} ç§’")
            
            # é¡¯ç¤ºçµæœæ‘˜è¦
            if 'comparison' in results and 'summary' in results['comparison']:
                summary = results['comparison']['summary']
                print(f"ğŸ† æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {summary.get('best_mechanism', 'N/A')}")
                print(f"ğŸ“ˆ æœ€ä½³ç¶œåˆå¾—åˆ†: {summary.get('best_score', 0):.4f}")
            
            logger.info(f"æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
            return results
            
        except Exception as e:
            handle_error(e, "æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ", show_traceback=True)
            raise
    
    def _find_text_column(self, df: pd.DataFrame) -> Optional[str]:
        """å°‹æ‰¾æœ‰æ•ˆçš„æ–‡æœ¬æ¬„ä½"""
        text_columns = ['processed_text', 'clean_text', 'text', 'review', 'content']
        
        for col in text_columns:
            if col in df.columns and not df[col].isna().all():
                logger.info(f"ä½¿ç”¨æ–‡æœ¬æ¬„ä½: {col}")
                return col
        
        return None
    
    def _get_embeddings(self, df: pd.DataFrame, text_column: str, encoder_type: str = 'bert') -> np.ndarray:
        """ç²å–æˆ–ç”Ÿæˆæ–‡æœ¬ç‰¹å¾µå‘é‡ï¼Œæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨"""
        # ä½¿ç”¨å„²å­˜ç®¡ç†å™¨æª¢æŸ¥å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡
        embeddings_file = None
        if self.storage_manager:
            existing_path = self.storage_manager.check_existing_embeddings(encoder_type)
            if existing_path:
                embeddings_file = existing_path
        elif self.output_dir:
            # èˆŠç‰ˆæœ¬å…¼å®¹ï¼šæ‰‹å‹•æª¢æŸ¥ç·¨ç¢¼ç›®éŒ„
            if any(subdir in self.output_dir for subdir in ["01_preprocessing", "02_bert_encoding", "02_encoding", "03_attention_testing", "04_analysis"]):
                run_dir = os.path.dirname(self.output_dir)
            else:
                run_dir = self.output_dir
            
            # ä½¿ç”¨å„²å­˜ç®¡ç†å™¨ç²å–ç·¨ç¢¼å™¨ç‰¹å®šç›®éŒ„
            try:
                from ..config.paths import get_path_config
                path_config = get_path_config()
                encoding_dir_name = path_config.get_subdirectory_name("encoding", encoder_type)
                encoding_dir = os.path.join(run_dir, encoding_dir_name)
                
                if os.path.exists(encoding_dir):
                    # ä½¿ç”¨é…ç½®ç²å–æ­£ç¢ºçš„æª”æ¡ˆæ¨¡å¼
                    embeddings_pattern = path_config.get_file_pattern("embeddings", encoder_type)
                    candidate_file = os.path.join(encoding_dir, embeddings_pattern)
                    if os.path.exists(candidate_file):
                        embeddings_file = candidate_file
                        logger.info(f"æ‰¾åˆ°{encoder_type.upper()}ç‰¹å¾µå‘é‡æª”æ¡ˆ: {candidate_file}")
                        
            except Exception as e:
                logger.warning(f"ä½¿ç”¨é…ç½®æª¢æŸ¥ç‰¹å¾µå‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                # å›é€€åˆ°èˆŠçš„æª¢æŸ¥æ–¹å¼
                encoding_dirs = [
                    os.path.join(run_dir, f"02_{encoder_type}_encoding"),
                    os.path.join(run_dir, "02_bert_encoding")
                ]
                
                for encoding_dir in encoding_dirs:
                    if os.path.exists(encoding_dir):
                        possible_files = [
                            f"02_{encoder_type}_embeddings.npy",
                            "02_bert_embeddings.npy",
                            f"{encoder_type}_embeddings.npy",
                            "embeddings.npy"
                        ]
                        
                        for filename in possible_files:
                            candidate_file = os.path.join(encoding_dir, filename)
                            if os.path.exists(candidate_file):
                                embeddings_file = candidate_file
                                break
                        
                        if embeddings_file:
                            break
            
        # å¦‚æœç•¶å‰ç›®éŒ„æ²’æœ‰ï¼Œæœç´¢æ‰€æœ‰runç›®éŒ„ä¸­çš„ç‰¹å¾µå‘é‡æ–‡ä»¶
        existing_embeddings_file = None
        if not (embeddings_file and os.path.exists(embeddings_file)):
            existing_embeddings_file = self._find_existing_embeddings(encoder_type)
            
        if embeddings_file and os.path.exists(embeddings_file):
            print(f"   ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„ {encoder_type.upper()} ç‰¹å¾µå‘é‡æ–‡ä»¶ï¼Œæ­£åœ¨è¼‰å…¥...")
            logger.info(f"è¼‰å…¥å·²å­˜åœ¨çš„{encoder_type.upper()}ç‰¹å¾µå‘é‡: {embeddings_file}")
            embeddings = np.load(embeddings_file)
            print(f"   âœ… {encoder_type.upper()}ç‰¹å¾µå‘é‡è¼‰å…¥å®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            logger.info(f"{encoder_type.upper()}ç‰¹å¾µå‘é‡å½¢ç‹€: {embeddings.shape}")
        elif existing_embeddings_file:
            print(f"   ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„ {encoder_type.upper()} ç‰¹å¾µå‘é‡æ–‡ä»¶ï¼Œæ­£åœ¨è¼‰å…¥...")
            print(f"   ğŸ“ ä¾†æº: {existing_embeddings_file}")
            logger.info(f"è¼‰å…¥å·²å­˜åœ¨çš„{encoder_type.upper()}ç‰¹å¾µå‘é‡: {existing_embeddings_file}")
            embeddings = np.load(existing_embeddings_file)
            print(f"   âœ… {encoder_type.upper()}ç‰¹å¾µå‘é‡è¼‰å…¥å®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            logger.info(f"{encoder_type.upper()}ç‰¹å¾µå‘é‡å½¢ç‹€: {embeddings.shape}")
            
            # å°‡æ‰¾åˆ°çš„ç‰¹å¾µå‘é‡è¤‡è£½åˆ°ç•¶å‰ç·¨ç¢¼ç›®éŒ„ï¼Œä»¥ä¾¿å¾ŒçºŒä½¿ç”¨
            if self.output_dir and embeddings_file:
                try:
                    import shutil
                    os.makedirs(os.path.dirname(embeddings_file), exist_ok=True)
                    shutil.copy2(existing_embeddings_file, embeddings_file)
                    print(f"   ğŸ“‹ å·²è¤‡è£½{encoder_type.upper()}ç‰¹å¾µå‘é‡åˆ°ç·¨ç¢¼ç›®éŒ„: {embeddings_file}")
                    logger.info(f"å·²è¤‡è£½{encoder_type.upper()}ç‰¹å¾µå‘é‡åˆ°ç·¨ç¢¼ç›®éŒ„: {embeddings_file}")
                except Exception as e:
                    logger.warning(f"è¤‡è£½{encoder_type.upper()}ç‰¹å¾µå‘é‡æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        else:
            # ç”Ÿæˆæ–°çš„ç‰¹å¾µå‘é‡ï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
            print(f"   ğŸ”„ æœªç™¼ç¾å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡ï¼Œé–‹å§‹ç”Ÿæˆæ–°çš„ {encoder_type.upper()} ç‰¹å¾µå‘é‡...")
            logger.info(f"ç”Ÿæˆ {encoder_type.upper()} ç‰¹å¾µå‘é‡...")
            
            # æ ¹æ“šç·¨ç¢¼å™¨é¡å‹é¸æ“‡åˆé©çš„ç·¨ç¢¼å™¨
            if encoder_type == 'bert':
                if self.bert_encoder is None:
                    self.bert_encoder = BertEncoder(output_dir=self.output_dir)
                encoder = self.bert_encoder
            else:
                # å°æ–¼å…¶ä»–ç·¨ç¢¼å™¨é¡å‹ï¼Œä½¿ç”¨æ¨¡çµ„åŒ–æ¶æ§‹
                try:
                    from .text_encoders import TextEncoderFactory
                    encoder = TextEncoderFactory.create_encoder(encoder_type, output_dir=self.output_dir, progress_callback=self.progress_callback)
                except Exception as e:
                    # å¦‚æœæ¨¡çµ„åŒ–æ¶æ§‹ä¸å¯ç”¨ï¼Œå›é€€åˆ°BERT
                    print(f"   âš ï¸ {encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ ({e})ï¼Œå›é€€ä½¿ç”¨BERT")
                    logger.warning(f"{encoder_type.upper()} ç·¨ç¢¼å™¨ä¸å¯ç”¨ï¼Œå›é€€ä½¿ç”¨BERT: {e}")
                    if self.bert_encoder is None:
                        self.bert_encoder = BertEncoder(output_dir=self.output_dir)
                    encoder = self.bert_encoder
                    # æ›´æ–°ç·¨ç¢¼å™¨é¡å‹ä»¥ä¿æŒä¸€è‡´æ€§
                    self.encoder_type = 'bert'
                    encoder_type = 'bert'  # æ›´æ–°å€åŸŸè®Šæ•¸
            
            embeddings = encoder.encode(df[text_column])
            print(f"   âœ… {encoder_type.upper()} ç‰¹å¾µå‘é‡ç”Ÿæˆå®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            logger.info(f"ç”Ÿæˆçš„ç‰¹å¾µå‘é‡å½¢ç‹€: {embeddings.shape}")
        
        return embeddings
    
    def _find_existing_embeddings(self, encoder_type: str = 'bert') -> Optional[str]:
        """
        åœ¨æ‰€æœ‰runç›®éŒ„ä¸­æœç´¢å·²å­˜åœ¨çš„æ–‡æœ¬ç‰¹å¾µå‘é‡æ–‡ä»¶ï¼Œæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨
        
        Args:
            encoder_type: ç·¨ç¢¼å™¨é¡å‹ (bert, gpt, t5, cnn, elmo)
        
        Returns:
            Optional[str]: æ‰¾åˆ°çš„æœ€æ–°ç‰¹å¾µå‘é‡æ–‡ä»¶è·¯å¾‘ï¼Œå¦‚æœæ²’æ‰¾åˆ°å‰‡è¿”å›None
        """
        try:
            if not self.output_dir:
                return None
                
            # ç²å–åŸºç¤è¼¸å‡ºç›®éŒ„ï¼ˆé€šå¸¸æ˜¯outputæˆ–åŒ…å«outputçš„ç›®éŒ„ï¼‰
            base_dir = self.output_dir
            while base_dir and not base_dir.endswith('output'):
                parent_dir = os.path.dirname(base_dir)
                if parent_dir == base_dir:  # å·²ç¶“åˆ°é”æ ¹ç›®éŒ„
                    break
                base_dir = parent_dir
                
            # å¦‚æœæ²’æ‰¾åˆ°outputç›®éŒ„ï¼Œå˜—è©¦æŸ¥æ‰¾å…„å¼Ÿç›®éŒ„ä¸­çš„output
            if not base_dir.endswith('output'):
                current_parent = os.path.dirname(self.output_dir)
                potential_output = os.path.join(current_parent, 'output')
                if os.path.exists(potential_output):
                    base_dir = potential_output
                else:
                    # æœ€å¾Œå˜—è©¦åœ¨Part05_ç›®éŒ„ä¸‹æŸ¥æ‰¾output
                    part05_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                    potential_output = os.path.join(part05_dir, 'output')
                    if os.path.exists(potential_output):
                        base_dir = potential_output
                    else:
                        logger.warning(f"ç„¡æ³•æ‰¾åˆ°outputç›®éŒ„ï¼Œæœç´¢è·¯å¾‘: {base_dir}")
                        return None
            
            logger.info(f"åœ¨ç›®éŒ„ä¸­æœç´¢ {encoder_type.upper()} ç‰¹å¾µå‘é‡: {base_dir}")
            
            # æœç´¢æ‰€æœ‰run_*ç›®éŒ„
            embeddings_files = []
            if os.path.exists(base_dir):
                run_dirs = [item for item in os.listdir(base_dir) if item.startswith('run_')]
                logger.info(f"æ‰¾åˆ° {len(run_dirs)} å€‹runç›®éŒ„: {run_dirs}")
                
                # ä½¿ç”¨è·¯å¾‘é…ç½®ç³»çµ±
                try:
                    from ..config.paths import get_path_config
                    path_config = get_path_config()
                    
                    for item in run_dirs:
                        run_dir = os.path.join(base_dir, item)
                        if os.path.isdir(run_dir):
                            logger.info(f"æª¢æŸ¥runç›®éŒ„: {run_dir}")
                            
                            # ä½¿ç”¨çµ±ä¸€çš„ç·¨ç¢¼ç›®éŒ„çµæ§‹
                            try:
                                encoding_dir_name = path_config.get_subdirectory_name("encoding")
                                encoding_dir = os.path.join(run_dir, encoding_dir_name)
                                logger.info(f"æª¢æŸ¥ç·¨ç¢¼ç›®éŒ„: {encoding_dir} (å­˜åœ¨: {os.path.exists(encoding_dir)})")
                                
                                if os.path.exists(encoding_dir):
                                    embeddings_pattern = path_config.get_file_pattern("embeddings", encoder_type)
                                    embeddings_file = os.path.join(encoding_dir, embeddings_pattern)
                                    logger.info(f"æª¢æŸ¥æ–‡ä»¶: {embeddings_file} (å­˜åœ¨: {os.path.exists(embeddings_file)})")
                                    
                                    if os.path.exists(embeddings_file):
                                        logger.info(f"é©—è­‰æ–‡ä»¶: {embeddings_file}, ç·¨ç¢¼å™¨é¡å‹: {encoder_type}")
                                        if self._validate_embeddings_file(embeddings_file, encoder_type):
                                            mtime = os.path.getmtime(embeddings_file)
                                            embeddings_files.append((embeddings_file, mtime))
                                            logger.info(f"âœ… æ‰¾åˆ°ä¸¦é©—è­‰ {encoder_type.upper()} ç‰¹å¾µå‘é‡: {embeddings_file}")
                                        else:
                                            logger.info(f"âŒ æ–‡ä»¶é©—è­‰å¤±æ•—: {embeddings_file}")
                                
                            except Exception as e:
                                logger.warning(f"ä½¿ç”¨é…ç½®æª¢æŸ¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œä½¿ç”¨èˆŠæ–¹æ³•")
                                # å›é€€åˆ°èˆŠçš„æª¢æŸ¥æ–¹å¼
                                encoding_dirs = [
                                    ('02_encoding', [
                                        f'02_{encoder_type}_embeddings.npy',
                                        f'{encoder_type}_embeddings.npy',
                                        'embeddings.npy'
                                    ]),
                                    ('02_bert_encoding', [
                                        f'02_{encoder_type}_embeddings.npy',
                                        f'{encoder_type}_embeddings.npy',
                                        '02_bert_embeddings.npy',
                                        'bert_embeddings.npy'
                                    ])
                                ]
                                
                                for dir_name, file_patterns in encoding_dirs:
                                    encoding_dir = os.path.join(run_dir, dir_name)
                                    logger.info(f"æª¢æŸ¥ç·¨ç¢¼ç›®éŒ„: {encoding_dir} (å­˜åœ¨: {os.path.exists(encoding_dir)})")
                                    if os.path.exists(encoding_dir):
                                        for pattern in file_patterns:
                                            embeddings_file = os.path.join(encoding_dir, pattern)
                                            logger.info(f"æª¢æŸ¥æ–‡ä»¶: {embeddings_file} (å­˜åœ¨: {os.path.exists(embeddings_file)})")
                                            if os.path.exists(embeddings_file):
                                                logger.info(f"é©—è­‰æ–‡ä»¶: {embeddings_file}")
                                                if self._validate_embeddings_file(embeddings_file, encoder_type):
                                                    mtime = os.path.getmtime(embeddings_file)
                                                    embeddings_files.append((embeddings_file, mtime))
                                                    logger.info(f"âœ… æ‰¾åˆ°ä¸¦é©—è­‰ {encoder_type.upper()} ç‰¹å¾µå‘é‡: {embeddings_file}")
                                                else:
                                                    logger.info(f"âŒ æ–‡ä»¶é©—è­‰å¤±æ•—: {embeddings_file}")
                                                break
                    
                except ImportError as e:
                    logger.warning(f"ç„¡æ³•å°å…¥è·¯å¾‘é…ç½®æ¨¡çµ„: {e}ï¼Œä½¿ç”¨èˆŠæ–¹æ³•")
                    # å®Œå…¨å›é€€åˆ°èˆŠçš„æª¢æŸ¥æ–¹å¼
                    for item in run_dirs:
                        run_dir = os.path.join(base_dir, item)
                        if os.path.isdir(run_dir):
                            logger.info(f"æª¢æŸ¥runç›®éŒ„: {run_dir}")
                            encoding_dirs = [
                                ('02_encoding', [
                                    f'02_{encoder_type}_embeddings.npy',
                                    f'{encoder_type}_embeddings.npy',
                                    'embeddings.npy'
                                ]),
                                ('02_bert_encoding', [
                                    f'02_{encoder_type}_embeddings.npy',
                                    f'{encoder_type}_embeddings.npy',
                                    '02_bert_embeddings.npy',
                                    'bert_embeddings.npy'
                                ])
                            ]
                            
                            for dir_name, file_patterns in encoding_dirs:
                                encoding_dir = os.path.join(run_dir, dir_name)
                                logger.info(f"æª¢æŸ¥ç·¨ç¢¼ç›®éŒ„: {encoding_dir} (å­˜åœ¨: {os.path.exists(encoding_dir)})")
                                if os.path.exists(encoding_dir):
                                    for pattern in file_patterns:
                                        embeddings_file = os.path.join(encoding_dir, pattern)
                                        logger.info(f"æª¢æŸ¥æ–‡ä»¶: {embeddings_file} (å­˜åœ¨: {os.path.exists(embeddings_file)})")
                                        if os.path.exists(embeddings_file):
                                            logger.info(f"é©—è­‰æ–‡ä»¶: {embeddings_file}")
                                            if self._validate_embeddings_file(embeddings_file, encoder_type):
                                                mtime = os.path.getmtime(embeddings_file)
                                                embeddings_files.append((embeddings_file, mtime))
                                                logger.info(f"âœ… æ‰¾åˆ°ä¸¦é©—è­‰ {encoder_type.upper()} ç‰¹å¾µå‘é‡: {embeddings_file}")
                                            else:
                                                logger.info(f"âŒ æ–‡ä»¶é©—è­‰å¤±æ•—: {embeddings_file}")
                                            break
            
            # å¦‚æœæ‰¾åˆ°æ–‡ä»¶ï¼Œè¿”å›æœ€æ–°çš„
            if embeddings_files:
                # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œè¿”å›æœ€æ–°çš„
                embeddings_files.sort(key=lambda x: x[1], reverse=True)
                latest_file = embeddings_files[0][0]
                logger.info(f"é¸æ“‡æœ€æ–°çš„ {encoder_type.upper()} ç‰¹å¾µå‘é‡æ–‡ä»¶: {latest_file}")
                return latest_file
            else:
                logger.info(f"æœªæ‰¾åˆ°ä»»ä½•å·²å­˜åœ¨çš„ {encoder_type.upper()} ç‰¹å¾µå‘é‡æ–‡ä»¶")
                return None
                
        except Exception as e:
            logger.error(f"æœç´¢å·²å­˜åœ¨ {encoder_type.upper()} ç‰¹å¾µå‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def _validate_embeddings_file(self, file_path: str, encoder_type: str) -> bool:
        """é©—è­‰ç‰¹å¾µå‘é‡æª”æ¡ˆæ˜¯å¦å°æ‡‰æ­£ç¢ºçš„ç·¨ç¢¼å™¨é¡å‹"""
        try:
            # åŸºæœ¬æª”æ¡ˆåæª¢æŸ¥
            filename = os.path.basename(file_path)
            logger.info(f"é©—è­‰æ–‡ä»¶: {filename}, ç·¨ç¢¼å™¨é¡å‹: {encoder_type}")
            
            # å¦‚æœæª”æ¡ˆååŒ…å«ç·¨ç¢¼å™¨é¡å‹ï¼Œç›´æ¥æª¢æŸ¥
            if encoder_type in filename.lower():
                logger.info(f"âœ… é©—è­‰é€šé: æª”æ¡ˆååŒ…å«ç·¨ç¢¼å™¨é¡å‹ '{encoder_type}'")
                return True
            
            # å¦‚æœæ˜¯èˆŠçš„BERTæª”æ¡ˆæ ¼å¼ï¼Œåªæ¥å—BERTç·¨ç¢¼å™¨
            if 'bert' in filename.lower() and encoder_type == 'bert':
                return True
            
            # å¦‚æœæ˜¯é€šç”¨æª”æ¡ˆåï¼Œå˜—è©¦é€šéç›®éŒ„çµæ§‹æˆ–æª”æ¡ˆå…§å®¹æ¨æ–·
            if filename in ['embeddings.npy', '02_embeddings.npy']:
                # æª¢æŸ¥åŒç›®éŒ„ä¸‹æ˜¯å¦æœ‰ç·¨ç¢¼å™¨ä¿¡æ¯æª”æ¡ˆ
                dir_path = os.path.dirname(file_path)
                info_files = [
                    f'encoder_info_{encoder_type}.json',
                    f'{encoder_type}_info.json',
                    'encoder_info.json'
                ]
                
                for info_file in info_files:
                    info_path = os.path.join(dir_path, info_file)
                    if os.path.exists(info_path):
                        try:
                            import json
                            with open(info_path, 'r', encoding='utf-8') as f:
                                info = json.load(f)
                                # æª¢æŸ¥encoder_typeæˆ–å¾æª”åæ¨æ–·
                                if (info.get('encoder_type') == encoder_type or 
                                    encoder_type in info_file.lower()):
                                    return True
                        except:
                            pass
                
                # å¦‚æœæ²’æœ‰ä¿¡æ¯æª”æ¡ˆï¼Œä½†æ˜¯ç‰¹å®šçš„ç·¨ç¢¼å™¨ç›®éŒ„ï¼Œä¹Ÿæ¥å—
                if '02_bert_encoding' in file_path and encoder_type == 'bert':
                    return True
                if '02_encoding' in file_path:
                    return True  # æ–°çš„æ¨¡çµ„åŒ–æ¶æ§‹ï¼Œé€šå¸¸æ˜¯å°æ‡‰çš„ç·¨ç¢¼å™¨
            
            return False
            
        except Exception as e:
            logger.warning(f"é©—è­‰ç‰¹å¾µå‘é‡æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return True  # å¦‚æœé©—è­‰å¤±æ•—ï¼Œå°±æ¥å—æª”æ¡ˆ
    
    def _prepare_metadata(self, df: pd.DataFrame) -> pd.DataFrame:
        """æº–å‚™ç”¨æ–¼æ³¨æ„åŠ›åˆ†æçš„å…ƒæ•¸æ“š"""
        metadata = df.copy()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment' not in metadata.columns:
            # å¦‚æœæ²’æœ‰æƒ…æ„Ÿæ¨™ç±¤ï¼Œå¯ä»¥å˜—è©¦å¾å…¶ä»–æ¬„ä½æ¨æ–·æˆ–å‰µå»ºå‡è¨­æ¨™ç±¤
            if 'label' in metadata.columns:
                metadata['sentiment'] = metadata['label']
            elif 'rating' in metadata.columns:
                # åŸºæ–¼è©•åˆ†å‰µå»ºæƒ…æ„Ÿæ¨™ç±¤
                metadata['sentiment'] = metadata['rating'].apply(self._rating_to_sentiment)
            else:
                # å‰µå»ºéš¨æ©Ÿæƒ…æ„Ÿæ¨™ç±¤ç”¨æ–¼æ¸¬è©¦
                sentiments = ['positive', 'negative', 'neutral']
                metadata['sentiment'] = np.random.choice(sentiments, size=len(metadata))
                logger.warning("æœªæ‰¾åˆ°æƒ…æ„Ÿæ¨™ç±¤ï¼Œå·²å‰µå»ºéš¨æ©Ÿæ¨™ç±¤ç”¨æ–¼æ¸¬è©¦")
        
        logger.info(f"æƒ…æ„Ÿæ¨™ç±¤åˆ†å¸ƒ: {metadata['sentiment'].value_counts().to_dict()}")
        return metadata
    
    def _rating_to_sentiment(self, rating):
        """å°‡è©•åˆ†è½‰æ›ç‚ºæƒ…æ„Ÿæ¨™ç±¤"""
        if pd.isna(rating):
            return 'neutral'
        elif rating >= 4:
            return 'positive'
        elif rating <= 2:
            return 'negative'
        else:
            return 'neutral'
    
    def _find_topic_labels_path(self) -> Optional[str]:
        """å°‹æ‰¾ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶"""
        possible_paths = [
            os.path.join(self.output_dir, "topic_labels.json") if self.output_dir else None,
            "utils/topic_labels.json",
            "Part05_/utils/topic_labels.json"
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                logger.info(f"æ‰¾åˆ°ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶: {path}")
                return path
        
        logger.warning("æœªæ‰¾åˆ°ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶")
        return None
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """ä¿å­˜åˆ†æçµæœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜å®Œæ•´çµæœ
            results_file = os.path.join(self.output_dir, f"03_attention_analysis_{timestamp}.json")
            self.attention_analyzer.save_results(results, results_file)
            
            # ä¿å­˜ç°¡åŒ–çš„æ¯”è¼ƒå ±å‘Š
            if 'comparison' in results:
                report_file = os.path.join(self.output_dir, f"03_attention_comparison_{timestamp}.json")
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(results['comparison'], f, ensure_ascii=False, indent=2)
                
                logger.info(f"æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜: {report_file}")
            
            # ä¿å­˜é¢å‘å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for attention_type, result in results.items():
                if attention_type in ['processing_info', 'comparison']:
                    continue
                    
                if 'aspect_vectors' in result:
                    vectors_file = os.path.join(self.output_dir, f"03_aspect_vectors_{attention_type}_{timestamp}.npy")
                    
                    # è½‰æ›ç‚ºæ•¸çµ„æ ¼å¼ä¿å­˜
                    aspect_vectors = result['aspect_vectors']
                    if aspect_vectors:
                        vector_array = np.array(list(aspect_vectors.values()))
                        np.save(vectors_file, vector_array)
                        logger.info(f"é¢å‘å‘é‡å·²ä¿å­˜: {vectors_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def compare_attention_mechanisms(self, 
                                   attention_types: List[str] = None,
                                   input_file: str = None,
                                   topics_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å°ˆé–€ç”¨æ–¼æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœçš„æ–¹æ³•
        
        Args:
            attention_types: è¦æ¯”è¼ƒçš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            input_file: è¼¸å…¥æ•¸æ“šæ–‡ä»¶
            topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
            
        Returns:
            Dict: æ¯”è¼ƒçµæœ
        """
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
        
        logger.info(f"é–‹å§‹æ¯”è¼ƒ {len(attention_types)} ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶")
        
        # åŸ·è¡Œåˆ†æ
        results = self.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            topics_path=topics_path,
            save_results=True
        )
        
        # æå–æ¯”è¼ƒçµæœ
        comparison = results.get('comparison', {})
        
        # ç”Ÿæˆè©³ç´°æ¯”è¼ƒå ±å‘Š
        report = self._generate_comparison_report(results, attention_types)
        
        return {
            'comparison': comparison,
            'detailed_report': report,
            'processing_info': results.get('processing_info', {})
        }
    
    def _generate_comparison_report(self, results: Dict[str, Any], attention_types: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒå ±å‘Š"""
        report = {
            'summary': {},
            'detailed_metrics': {},
            'recommendations': []
        }
        
        # æ”¶é›†æŒ‡æ¨™
        metrics_data = []
        for attention_type in attention_types:
            if attention_type in results:
                metrics = results[attention_type].get('metrics', {})
                metrics_data.append({
                    'type': attention_type,
                    'coherence': metrics.get('coherence', 0),
                    'separation': metrics.get('separation', 0),
                    'combined_score': metrics.get('combined_score', 0)
                })
        
        if metrics_data:
            # æ‰¾å‡ºæœ€ä½³æ©Ÿåˆ¶
            best_combined = max(metrics_data, key=lambda x: x['combined_score'])
            best_coherence = max(metrics_data, key=lambda x: x['coherence'])
            best_separation = max(metrics_data, key=lambda x: x['separation'])
            
            report['summary'] = {
                'best_overall': best_combined['type'],
                'best_coherence': best_coherence['type'],
                'best_separation': best_separation['type'],
                'total_mechanisms_tested': len(metrics_data)
            }
            
            report['detailed_metrics'] = metrics_data
            
            # ç”Ÿæˆå»ºè­°
            if best_combined['type'] == 'combined':
                report['recommendations'].append("çµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œå»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨")
            elif best_combined['type'] == 'similarity':
                report['recommendations'].append("ç›¸ä¼¼åº¦æ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆèªç¾©ç›¸ä¼¼æ€§é‡è¦çš„ä»»å‹™")
            elif best_combined['type'] == 'keyword':
                report['recommendations'].append("é—œéµè©æ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆç‰¹å®šè¡“èªé‡è¦çš„ä»»å‹™")
            elif best_combined['type'] == 'self':
                report['recommendations'].append("è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆæ–‡æª”é–“é—œä¿‚è¤‡é›œçš„ä»»å‹™")
            
            # æ€§èƒ½å·®ç•°åˆ†æ
            scores = [item['combined_score'] for item in metrics_data]
            score_std = np.std(scores)
            if score_std < 0.05:
                report['recommendations'].append("å„æ©Ÿåˆ¶æ€§èƒ½å·®ç•°è¼ƒå°ï¼Œå¯è€ƒæ…®è¨ˆç®—æˆæœ¬é¸æ“‡ç°¡å–®æ©Ÿåˆ¶")
            else:
                report['recommendations'].append("å„æ©Ÿåˆ¶æ€§èƒ½å·®ç•°æ˜é¡¯ï¼Œå»ºè­°é¸æ“‡æœ€ä½³æ€§èƒ½çš„æ©Ÿåˆ¶")
        
        return report
    
    def load_previous_results(self, results_file: str) -> Dict[str, Any]:
        """è¼‰å…¥ä¹‹å‰çš„åˆ†æçµæœ"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            logger.info(f"æˆåŠŸè¼‰å…¥çµæœ: {results_file}")
            return results
            
        except Exception as e:
            logger.error(f"è¼‰å…¥çµæœå¤±æ•—: {str(e)}")
            return {}
    
    def export_comparison_report(self, results: Dict[str, Any], output_file: str):
        """åŒ¯å‡ºæ¯”è¼ƒå ±å‘Šç‚ºå¯è®€æ ¼å¼"""
        try:
            comparison = results.get('comparison', {})
            
            report_lines = []
            report_lines.append("=" * 60)
            report_lines.append("æ³¨æ„åŠ›æ©Ÿåˆ¶æ¯”è¼ƒå ±å‘Š")
            report_lines.append("=" * 60)
            report_lines.append("")
            
            # ç¸½é«”æ‘˜è¦
            if 'summary' in comparison:
                summary = comparison['summary']
                report_lines.append("ç¸½é«”æ‘˜è¦:")
                report_lines.append(f"  æœ€ä½³æ©Ÿåˆ¶: {summary.get('best_mechanism', 'N/A')}")
                report_lines.append(f"  æœ€ä½³å¾—åˆ†: {summary.get('best_score', 0):.4f}")
                report_lines.append(f"  æ¸¬è©¦æ©Ÿåˆ¶æ•¸: {summary.get('total_mechanisms', 0)}")
                report_lines.append("")
            
            # è©³ç´°æ’å
            rankings = ['coherence_ranking', 'separation_ranking', 'combined_ranking']
            ranking_names = ['å…§èšåº¦æ’å', 'åˆ†é›¢åº¦æ’å', 'ç¶œåˆå¾—åˆ†æ’å']
            
            for ranking, name in zip(rankings, ranking_names):
                if ranking in comparison:
                    report_lines.append(f"{name}:")
                    for i, (mechanism, score) in enumerate(comparison[ranking], 1):
                        report_lines.append(f"  {i}. {mechanism}: {score:.4f}")
                    report_lines.append("")
            
            # å¯«å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            
            logger.info(f"æ¯”è¼ƒå ±å‘Šå·²åŒ¯å‡º: {output_file}")
            
        except Exception as e:
            logger.error(f"åŒ¯å‡ºå ±å‘Šå¤±æ•—: {str(e)}")


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # æ¸¬è©¦æ³¨æ„åŠ›è™•ç†å™¨
    processor = AttentionProcessor()
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'text': [
            'This is a great product!',
            'I love this item very much.',
            'Not bad, could be better.',
            'Terrible quality, very disappointed.',
            'Worst purchase ever made.'
        ],
        'sentiment': ['positive', 'positive', 'neutral', 'negative', 'negative']
    })
    
    test_file = 'test_data.csv'
    test_data.to_csv(test_file, index=False)
    
    try:
        # åŸ·è¡Œæ¯”è¼ƒ
        results = processor.compare_attention_mechanisms(
            input_file=test_file,
            attention_types=['no', 'similarity', 'combined']
        )
        
        print("æ¯”è¼ƒå®Œæˆï¼")
        print(f"æœ€ä½³æ©Ÿåˆ¶: {results['comparison']['summary']['best_mechanism']}")
        
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        os.remove(test_file)
        
    except Exception as e:
        print(f"æ¸¬è©¦å¤±æ•—: {str(e)}")
        if os.path.exists(test_file):
            os.remove(test_file) 