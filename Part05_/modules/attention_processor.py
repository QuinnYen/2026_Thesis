"""
æ³¨æ„åŠ›è™•ç†å™¨ - è² è²¬åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå’Œæ¯”è¼ƒ
"""

import os
import numpy as np
import pandas as pd
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from tqdm import tqdm

from .attention_analyzer import AttentionAnalyzer
from .bert_encoder import BertEncoder
from .run_manager import RunManager

logger = logging.getLogger(__name__)

class AttentionProcessor:
    """æ³¨æ„åŠ›æ©Ÿåˆ¶è™•ç†å™¨ï¼Œç”¨æ–¼åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†ææµç¨‹"""
    
    def __init__(self, output_dir: Optional[str] = None, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›è™•ç†å™¨
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            config: é…ç½®åƒæ•¸
        """
        self.output_dir = output_dir
        self.config = config or {}
        self.run_manager = RunManager(output_dir) if output_dir else None
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.bert_encoder = None
        self.attention_analyzer = None
        
        logger.info("æ³¨æ„åŠ›è™•ç†å™¨å·²åˆå§‹åŒ–")
    
    def process_with_attention(self, 
                             input_file: str,
                             attention_types: List[str] = None,
                             topics_path: Optional[str] = None,
                             attention_weights: Optional[Dict] = None,
                             save_results: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†ææµç¨‹
        
        Args:
            input_file: è¼¸å…¥çš„é è™•ç†æ•¸æ“šæ–‡ä»¶
            attention_types: è¦æ¸¬è©¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
            attention_weights: çµ„åˆæ³¨æ„åŠ›æ¬Šé‡é…ç½®
            save_results: æ˜¯å¦ä¿å­˜çµæœ
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æçµæœ
        """
        try:
            start_time = datetime.now()
            logger.info("="*60)
            logger.info("é–‹å§‹æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†ææµç¨‹")
            logger.info("="*60)
            
            # è¨­å®šé»˜èªçš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            if attention_types is None:
                attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
            
            total_steps = 6  # ç¸½å…±6å€‹ä¸»è¦æ­¥é©Ÿ
            
            # 1. è®€å–é è™•ç†æ•¸æ“š
            print(f"\nğŸ“Š æ­¥é©Ÿ 1/{total_steps}: è®€å–æ•¸æ“š...")
            logger.info(f"è®€å–æ•¸æ“š: {input_file}")
            if not os.path.exists(input_file):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æ–‡ä»¶: {input_file}")
            
            df = pd.read_csv(input_file)
            print(f"âœ… æˆåŠŸè®€å– {len(df)} æ¢æ•¸æ“š")
            logger.info(f"æˆåŠŸè®€å– {len(df)} æ¢æ•¸æ“š")
            
            # 2. æª¢æŸ¥å¿…è¦æ¬„ä½
            print(f"\nğŸ” æ­¥é©Ÿ 2/{total_steps}: æª¢æŸ¥æ•¸æ“šæ¬„ä½...")
            text_column = self._find_text_column(df)
            if text_column is None:
                raise ValueError("æ‰¾ä¸åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ¬„ä½")
            print(f"âœ… ä½¿ç”¨æ–‡æœ¬æ¬„ä½: {text_column}")
            
            # 3. åˆå§‹åŒ–BERTç·¨ç¢¼å™¨å’Œç²å–ç‰¹å¾µå‘é‡
            print(f"\nğŸ¤– æ­¥é©Ÿ 3/{total_steps}: è™•ç†BERTç‰¹å¾µå‘é‡...")
            embeddings = self._get_embeddings(df, text_column)
            print(f"âœ… ç‰¹å¾µå‘é‡æº–å‚™å®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            
            # 4. æº–å‚™å…ƒæ•¸æ“š
            print(f"\nğŸ“‹ æ­¥é©Ÿ 4/{total_steps}: æº–å‚™å…ƒæ•¸æ“š...")
            metadata = self._prepare_metadata(df)
            print(f"âœ… å…ƒæ•¸æ“šæº–å‚™å®Œæˆ")
            
            # 5. åˆå§‹åŒ–æ³¨æ„åŠ›åˆ†æå™¨
            print(f"\nâš™ï¸ æ­¥é©Ÿ 5/{total_steps}: åˆå§‹åŒ–æ³¨æ„åŠ›åˆ†æå™¨...")
            if self.attention_analyzer is None:
                topic_labels_path = self._find_topic_labels_path()
                self.attention_analyzer = AttentionAnalyzer(
                    topic_labels_path=topic_labels_path,
                    config=self.config
                )
            print(f"âœ… æ³¨æ„åŠ›åˆ†æå™¨å·²åˆå§‹åŒ–")
            
            # 6. åŸ·è¡Œæ³¨æ„åŠ›åˆ†æ
            print(f"\nğŸ”¬ æ­¥é©Ÿ 6/{total_steps}: åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ...")
            print(f"å°‡æ¸¬è©¦ä»¥ä¸‹æ³¨æ„åŠ›æ©Ÿåˆ¶: {', '.join(attention_types)}")
            
            results = self.attention_analyzer.analyze_with_attention(
                embeddings=embeddings,
                metadata=metadata,
                attention_types=attention_types,
                topics_path=topics_path,
                attention_weights=attention_weights
            )
            
            # 7. æ·»åŠ è™•ç†ä¿¡æ¯
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            results['processing_info'] = {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'processing_time_seconds': processing_time,
                'input_file': input_file,
                'data_samples': len(df),
                'text_column': text_column,
                'embeddings_shape': embeddings.shape,
                'attention_types_tested': attention_types or ['no', 'similarity', 'keyword', 'self', 'combined']
            }
            
            # 8. ä¿å­˜çµæœ
            if save_results and self.output_dir:
                print(f"\nğŸ’¾ ä¿å­˜åˆ†æçµæœ...")
                self._save_analysis_results(results)
                print(f"âœ… çµæœå·²ä¿å­˜è‡³: {self.output_dir}")
            
            print(f"\nğŸ‰ æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š ç¸½è€—æ™‚: {processing_time:.2f} ç§’")
            
            # é¡¯ç¤ºçµæœæ‘˜è¦
            if 'comparison' in results and 'summary' in results['comparison']:
                summary = results['comparison']['summary']
                print(f"ğŸ† æœ€ä½³æ³¨æ„åŠ›æ©Ÿåˆ¶: {summary.get('best_mechanism', 'N/A')}")
                print(f"ğŸ“ˆ æœ€ä½³ç¶œåˆå¾—åˆ†: {summary.get('best_score', 0):.4f}")
            
            logger.info(f"æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æå®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’")
            return results
            
        except Exception as e:
            logger.error(f"æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
    
    def _find_text_column(self, df: pd.DataFrame) -> Optional[str]:
        """å°‹æ‰¾æœ‰æ•ˆçš„æ–‡æœ¬æ¬„ä½"""
        text_columns = ['processed_text', 'clean_text', 'text', 'review', 'content']
        
        for col in text_columns:
            if col in df.columns and not df[col].isna().all():
                logger.info(f"ä½¿ç”¨æ–‡æœ¬æ¬„ä½: {col}")
                return col
        
        return None
    
    def _get_embeddings(self, df: pd.DataFrame, text_column: str) -> np.ndarray:
        """ç²å–æˆ–ç”ŸæˆBERTç‰¹å¾µå‘é‡"""
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç‰¹å¾µå‘é‡æ–‡ä»¶
        embeddings_file = None
        if self.output_dir:
            embeddings_file = os.path.join(self.output_dir, "02_bert_embeddings.npy")
            
        if embeddings_file and os.path.exists(embeddings_file):
            print(f"   ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡æ–‡ä»¶ï¼Œæ­£åœ¨è¼‰å…¥...")
            logger.info(f"è¼‰å…¥å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡: {embeddings_file}")
            embeddings = np.load(embeddings_file)
            print(f"   âœ… ç‰¹å¾µå‘é‡è¼‰å…¥å®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            logger.info(f"ç‰¹å¾µå‘é‡å½¢ç‹€: {embeddings.shape}")
        else:
            # ç”Ÿæˆæ–°çš„ç‰¹å¾µå‘é‡
            print(f"   ğŸ”„ æœªç™¼ç¾å·²å­˜åœ¨çš„ç‰¹å¾µå‘é‡ï¼Œé–‹å§‹ç”Ÿæˆæ–°çš„BERTç‰¹å¾µå‘é‡...")
            logger.info("ç”ŸæˆBERTç‰¹å¾µå‘é‡...")
            if self.bert_encoder is None:
                self.bert_encoder = BertEncoder(output_dir=self.output_dir)
            
            embeddings = self.bert_encoder.encode(df[text_column])
            print(f"   âœ… BERTç‰¹å¾µå‘é‡ç”Ÿæˆå®Œæˆ (å½¢ç‹€: {embeddings.shape})")
            logger.info(f"ç”Ÿæˆçš„ç‰¹å¾µå‘é‡å½¢ç‹€: {embeddings.shape}")
        
        return embeddings
    
    def _prepare_metadata(self, df: pd.DataFrame) -> pd.DataFrame:
        """æº–å‚™ç”¨æ–¼æ³¨æ„åŠ›åˆ†æçš„å…ƒæ•¸æ“š"""
        metadata = df.copy()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment' not in metadata.columns:
            # å¦‚æœæ²’æœ‰æƒ…æ„Ÿæ¨™ç±¤ï¼Œå¯ä»¥å˜—è©¦å¾å…¶ä»–æ¬„ä½æ¨æ–·æˆ–å‰µå»ºå‡è¨­æ¨™ç±¤
            if 'label' in metadata.columns:
                metadata['sentiment'] = metadata['label']
            elif 'rating' in metadata.columns:
                # åŸºæ–¼è©•åˆ†å‰µå»ºæƒ…æ„Ÿæ¨™ç±¤
                metadata['sentiment'] = metadata['rating'].apply(self._rating_to_sentiment)
            else:
                # å‰µå»ºéš¨æ©Ÿæƒ…æ„Ÿæ¨™ç±¤ç”¨æ–¼æ¸¬è©¦
                sentiments = ['positive', 'negative', 'neutral']
                metadata['sentiment'] = np.random.choice(sentiments, size=len(metadata))
                logger.warning("æœªæ‰¾åˆ°æƒ…æ„Ÿæ¨™ç±¤ï¼Œå·²å‰µå»ºéš¨æ©Ÿæ¨™ç±¤ç”¨æ–¼æ¸¬è©¦")
        
        logger.info(f"æƒ…æ„Ÿæ¨™ç±¤åˆ†å¸ƒ: {metadata['sentiment'].value_counts().to_dict()}")
        return metadata
    
    def _rating_to_sentiment(self, rating):
        """å°‡è©•åˆ†è½‰æ›ç‚ºæƒ…æ„Ÿæ¨™ç±¤"""
        if pd.isna(rating):
            return 'neutral'
        elif rating >= 4:
            return 'positive'
        elif rating <= 2:
            return 'negative'
        else:
            return 'neutral'
    
    def _find_topic_labels_path(self) -> Optional[str]:
        """å°‹æ‰¾ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶"""
        possible_paths = [
            os.path.join(self.output_dir, "topic_labels.json") if self.output_dir else None,
            "utils/topic_labels.json",
            "Part05_/utils/topic_labels.json"
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                logger.info(f"æ‰¾åˆ°ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶: {path}")
                return path
        
        logger.warning("æœªæ‰¾åˆ°ä¸»é¡Œæ¨™ç±¤æ–‡ä»¶")
        return None
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """ä¿å­˜åˆ†æçµæœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜å®Œæ•´çµæœ
            results_file = os.path.join(self.output_dir, f"03_attention_analysis_{timestamp}.json")
            self.attention_analyzer.save_results(results, results_file)
            
            # ä¿å­˜ç°¡åŒ–çš„æ¯”è¼ƒå ±å‘Š
            if 'comparison' in results:
                report_file = os.path.join(self.output_dir, f"03_attention_comparison_{timestamp}.json")
                with open(report_file, 'w', encoding='utf-8') as f:
                    json.dump(results['comparison'], f, ensure_ascii=False, indent=2)
                
                logger.info(f"æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜: {report_file}")
            
            # ä¿å­˜é¢å‘å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for attention_type, result in results.items():
                if attention_type in ['processing_info', 'comparison']:
                    continue
                    
                if 'aspect_vectors' in result:
                    vectors_file = os.path.join(self.output_dir, f"03_aspect_vectors_{attention_type}_{timestamp}.npy")
                    
                    # è½‰æ›ç‚ºæ•¸çµ„æ ¼å¼ä¿å­˜
                    aspect_vectors = result['aspect_vectors']
                    if aspect_vectors:
                        vector_array = np.array(list(aspect_vectors.values()))
                        np.save(vectors_file, vector_array)
                        logger.info(f"é¢å‘å‘é‡å·²ä¿å­˜: {vectors_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def compare_attention_mechanisms(self, 
                                   attention_types: List[str] = None,
                                   input_file: str = None,
                                   topics_path: Optional[str] = None) -> Dict[str, Any]:
        """
        å°ˆé–€ç”¨æ–¼æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶æ•ˆæœçš„æ–¹æ³•
        
        Args:
            attention_types: è¦æ¯”è¼ƒçš„æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
            input_file: è¼¸å…¥æ•¸æ“šæ–‡ä»¶
            topics_path: é—œéµè©æ–‡ä»¶è·¯å¾‘
            
        Returns:
            Dict: æ¯”è¼ƒçµæœ
        """
        if attention_types is None:
            attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
        
        logger.info(f"é–‹å§‹æ¯”è¼ƒ {len(attention_types)} ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶")
        
        # åŸ·è¡Œåˆ†æ
        results = self.process_with_attention(
            input_file=input_file,
            attention_types=attention_types,
            topics_path=topics_path,
            save_results=True
        )
        
        # æå–æ¯”è¼ƒçµæœ
        comparison = results.get('comparison', {})
        
        # ç”Ÿæˆè©³ç´°æ¯”è¼ƒå ±å‘Š
        report = self._generate_comparison_report(results, attention_types)
        
        return {
            'comparison': comparison,
            'detailed_report': report,
            'processing_info': results.get('processing_info', {})
        }
    
    def _generate_comparison_report(self, results: Dict[str, Any], attention_types: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆè©³ç´°çš„æ¯”è¼ƒå ±å‘Š"""
        report = {
            'summary': {},
            'detailed_metrics': {},
            'recommendations': []
        }
        
        # æ”¶é›†æŒ‡æ¨™
        metrics_data = []
        for attention_type in attention_types:
            if attention_type in results:
                metrics = results[attention_type].get('metrics', {})
                metrics_data.append({
                    'type': attention_type,
                    'coherence': metrics.get('coherence', 0),
                    'separation': metrics.get('separation', 0),
                    'combined_score': metrics.get('combined_score', 0)
                })
        
        if metrics_data:
            # æ‰¾å‡ºæœ€ä½³æ©Ÿåˆ¶
            best_combined = max(metrics_data, key=lambda x: x['combined_score'])
            best_coherence = max(metrics_data, key=lambda x: x['coherence'])
            best_separation = max(metrics_data, key=lambda x: x['separation'])
            
            report['summary'] = {
                'best_overall': best_combined['type'],
                'best_coherence': best_coherence['type'],
                'best_separation': best_separation['type'],
                'total_mechanisms_tested': len(metrics_data)
            }
            
            report['detailed_metrics'] = metrics_data
            
            # ç”Ÿæˆå»ºè­°
            if best_combined['type'] == 'combined':
                report['recommendations'].append("çµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œå»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨")
            elif best_combined['type'] == 'similarity':
                report['recommendations'].append("ç›¸ä¼¼åº¦æ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆèªç¾©ç›¸ä¼¼æ€§é‡è¦çš„ä»»å‹™")
            elif best_combined['type'] == 'keyword':
                report['recommendations'].append("é—œéµè©æ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆç‰¹å®šè¡“èªé‡è¦çš„ä»»å‹™")
            elif best_combined['type'] == 'self':
                report['recommendations'].append("è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶è¡¨ç¾æœ€ä½³ï¼Œé©åˆæ–‡æª”é–“é—œä¿‚è¤‡é›œçš„ä»»å‹™")
            
            # æ€§èƒ½å·®ç•°åˆ†æ
            scores = [item['combined_score'] for item in metrics_data]
            score_std = np.std(scores)
            if score_std < 0.05:
                report['recommendations'].append("å„æ©Ÿåˆ¶æ€§èƒ½å·®ç•°è¼ƒå°ï¼Œå¯è€ƒæ…®è¨ˆç®—æˆæœ¬é¸æ“‡ç°¡å–®æ©Ÿåˆ¶")
            else:
                report['recommendations'].append("å„æ©Ÿåˆ¶æ€§èƒ½å·®ç•°æ˜é¡¯ï¼Œå»ºè­°é¸æ“‡æœ€ä½³æ€§èƒ½çš„æ©Ÿåˆ¶")
        
        return report
    
    def load_previous_results(self, results_file: str) -> Dict[str, Any]:
        """è¼‰å…¥ä¹‹å‰çš„åˆ†æçµæœ"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            logger.info(f"æˆåŠŸè¼‰å…¥çµæœ: {results_file}")
            return results
            
        except Exception as e:
            logger.error(f"è¼‰å…¥çµæœå¤±æ•—: {str(e)}")
            return {}
    
    def export_comparison_report(self, results: Dict[str, Any], output_file: str):
        """åŒ¯å‡ºæ¯”è¼ƒå ±å‘Šç‚ºå¯è®€æ ¼å¼"""
        try:
            comparison = results.get('comparison', {})
            
            report_lines = []
            report_lines.append("=" * 60)
            report_lines.append("æ³¨æ„åŠ›æ©Ÿåˆ¶æ¯”è¼ƒå ±å‘Š")
            report_lines.append("=" * 60)
            report_lines.append("")
            
            # ç¸½é«”æ‘˜è¦
            if 'summary' in comparison:
                summary = comparison['summary']
                report_lines.append("ç¸½é«”æ‘˜è¦:")
                report_lines.append(f"  æœ€ä½³æ©Ÿåˆ¶: {summary.get('best_mechanism', 'N/A')}")
                report_lines.append(f"  æœ€ä½³å¾—åˆ†: {summary.get('best_score', 0):.4f}")
                report_lines.append(f"  æ¸¬è©¦æ©Ÿåˆ¶æ•¸: {summary.get('total_mechanisms', 0)}")
                report_lines.append("")
            
            # è©³ç´°æ’å
            rankings = ['coherence_ranking', 'separation_ranking', 'combined_ranking']
            ranking_names = ['å…§èšåº¦æ’å', 'åˆ†é›¢åº¦æ’å', 'ç¶œåˆå¾—åˆ†æ’å']
            
            for ranking, name in zip(rankings, ranking_names):
                if ranking in comparison:
                    report_lines.append(f"{name}:")
                    for i, (mechanism, score) in enumerate(comparison[ranking], 1):
                        report_lines.append(f"  {i}. {mechanism}: {score:.4f}")
                    report_lines.append("")
            
            # å¯«å…¥æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(report_lines))
            
            logger.info(f"æ¯”è¼ƒå ±å‘Šå·²åŒ¯å‡º: {output_file}")
            
        except Exception as e:
            logger.error(f"åŒ¯å‡ºå ±å‘Šå¤±æ•—: {str(e)}")


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # æ¸¬è©¦æ³¨æ„åŠ›è™•ç†å™¨
    processor = AttentionProcessor()
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'text': [
            'This is a great product!',
            'I love this item very much.',
            'Not bad, could be better.',
            'Terrible quality, very disappointed.',
            'Worst purchase ever made.'
        ],
        'sentiment': ['positive', 'positive', 'neutral', 'negative', 'negative']
    })
    
    test_file = 'test_data.csv'
    test_data.to_csv(test_file, index=False)
    
    try:
        # åŸ·è¡Œæ¯”è¼ƒ
        results = processor.compare_attention_mechanisms(
            input_file=test_file,
            attention_types=['no', 'similarity', 'combined']
        )
        
        print("æ¯”è¼ƒå®Œæˆï¼")
        print(f"æœ€ä½³æ©Ÿåˆ¶: {results['comparison']['summary']['best_mechanism']}")
        
        # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
        os.remove(test_file)
        
    except Exception as e:
        print(f"æ¸¬è©¦å¤±æ•—: {str(e)}")
        if os.path.exists(test_file):
            os.remove(test_file) 