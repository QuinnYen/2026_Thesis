#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½å‹•æ…‹æ¬Šé‡å­¸ç¿’å™¨ - è‡ªå‹•å„ªåŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶çµ„åˆæ¬Šé‡
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional, Any, Callable
from abc import ABC, abstractmethod
import json
import os
from datetime import datetime

# å„ªåŒ–åº«
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from scipy.optimize import minimize, differential_evolution

# å¯é¸çš„å„ªåŒ–åº«
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    logger.warning("Optunaæœªå®‰è£ï¼Œè²è‘‰æ–¯å„ªåŒ–åŠŸèƒ½å°‡ä¸å¯ç”¨")

# æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

# é …ç›®å…§éƒ¨æ¨¡çµ„
from .attention_mechanism import create_attention_mechanism, CombinedAttention
from .sentiment_classifier import SentimentClassifier

logger = logging.getLogger(__name__)

class BaseWeightLearner(ABC):
    """æ¬Šé‡å­¸ç¿’å™¨åŸºé¡"""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.best_weights = None
        self.best_score = None
        self.learning_history = []
        
    @abstractmethod
    def learn_weights(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_val: np.ndarray, y_val: np.ndarray, 
                     metadata_train: pd.DataFrame, metadata_val: pd.DataFrame) -> Dict[str, float]:
        """å­¸ç¿’æœ€ä½³æ¬Šé‡çµ„åˆ"""
        pass
    
    @abstractmethod
    def get_name(self) -> str:
        """è¿”å›å­¸ç¿’å™¨åç¨±"""
        pass
    
    def _print_learning_results(self, method_name: str, weights: Dict[str, float], score: float):
        """åœ¨çµ‚ç«¯æ©Ÿé¡¯è‘—æ‰“å°æ¬Šé‡å­¸ç¿’çµæœ"""
        if not weights:
            print(f"\nâš ï¸  {method_name} å­¸ç¿’å¤±æ•—ï¼Œç„¡æœ‰æ•ˆæ¬Šé‡")
            return
        
        print("\n" + "ğŸ”¥" * 50)
        print("ğŸ§  æ™ºèƒ½æ¬Šé‡å­¸ç¿’å®Œæˆï¼")
        print("ğŸ”¥" * 50)
        print(f"ğŸ¯ å­¸ç¿’æ–¹æ³•: {method_name}")
        print(f"ğŸ“ˆ å­¸ç¿’åˆ†æ•¸: {score:.6f}")
        print(f"â­ æœ€ä½³æ¬Šé‡é…ç½®:")
        
        # è¨ˆç®—æ¬Šé‡çµ±è¨ˆ
        weights_list = list(weights.values())
        total_weight = sum(weights_list)
        
        # æŒ‰æ¬Šé‡å¤§å°æ’åºé¡¯ç¤º
        sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)
        
        for i, (mechanism, weight) in enumerate(sorted_weights):
            percentage = (weight / total_weight) * 100 if total_weight > 0 else 0
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i] if i < 3 else "ğŸ”¸"
            print(f"   {rank_emoji} {mechanism}: {weight:.6f} ({percentage:.2f}%)")
        
        # é¡¯ç¤ºæ¬Šé‡åˆ†å¸ƒåˆ†æ
        if len(weights_list) > 1:
            max_weight = max(weights_list)
            min_weight = min(weights_list)
            weight_range = max_weight - min_weight
            dominant_mechanism = max(weights.items(), key=lambda x: x[1])
            
            print(f"\nğŸ“Š æ¬Šé‡åˆ†å¸ƒåˆ†æ:")
            print(f"   â€¢ ä¸»å°æ©Ÿåˆ¶: {dominant_mechanism[0]} ({dominant_mechanism[1]*100:.2f}%)")
            print(f"   â€¢ æ¬Šé‡ç¯„åœ: {weight_range:.6f}")
            print(f"   â€¢ æ¬Šé‡åˆ†å¸ƒ: {'å‡è¡¡' if weight_range < 0.3 else 'åå‘' if weight_range < 0.6 else 'æ¥µç«¯åå‘'}")
        
        # é¡¯ç¤ºæ¬Šé‡å»ºè­°
        print(f"\nğŸ’¡ æ¬Šé‡æ‡‰ç”¨å»ºè­°:")
        if max(weights_list) > 0.7:
            print(f"   â€¢ ç³»çµ±åå‘å–®ä¸€æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œå»ºè­°é—œæ³¨ä¸»å°æ©Ÿåˆ¶çš„æ€§èƒ½")
        elif max(weights_list) < 0.4:
            print(f"   â€¢ æ¬Šé‡åˆ†å¸ƒå‡è¡¡ï¼Œå„æ©Ÿåˆ¶å”åŒå·¥ä½œæ•ˆæœè¼ƒå¥½")
        else:
            print(f"   â€¢ å­˜åœ¨æ˜é¡¯çš„ä¸»æ¬¡é—œä¿‚ï¼Œå»ºè­°å„ªåŒ–ä¸»å°æ©Ÿåˆ¶")
        
        print("ğŸ”¥" * 50)
        
        # è¨˜éŒ„åˆ°æ—¥èªŒ
        logger.info(f"æ™ºèƒ½æ¬Šé‡å­¸ç¿’çµæœ - {method_name}: {weights}, åˆ†æ•¸: {score:.6f}")

class GridSearchWeightLearner(BaseWeightLearner):
    """ç¶²æ ¼æœç´¢æ¬Šé‡å­¸ç¿’å™¨"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.search_space = self._create_search_space()
        
    def _create_search_space(self) -> List[Dict[str, float]]:
        """å‰µå»ºæœç´¢ç©ºé–“"""
        search_space = []
        
        # å®šç¾©å„ç¨®æ¬Šé‡çµ„åˆ
        weight_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        
        # å–®ä¸€æ©Ÿåˆ¶
        for weight in weight_values:
            if weight > 0:
                search_space.append({'similarity': weight, 'keyword': 0, 'self': 0})
                search_space.append({'similarity': 0, 'keyword': weight, 'self': 0})
                search_space.append({'similarity': 0, 'keyword': 0, 'self': weight})
        
        # é›™é‡çµ„åˆ
        for w1 in weight_values:
            for w2 in weight_values:
                if w1 + w2 == 1.0 and w1 > 0 and w2 > 0:
                    search_space.append({'similarity': w1, 'keyword': w2, 'self': 0})
                    search_space.append({'similarity': w1, 'keyword': 0, 'self': w2})
                    search_space.append({'similarity': 0, 'keyword': w1, 'self': w2})
        
        # ä¸‰é‡çµ„åˆ
        for w1 in [0.2, 0.3, 0.4, 0.5]:
            for w2 in [0.2, 0.3, 0.4, 0.5]:
                w3 = 1.0 - w1 - w2
                if 0.1 <= w3 <= 0.6 and abs(w1 + w2 + w3 - 1.0) < 1e-6:
                    search_space.append({'similarity': w1, 'keyword': w2, 'self': w3})
        
        return search_space
    
    def get_name(self) -> str:
        return "GridSearchWeightLearner"
    
    def learn_weights(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_val: np.ndarray, y_val: np.ndarray, 
                     metadata_train: pd.DataFrame, metadata_val: pd.DataFrame) -> Dict[str, float]:
        """ä½¿ç”¨ç¶²æ ¼æœç´¢å­¸ç¿’æœ€ä½³æ¬Šé‡"""
        
        logger.info(f"é–‹å§‹ç¶²æ ¼æœç´¢æ¬Šé‡å­¸ç¿’ï¼Œæœç´¢ç©ºé–“å¤§å°: {len(self.search_space)}")
        
        best_weights = None
        best_score = -1
        
        for i, weights in enumerate(self.search_space):
            try:
                # æ•¸æ“šé©—è­‰
                if X_train is None or X_val is None or metadata_train is None or metadata_val is None:
                    logger.warning("è¼¸å…¥æ•¸æ“šåŒ…å«Noneå€¼")
                    continue
                
                # è¨ˆç®—çµ„åˆæ³¨æ„åŠ›
                config = self.config or {}
                combined_attention = CombinedAttention(config)
                
                # è¨ˆç®—è¨“ç·´é›†æ³¨æ„åŠ›æ¬Šé‡
                train_attn_weights, _ = combined_attention.compute_attention(
                    X_train, metadata_train, weights=weights)
                
                if train_attn_weights is None:
                    logger.warning(f"æ¬Šé‡çµ„åˆ {weights} è¨“ç·´é›†æ³¨æ„åŠ›æ¬Šé‡è¨ˆç®—å¤±æ•—")
                    continue
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if train_attn_weights.ndim == 2:
                    train_attn_weights = train_attn_weights.mean(axis=0)
                
                # æ‡‰ç”¨æ³¨æ„åŠ›æ¬Šé‡åˆ°ç‰¹å¾µ
                weighted_X_train = X_train * train_attn_weights.reshape(-1, 1)
                
                # è¨ˆç®—é©—è­‰é›†æ³¨æ„åŠ›æ¬Šé‡  
                val_attn_weights, _ = combined_attention.compute_attention(
                    X_val, metadata_val, weights=weights)
                
                if val_attn_weights is None:
                    logger.warning(f"æ¬Šé‡çµ„åˆ {weights} é©—è­‰é›†æ³¨æ„åŠ›æ¬Šé‡è¨ˆç®—å¤±æ•—")
                    continue
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if val_attn_weights.ndim == 2:
                    val_attn_weights = val_attn_weights.mean(axis=0)
                
                weighted_X_val = X_val * val_attn_weights.reshape(-1, 1)
                
                # è¨“ç·´åˆ†é¡å™¨
                classifier = LogisticRegression(random_state=42, max_iter=1000)
                classifier.fit(weighted_X_train, y_train)
                
                # è©•ä¼°æ€§èƒ½
                y_pred = classifier.predict(weighted_X_val)
                score = f1_score(y_val, y_pred, average='weighted')
                
                # è¨˜éŒ„çµæœ
                self.learning_history.append({
                    'weights': weights.copy(),
                    'score': score,
                    'iteration': i
                })
                
                if score > best_score:
                    best_score = score
                    best_weights = weights.copy()
                
                if i % 50 == 0:
                    logger.info(f"é€²åº¦: {i}/{len(self.search_space)}, ç•¶å‰æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
                    
            except Exception as e:
                logger.warning(f"æ¬Šé‡çµ„åˆ {weights} è©•ä¼°å¤±æ•—: {str(e)}")
                import traceback
                logger.warning(f"éŒ¯èª¤è¿½è¹¤: {traceback.format_exc()}")
                continue
        
        self.best_weights = best_weights
        self.best_score = best_score
        
        logger.info(f"ç¶²æ ¼æœç´¢å®Œæˆï¼Œæœ€ä½³æ¬Šé‡: {best_weights}, æœ€ä½³åˆ†æ•¸: {best_score:.4f}")
        
        # åœ¨çµ‚ç«¯æ©Ÿé¡¯è‘—æ‰“å°å­¸ç¿’çµæœ
        self._print_learning_results("ç¶²æ ¼æœç´¢ (Grid Search)", best_weights, best_score)
        
        return best_weights

class GeneticAlgorithmWeightLearner(BaseWeightLearner):
    """éºå‚³ç®—æ³•æ¬Šé‡å­¸ç¿’å™¨"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.population_size = (config or {}).get('population_size', 50)
        self.max_generations = (config or {}).get('max_generations', 100)
        
    def get_name(self) -> str:
        return "GeneticAlgorithmWeightLearner"
    
    def learn_weights(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_val: np.ndarray, y_val: np.ndarray, 
                     metadata_train: pd.DataFrame, metadata_val: pd.DataFrame) -> Dict[str, float]:
        """ä½¿ç”¨éºå‚³ç®—æ³•å­¸ç¿’æœ€ä½³æ¬Šé‡"""
        
        def objective_function(weights_array):
            """ç›®æ¨™å‡½æ•¸ï¼šè¼¸å…¥æ¬Šé‡æ•¸çµ„ï¼Œè¿”å›è² çš„F1åˆ†æ•¸ï¼ˆå› ç‚ºminimizeæ˜¯æœ€å°åŒ–ï¼‰"""
            try:
                # æ•¸æ“šé©—è­‰
                if X_train is None or X_val is None or metadata_train is None or metadata_val is None:
                    return 1.0
                
                # æ­¸ä¸€åŒ–æ¬Šé‡
                weights_sum = np.sum(weights_array)
                if weights_sum == 0:
                    return 1.0  # è¿”å›æœ€å·®åˆ†æ•¸
                
                normalized_weights = weights_array / weights_sum
                weights_dict = {
                    'similarity': normalized_weights[0],
                    'keyword': normalized_weights[1], 
                    'self': normalized_weights[2]
                }
                
                # è¨ˆç®—çµ„åˆæ³¨æ„åŠ›
                config = self.config or {}
                combined_attention = CombinedAttention(config)
                
                # è¨ˆç®—è¨“ç·´é›†æ³¨æ„åŠ›æ¬Šé‡
                train_attn_weights, _ = combined_attention.compute_attention(
                    X_train, metadata_train, weights=weights_dict)
                
                if train_attn_weights is None:
                    return 1.0
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if train_attn_weights.ndim == 2:
                    train_attn_weights = train_attn_weights.mean(axis=0)
                
                weighted_X_train = X_train * train_attn_weights.reshape(-1, 1)
                
                # è¨ˆç®—é©—è­‰é›†æ³¨æ„åŠ›æ¬Šé‡
                val_attn_weights, _ = combined_attention.compute_attention(
                    X_val, metadata_val, weights=weights_dict)
                
                if val_attn_weights is None:
                    return 1.0
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if val_attn_weights.ndim == 2:
                    val_attn_weights = val_attn_weights.mean(axis=0)
                
                weighted_X_val = X_val * val_attn_weights.reshape(-1, 1)
                
                # è¨“ç·´åˆ†é¡å™¨
                classifier = LogisticRegression(random_state=42, max_iter=500)
                classifier.fit(weighted_X_train, y_train)
                
                # è©•ä¼°æ€§èƒ½
                y_pred = classifier.predict(weighted_X_val)
                score = f1_score(y_val, y_pred, average='weighted')
                
                return -score  # è¿”å›è² å€¼å› ç‚ºè¦æœ€å°åŒ–
                
            except Exception as e:
                logger.warning(f"ç›®æ¨™å‡½æ•¸è©•ä¼°å¤±æ•—: {str(e)}")
                import traceback
                logger.warning(f"éŒ¯èª¤è¿½è¹¤: {traceback.format_exc()}")
                return 1.0  # è¿”å›æœ€å·®åˆ†æ•¸
        
        logger.info("é–‹å§‹éºå‚³ç®—æ³•æ¬Šé‡å­¸ç¿’")
        
        # ä½¿ç”¨scipyçš„differential_evolutioné€²è¡Œå„ªåŒ–
        bounds = [(0, 1), (0, 1), (0, 1)]  # ä¸‰å€‹æ¬Šé‡çš„ç¯„åœéƒ½æ˜¯[0,1]
        
        result = differential_evolution(
            objective_function,
            bounds,
            seed=42,
            maxiter=self.max_generations,
            popsize=self.population_size,
            atol=1e-6,
            tol=1e-6
        )
        
        # æ­¸ä¸€åŒ–æœ€ä½³æ¬Šé‡
        best_weights_array = result.x / np.sum(result.x)
        best_weights = {
            'similarity': best_weights_array[0],
            'keyword': best_weights_array[1],
            'self': best_weights_array[2]
        }
        
        self.best_weights = best_weights
        self.best_score = -result.fun  # è½‰å›æ­£å€¼
        
        logger.info(f"éºå‚³ç®—æ³•å®Œæˆï¼Œæœ€ä½³æ¬Šé‡: {best_weights}, æœ€ä½³åˆ†æ•¸: {self.best_score:.4f}")
        
        # åœ¨çµ‚ç«¯æ©Ÿé¡¯è‘—æ‰“å°å­¸ç¿’çµæœ
        self._print_learning_results("éºå‚³ç®—æ³• (Genetic Algorithm)", best_weights, self.best_score)
        
        return best_weights

class BayesianOptimizationWeightLearner(BaseWeightLearner):
    """è²è‘‰æ–¯å„ªåŒ–æ¬Šé‡å­¸ç¿’å™¨"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.n_trials = (config or {}).get('n_trials', 100)
        
    def get_name(self) -> str:
        return "BayesianOptimizationWeightLearner"
    
    def learn_weights(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_val: np.ndarray, y_val: np.ndarray, 
                     metadata_train: pd.DataFrame, metadata_val: pd.DataFrame) -> Dict[str, float]:
        """ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–å­¸ç¿’æœ€ä½³æ¬Šé‡"""
        
        if not OPTUNA_AVAILABLE:
            logger.warning("Optunaæœªå®‰è£ï¼Œä½¿ç”¨ç¶²æ ¼æœç´¢æ›¿ä»£è²è‘‰æ–¯å„ªåŒ–")
            # é€€å›åˆ°ç¶²æ ¼æœç´¢
            grid_learner = GridSearchWeightLearner(self.config)
            return grid_learner.learn_weights(X_train, y_train, X_val, y_val, 
                                            metadata_train, metadata_val)
        
        def objective(trial):
            """Optunaç›®æ¨™å‡½æ•¸"""
            try:
                # æ•¸æ“šé©—è­‰
                if X_train is None or X_val is None or metadata_train is None or metadata_val is None:
                    logger.warning("è¼¸å…¥æ•¸æ“šåŒ…å«Noneå€¼")
                    return 0.0
                
                if len(X_train) == 0 or len(X_val) == 0:
                    logger.warning("è¼¸å…¥æ•¸æ“šç‚ºç©º")
                    return 0.0
                
                # æ¡æ¨£æ¬Šé‡
                w1 = trial.suggest_float('similarity', 0.0, 1.0)
                w2 = trial.suggest_float('keyword', 0.0, 1.0)
                w3 = trial.suggest_float('self', 0.0, 1.0)
                
                # æ­¸ä¸€åŒ–
                total = w1 + w2 + w3
                if total == 0:
                    return 0.0
                
                weights_dict = {
                    'similarity': w1 / total,
                    'keyword': w2 / total,
                    'self': w3 / total
                }
                
                # è¨ˆç®—çµ„åˆæ³¨æ„åŠ›
                config = self.config or {}
                combined_attention = CombinedAttention(config)
                
                # è¨ˆç®—è¨“ç·´é›†æ³¨æ„åŠ›æ¬Šé‡
                train_attn_weights, _ = combined_attention.compute_attention(
                    X_train, metadata_train, weights=weights_dict)
                
                if train_attn_weights is None:
                    logger.warning("è¨“ç·´é›†æ³¨æ„åŠ›æ¬Šé‡è¨ˆç®—å¤±æ•—")
                    return 0.0
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if train_attn_weights.ndim == 2:
                    train_attn_weights = train_attn_weights.mean(axis=0)
                
                weighted_X_train = X_train * train_attn_weights.reshape(-1, 1)
                
                # è¨ˆç®—é©—è­‰é›†æ³¨æ„åŠ›æ¬Šé‡
                val_attn_weights, _ = combined_attention.compute_attention(
                    X_val, metadata_val, weights=weights_dict)
                
                if val_attn_weights is None:
                    logger.warning("é©—è­‰é›†æ³¨æ„åŠ›æ¬Šé‡è¨ˆç®—å¤±æ•—")
                    return 0.0
                
                # ç¢ºä¿æ¬Šé‡å½¢ç‹€æ­£ç¢º
                if val_attn_weights.ndim == 2:
                    val_attn_weights = val_attn_weights.mean(axis=0)
                
                weighted_X_val = X_val * val_attn_weights.reshape(-1, 1)
                
                # è¨“ç·´åˆ†é¡å™¨
                classifier = LogisticRegression(random_state=42, max_iter=500)
                classifier.fit(weighted_X_train, y_train)
                
                # è©•ä¼°æ€§èƒ½
                y_pred = classifier.predict(weighted_X_val)
                score = f1_score(y_val, y_pred, average='weighted')
                
                return score
                
            except Exception as e:
                logger.warning(f"è²è‘‰æ–¯å„ªåŒ–ç›®æ¨™å‡½æ•¸è©•ä¼°å¤±æ•—: {str(e)}")
                import traceback
                logger.warning(f"éŒ¯èª¤è¿½è¹¤: {traceback.format_exc()}")
                return 0.0
        
        logger.info("é–‹å§‹è²è‘‰æ–¯å„ªåŒ–æ¬Šé‡å­¸ç¿’")
        
        # å‰µå»ºstudy
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=self.n_trials)
        
        # ç²å–æœ€ä½³åƒæ•¸
        best_params = study.best_params
        total = sum(best_params.values())
        
        best_weights = {
            'similarity': best_params['similarity'] / total,
            'keyword': best_params['keyword'] / total, 
            'self': best_params['self'] / total
        }
        
        self.best_weights = best_weights
        self.best_score = study.best_value
        
        logger.info(f"è²è‘‰æ–¯å„ªåŒ–å®Œæˆï¼Œæœ€ä½³æ¬Šé‡: {best_weights}, æœ€ä½³åˆ†æ•¸: {self.best_score:.4f}")
        
        # åœ¨çµ‚ç«¯æ©Ÿé¡¯è‘—æ‰“å°å­¸ç¿’çµæœ
        self._print_learning_results("è²è‘‰æ–¯å„ªåŒ– (Bayesian Optimization)", best_weights, self.best_score)
        
        return best_weights

class AdaptiveWeightLearner:
    """è‡ªé©æ‡‰æ¬Šé‡å­¸ç¿’å™¨ä¸»é¡"""
    
    def __init__(self, output_dir: Optional[str] = None, config: Optional[Dict] = None):
        self.output_dir = output_dir
        self.config = config or {}
        
        # åˆå§‹åŒ–å„ç¨®å­¸ç¿’å™¨ï¼ˆæ ¹æ“šå¯ç”¨æ€§ï¼‰
        self.learners = [
            GridSearchWeightLearner(self.config),
            GeneticAlgorithmWeightLearner(self.config)
        ]
        
        # åªæœ‰åœ¨Optunaå¯ç”¨æ™‚æ‰æ·»åŠ è²è‘‰æ–¯å„ªåŒ–å­¸ç¿’å™¨
        if OPTUNA_AVAILABLE:
            self.learners.append(BayesianOptimizationWeightLearner(self.config))
        
        self.best_learner = None
        self.learning_results = {}
        
    def learn_optimal_weights(self, X_train: np.ndarray, y_train: np.ndarray,
                            X_val: np.ndarray, y_val: np.ndarray,
                            metadata_train: pd.DataFrame, metadata_val: pd.DataFrame,
                            learner_name: str = 'auto') -> Dict[str, Any]:
        """å­¸ç¿’æœ€ä½³æ¬Šé‡çµ„åˆ
        
        Args:
            X_train: è¨“ç·´ç‰¹å¾µ
            y_train: è¨“ç·´æ¨™ç±¤
            X_val: é©—è­‰ç‰¹å¾µ
            y_val: é©—è­‰æ¨™ç±¤
            metadata_train: è¨“ç·´å…ƒæ•¸æ“š
            metadata_val: é©—è­‰å…ƒæ•¸æ“š
            learner_name: å­¸ç¿’å™¨åç¨± ('grid', 'genetic', 'bayesian', 'auto')
            
        Returns:
            Dict: åŒ…å«æœ€ä½³æ¬Šé‡å’Œå­¸ç¿’çµæœçš„å­—å…¸
        """
        
        logger.info(f"é–‹å§‹è‡ªé©æ‡‰æ¬Šé‡å­¸ç¿’ï¼Œä½¿ç”¨å­¸ç¿’å™¨: {learner_name}")
        
        if learner_name == 'auto':
            # è‡ªå‹•é¸æ“‡ï¼šæ ¹æ“šæ•¸æ“šå¤§å°é¸æ“‡åˆé©çš„å­¸ç¿’å™¨
            data_size = len(X_train)
            if data_size < 1000:
                learner_name = 'grid'  # å°æ•¸æ“šé›†ä½¿ç”¨ç¶²æ ¼æœç´¢
            elif data_size < 5000:
                learner_name = 'genetic'  # ä¸­ç­‰æ•¸æ“šé›†ä½¿ç”¨éºå‚³ç®—æ³•
            else:
                learner_name = 'bayesian'  # å¤§æ•¸æ“šé›†ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–
            
            logger.info(f"æ ¹æ“šæ•¸æ“šå¤§å° {data_size} è‡ªå‹•é¸æ“‡å­¸ç¿’å™¨: {learner_name}")
        
        # é¸æ“‡å­¸ç¿’å™¨ï¼ˆæ ¹æ“šå¯ç”¨æ€§ï¼‰
        learner_map = {
            'grid': GridSearchWeightLearner,
            'genetic': GeneticAlgorithmWeightLearner
        }
        
        # åªæœ‰åœ¨Optunaå¯ç”¨æ™‚æ‰æ·»åŠ è²è‘‰æ–¯å„ªåŒ–é¸é …
        if OPTUNA_AVAILABLE:
            learner_map['bayesian'] = BayesianOptimizationWeightLearner
        elif learner_name == 'bayesian':
            logger.warning("è²è‘‰æ–¯å„ªåŒ–ä¸å¯ç”¨ï¼Œä½¿ç”¨éºå‚³ç®—æ³•æ›¿ä»£")
            learner_name = 'genetic'
        
        if learner_name not in learner_map:
            logger.warning(f"æœªçŸ¥çš„å­¸ç¿’å™¨åç¨±: {learner_name}ï¼Œä½¿ç”¨ç¶²æ ¼æœç´¢")
            learner_name = 'grid'
        
        learner = learner_map[learner_name](self.config)
        
        # å­¸ç¿’æ¬Šé‡
        start_time = datetime.now()
        best_weights = learner.learn_weights(X_train, y_train, X_val, y_val, 
                                           metadata_train, metadata_val)
        end_time = datetime.now()
        
        # ä¿å­˜çµæœ
        results = {
            'best_weights': best_weights,
            'best_score': learner.best_score,
            'learner_name': learner.get_name(),
            'learning_time': (end_time - start_time).total_seconds(),
            'learning_history': getattr(learner, 'learning_history', []),
            'timestamp': datetime.now().isoformat()
        }
        
        self.learning_results[learner_name] = results
        self.best_learner = learner
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if self.output_dir:
            self._save_results(results, learner_name)
        
        logger.info(f"æ¬Šé‡å­¸ç¿’å®Œæˆï¼Œè€—æ™‚: {results['learning_time']:.2f}ç§’")
        
        # å¦‚æœæœ‰æœ‰æ•ˆæ¬Šé‡ï¼Œåœ¨çµ‚ç«¯æ©Ÿé¡¯ç¤ºæœ€çµ‚ç¸½çµ
        if best_weights:
            print("\n" + "â­" * 60)
            print("ğŸ‰ æ™ºèƒ½æ¬Šé‡å­¸ç¿’ä»»å‹™å®Œæˆï¼")
            print("â­" * 60)
            print(f"ğŸ ç¸½è€—æ™‚: {results['learning_time']:.2f} ç§’")
            print(f"ğŸ¯ æœ€çµ‚æ¡ç”¨æ¬Šé‡:")
            for mechanism, weight in best_weights.items():
                print(f"   ğŸ”¸ {mechanism}: {weight:.6f}")
            print(f"ğŸ“ˆ æœ€çµ‚å­¸ç¿’åˆ†æ•¸: {learner.best_score:.6f}")
            print("â­" * 60)
        
        return results
    
    def compare_learners(self, X_train: np.ndarray, y_train: np.ndarray,
                        X_val: np.ndarray, y_val: np.ndarray,
                        metadata_train: pd.DataFrame, metadata_val: pd.DataFrame) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒå­¸ç¿’å™¨çš„æ€§èƒ½"""
        
        logger.info("é–‹å§‹æ¯”è¼ƒä¸åŒæ¬Šé‡å­¸ç¿’å™¨çš„æ€§èƒ½")
        
        comparison_results = {}
        
        for learner in self.learners:
            learner_name = learner.get_name()
            
            try:
                logger.info(f"æ¸¬è©¦å­¸ç¿’å™¨: {learner_name}")
                
                start_time = datetime.now()
                best_weights = learner.learn_weights(X_train, y_train, X_val, y_val,
                                                   metadata_train, metadata_val)
                end_time = datetime.now()
                
                comparison_results[learner_name] = {
                    'best_weights': best_weights,
                    'best_score': learner.best_score,
                    'learning_time': (end_time - start_time).total_seconds(),
                    'success': True
                }
                
            except Exception as e:
                logger.error(f"å­¸ç¿’å™¨ {learner_name} å¤±æ•—: {str(e)}")
                comparison_results[learner_name] = {
                    'success': False,
                    'error': str(e)
                }
        
        # æ‰¾å‡ºæœ€ä½³å­¸ç¿’å™¨
        best_learner_name = None
        best_score = -1
        
        for name, result in comparison_results.items():
            if result.get('success', False) and result.get('best_score', -1) > best_score:
                best_score = result['best_score']
                best_learner_name = name
        
        comparison_results['summary'] = {
            'best_learner': best_learner_name,
            'best_score': best_score,
            'comparison_timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æ¯”è¼ƒçµæœ
        if self.output_dir:
            self._save_comparison_results(comparison_results)
        
        logger.info(f"å­¸ç¿’å™¨æ¯”è¼ƒå®Œæˆï¼Œæœ€ä½³å­¸ç¿’å™¨: {best_learner_name}")
        
        return comparison_results
    
    def _save_results(self, results: Dict[str, Any], learner_name: str):
        """ä¿å­˜å­¸ç¿’çµæœåˆ°æ–‡ä»¶"""
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            
            filename = f"weight_learning_{learner_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(self.output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ¬Šé‡å­¸ç¿’çµæœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¬Šé‡å­¸ç¿’çµæœå¤±æ•—: {str(e)}")
    
    def _save_comparison_results(self, comparison_results: Dict[str, Any]):
        """ä¿å­˜æ¯”è¼ƒçµæœåˆ°æ–‡ä»¶"""
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            
            filename = f"learner_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(self.output_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(comparison_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"å­¸ç¿’å™¨æ¯”è¼ƒçµæœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å­¸ç¿’å™¨æ¯”è¼ƒçµæœå¤±æ•—: {str(e)}")

def create_adaptive_weight_learner(output_dir: Optional[str] = None, 
                                 config: Optional[Dict] = None) -> AdaptiveWeightLearner:
    """å‰µå»ºè‡ªé©æ‡‰æ¬Šé‡å­¸ç¿’å™¨
    
    Args:
        output_dir: è¼¸å‡ºç›®éŒ„
        config: é…ç½®åƒæ•¸
        
    Returns:
        AdaptiveWeightLearner: è‡ªé©æ‡‰æ¬Šé‡å­¸ç¿’å™¨å¯¦ä¾‹
    """
    return AdaptiveWeightLearner(output_dir, config)