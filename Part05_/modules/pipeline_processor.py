#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµç¨‹è™•ç†å™¨æ¨¡çµ„ - æ•´åˆå¤šç¨®æ–‡æœ¬ç·¨ç¢¼å™¨å’Œåˆ†é¡æ–¹æ³•
æ”¯æ´éˆæ´»çš„æ–‡æœ¬åˆ†ææµç¨‹é…ç½®
"""

import os
import json
import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
from datetime import datetime
import time

from .text_encoders import TextEncoderFactory, BaseTextEncoder
from .classification_methods import ClassificationMethodFactory, BaseClassificationMethod

logger = logging.getLogger(__name__)

class AnalysisPipeline:
    """åˆ†ææµç¨‹ç®¡é“é¡"""
    
    def __init__(self, output_dir: Optional[str] = None, progress_callback=None):
        """
        åˆå§‹åŒ–åˆ†ææµç¨‹
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        """
        self.output_dir = output_dir
        self.progress_callback = progress_callback
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        if self.output_dir:
            os.makedirs(self.output_dir, exist_ok=True)
        
        # æµç¨‹çµ„ä»¶
        self.encoder = None
        self.classifier = None
        
        # çµæœå­˜å„²
        self.encoding_results = {}
        self.classification_results = {}
        self.pipeline_config = {}
        
        logger.info("åˆ†ææµç¨‹å·²åˆå§‹åŒ–")
    
    def configure_pipeline(self, 
                          encoder_type: str,
                          classifier_type: str,
                          encoder_config: Optional[Dict] = None,
                          classifier_config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        é…ç½®åˆ†ææµç¨‹
        
        Args:
            encoder_type: ç·¨ç¢¼å™¨é¡å‹ ('bert', 'gpt', 't5', 'cnn', 'elmo')
            classifier_type: åˆ†é¡å™¨é¡å‹ ('sentiment', 'lda', 'bertopic', 'nmf', 'clustering')
            encoder_config: ç·¨ç¢¼å™¨é…ç½®åƒæ•¸
            classifier_config: åˆ†é¡å™¨é…ç½®åƒæ•¸
            
        Returns:
            Dict: é…ç½®ä¿¡æ¯
        """
        if encoder_config is None:
            encoder_config = {}
        if classifier_config is None:
            classifier_config = {}
        
        # æ·»åŠ å…±åŒåƒæ•¸
        encoder_config.update({
            'output_dir': self.output_dir,
            'progress_callback': self.progress_callback
        })
        classifier_config.update({
            'output_dir': self.output_dir,
            'progress_callback': self.progress_callback
        })
        
        # å‰µå»ºç·¨ç¢¼å™¨
        try:
            self.encoder = TextEncoderFactory.create_encoder(encoder_type, **encoder_config)
            logger.info(f"å·²é…ç½®ç·¨ç¢¼å™¨: {encoder_type}")
        except Exception as e:
            raise ValueError(f"ç·¨ç¢¼å™¨é…ç½®å¤±æ•—: {str(e)}")
        
        # å‰µå»ºåˆ†é¡å™¨
        try:
            self.classifier = ClassificationMethodFactory.create_method(classifier_type, **classifier_config)
            logger.info(f"å·²é…ç½®åˆ†é¡å™¨: {classifier_type}")
        except Exception as e:
            raise ValueError(f"åˆ†é¡å™¨é…ç½®å¤±æ•—: {str(e)}")
        
        # ä¿å­˜é…ç½®
        self.pipeline_config = {
            'encoder_type': encoder_type,
            'classifier_type': classifier_type,
            'encoder_config': encoder_config,
            'classifier_config': classifier_config,
            'configured_at': datetime.now().isoformat()
        }
        
        # æª¢æŸ¥å…¼å®¹æ€§
        compatibility_info = self._check_compatibility()
        
        config_info = {
            'pipeline_configured': True,
            'encoder': {
                'type': encoder_type,
                'name': self.encoder.__class__.__name__,
                'embedding_dim': self.encoder.get_embedding_dim()
            },
            'classifier': {
                'type': classifier_type,
                'name': self.classifier.__class__.__name__,
                'method_name': self.classifier.get_method_name()
            },
            'compatibility': compatibility_info,
            'config': self.pipeline_config
        }
        
        self._save_config(config_info)
        
        logger.info("æµç¨‹é…ç½®å®Œæˆ")
        return config_info
    
    def _check_compatibility(self) -> Dict[str, Any]:
        """æª¢æŸ¥çµ„ä»¶å…¼å®¹æ€§"""
        compatibility = {
            'status': 'compatible',
            'warnings': [],
            'recommendations': []
        }
        
        encoder_type = self.pipeline_config['encoder_type']
        classifier_type = self.pipeline_config['classifier_type']
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦åŸå§‹æ–‡æœ¬
        text_dependent_classifiers = ['lda', 'bertopic', 'nmf']
        if classifier_type in text_dependent_classifiers:
            compatibility['requires_text'] = True
            compatibility['recommendations'].append(
                f"{classifier_type}éœ€è¦åŸå§‹æ–‡æœ¬ï¼Œè«‹ç¢ºä¿åœ¨é‹è¡Œæ™‚æä¾›æ–‡æœ¬æ•¸æ“š"
            )
        else:
            compatibility['requires_text'] = False
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¨™ç±¤
        if classifier_type == 'sentiment':
            compatibility['requires_labels'] = True
            compatibility['recommendations'].append(
                "æƒ…æ„Ÿåˆ†æéœ€è¦æ¨™ç±¤æ•¸æ“šï¼Œè«‹ç¢ºä¿æä¾›æ­£ç¢ºçš„æ¨™ç±¤"
            )
        else:
            compatibility['requires_labels'] = False
        
        # æ€§èƒ½å»ºè­°
        if encoder_type in ['gpt', 't5', 'elmo'] and classifier_type == 'bertopic':
            compatibility['warnings'].append(
                "å¤§å‹ç·¨ç¢¼å™¨èˆ‡BERTopicçµ„åˆå¯èƒ½éœ€è¦å¤§é‡è¨ˆç®—è³‡æº"
            )
        
        if encoder_type == 'cnn' and classifier_type in ['lda', 'bertopic', 'nmf']:
            compatibility['recommendations'].append(
                "CNNç·¨ç¢¼å™¨èˆ‡ä¸»é¡Œå»ºæ¨¡çµ„åˆï¼Œå»ºè­°èª¿æ•´CNNçš„å‘é‡ç¶­åº¦ä»¥ç²å¾—æ›´å¥½æ•ˆæœ"
            )
        
        return compatibility
    
    def run_pipeline(self, 
                    texts: pd.Series,
                    labels: Optional[pd.Series] = None,
                    save_intermediates: bool = True) -> Dict[str, Any]:
        """
        é‹è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            texts: è¼¸å…¥æ–‡æœ¬
            labels: æ¨™ç±¤ï¼ˆå¦‚æœéœ€è¦ï¼‰
            save_intermediates: æ˜¯å¦ä¿å­˜ä¸­é–“çµæœ
            
        Returns:
            Dict: å®Œæ•´çš„åˆ†æçµæœ
        """
        if self.encoder is None or self.classifier is None:
            raise ValueError("æµç¨‹æœªé…ç½®ï¼Œè«‹å…ˆèª¿ç”¨configure_pipelineæ–¹æ³•")
        
        start_time = time.time()
        
        # é€šçŸ¥æµç¨‹é–‹å§‹
        if self.progress_callback:
            self.progress_callback('phase', {
                'phase_name': 'é–‹å§‹åˆ†ææµç¨‹',
                'current_phase': 1,
                'total_phases': 3
            })
        
        print(f"\nğŸš€ é–‹å§‹åŸ·è¡Œåˆ†ææµç¨‹")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   â€¢ ç·¨ç¢¼å™¨: {self.pipeline_config['encoder_type']}")
        print(f"   â€¢ åˆ†é¡å™¨: {self.pipeline_config['classifier_type']}")
        print(f"   â€¢ æ–‡æœ¬æ•¸é‡: {len(texts)}")
        print("="*60)
        
        try:
            # éšæ®µ1: æ–‡æœ¬ç·¨ç¢¼
            print(f"\nğŸ”¤ éšæ®µ 1/3: æ–‡æœ¬å‘é‡åŒ–")
            print("-" * 40)
            
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': f'æ–‡æœ¬ç·¨ç¢¼ ({self.pipeline_config["encoder_type"]})',
                    'current_phase': 1,
                    'total_phases': 3
                })
            
            embeddings = self.encoder.encode(texts)
            
            self.encoding_results = {
                'encoder_type': self.pipeline_config['encoder_type'],
                'embedding_shape': embeddings.shape,
                'embedding_dim': self.encoder.get_embedding_dim(),
                'n_samples': len(texts),
                'encoding_completed_at': datetime.now().isoformat()
            }
            
            print(f"âœ… ç·¨ç¢¼å®Œæˆ - å‘é‡å½¢ç‹€: {embeddings.shape}")
            
            # éšæ®µ2: åˆ†é¡/åˆ†æ
            print(f"\nğŸ¯ éšæ®µ 2/3: {self.classifier.get_method_name()}")
            print("-" * 40)
            
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': f'åˆ†é¡åˆ†æ ({self.pipeline_config["classifier_type"]})',
                    'current_phase': 2,
                    'total_phases': 3
                })
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ–‡æœ¬å’Œæ¨™ç±¤
            fit_kwargs = {'features': embeddings}
            if self._check_compatibility()['requires_text']:
                fit_kwargs['texts'] = texts
            if self._check_compatibility()['requires_labels']:
                if labels is None:
                    raise ValueError(f"{self.classifier.get_method_name()}éœ€è¦æ¨™ç±¤æ•¸æ“š")
                fit_kwargs['labels'] = labels.values if hasattr(labels, 'values') else labels
            
            classification_result = self.classifier.fit(**fit_kwargs)
            
            self.classification_results = classification_result
            
            print(f"âœ… åˆ†æå®Œæˆ")
            
            # éšæ®µ3: çµæœæ•´åˆ
            print(f"\nğŸ“Š éšæ®µ 3/3: çµæœæ•´åˆ")
            print("-" * 40)
            
            if self.progress_callback:
                self.progress_callback('phase', {
                    'phase_name': 'çµæœæ•´åˆ',
                    'current_phase': 3,
                    'total_phases': 3
                })
            
            # æ•´åˆçµæœ
            pipeline_results = self._integrate_results(embeddings, start_time)
            
            # ä¿å­˜çµæœ
            if save_intermediates:
                self._save_pipeline_results(pipeline_results)
            
            print(f"âœ… æµç¨‹å®Œæˆ")
            print(f"â±ï¸  ç¸½ç”¨æ™‚: {pipeline_results['processing_time']:.2f}ç§’")
            print(f"ğŸ“ çµæœä¿å­˜åœ¨: {self.output_dir}")
            print("="*60)
            
            return pipeline_results
            
        except Exception as e:
            error_msg = f"æµç¨‹åŸ·è¡Œå¤±æ•—: {str(e)}"
            logger.error(error_msg)
            
            # å˜—è©¦GPUè¨˜æ†¶é«”æ¸…ç†
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
            except:
                pass
            
            raise RuntimeError(error_msg) from e
    
    def _integrate_results(self, embeddings: np.ndarray, start_time: float) -> Dict[str, Any]:
        """æ•´åˆåˆ†æçµæœ"""
        end_time = time.time()
        processing_time = end_time - start_time
        
        # åŸºæœ¬ä¿¡æ¯
        integrated_results = {
            'pipeline_info': {
                'encoder_type': self.pipeline_config['encoder_type'],
                'classifier_type': self.pipeline_config['classifier_type'],
                'pipeline_config': self.pipeline_config,
                'processing_time': processing_time,
                'completed_at': datetime.now().isoformat()
            },
            'encoding_results': self.encoding_results,
            'classification_results': self.classification_results,
            'embeddings_info': {
                'shape': embeddings.shape,
                'dtype': str(embeddings.dtype),
                'memory_usage_mb': embeddings.nbytes / (1024 * 1024)
            }
        }
        
        # æ ¹æ“šåˆ†é¡å™¨é¡å‹æ·»åŠ ç‰¹å®šä¿¡æ¯
        classifier_type = self.pipeline_config['classifier_type']
        
        if classifier_type == 'sentiment':
            # æƒ…æ„Ÿåˆ†æç‰¹å®šä¿¡æ¯
            if 'test_accuracy' in self.classification_results:
                integrated_results['summary'] = {
                    'analysis_type': 'æƒ…æ„Ÿåˆ†æ',
                    'accuracy': self.classification_results['test_accuracy'],
                    'f1_score': self.classification_results.get('f1_score', 0),
                    'n_classes': self.classification_results.get('n_classes', 0)
                }
        
        elif classifier_type in ['lda', 'bertopic', 'nmf']:
            # ä¸»é¡Œå»ºæ¨¡ç‰¹å®šä¿¡æ¯
            n_topics = self.classification_results.get('n_topics', 0)
            integrated_results['summary'] = {
                'analysis_type': 'ä¸»é¡Œå»ºæ¨¡',
                'method': classifier_type.upper(),
                'n_topics': n_topics,
                'corpus_size': self.classification_results.get('corpus_size', 0)
            }
            
            # æ·»åŠ ä¸»é¡Œè³ªé‡æŒ‡æ¨™
            if classifier_type == 'lda' and 'coherence_score' in self.classification_results:
                integrated_results['summary']['coherence_score'] = self.classification_results['coherence_score']
            elif classifier_type == 'nmf' and 'reconstruction_error' in self.classification_results:
                integrated_results['summary']['reconstruction_error'] = self.classification_results['reconstruction_error']
        
        elif classifier_type == 'clustering':
            # èšé¡åˆ†æç‰¹å®šä¿¡æ¯
            integrated_results['summary'] = {
                'analysis_type': 'èšé¡åˆ†æ',
                'clustering_method': self.classification_results.get('clustering_method', ''),
                'n_clusters': self.classification_results.get('n_clusters', 0),
                'silhouette_score': self.classification_results.get('silhouette_score', None)
            }
        
        return integrated_results
    
    def predict_new_data(self, 
                        new_texts: pd.Series,
                        save_results: bool = True) -> Dict[str, Any]:
        """
        å°æ–°æ•¸æ“šé€²è¡Œé æ¸¬
        
        Args:
            new_texts: æ–°çš„æ–‡æœ¬æ•¸æ“š
            save_results: æ˜¯å¦ä¿å­˜é æ¸¬çµæœ
            
        Returns:
            Dict: é æ¸¬çµæœ
        """
        if self.encoder is None or self.classifier is None:
            raise ValueError("æµç¨‹æœªé…ç½®æˆ–æœªè¨“ç·´ï¼Œè«‹å…ˆé‹è¡Œå®Œæ•´æµç¨‹")
        
        print(f"\nğŸ”® é–‹å§‹é æ¸¬æ–°æ•¸æ“š")
        print(f"ğŸ“Š æ–°æ•¸æ“šé‡: {len(new_texts)}")
        
        # ç·¨ç¢¼æ–°æ–‡æœ¬
        print("ğŸ”¤ ç·¨ç¢¼æ–°æ–‡æœ¬...")
        new_embeddings = self.encoder.encode(new_texts)
        
        # é æ¸¬
        print(f"ğŸ¯ åŸ·è¡Œ{self.classifier.get_method_name()}é æ¸¬...")
        predict_kwargs = {'features': new_embeddings}
        if self._check_compatibility()['requires_text']:
            predict_kwargs['texts'] = new_texts
        
        predictions = self.classifier.predict(**predict_kwargs)
        
        # æ•´åˆé æ¸¬çµæœ
        prediction_results = {
            'prediction_info': {
                'encoder_type': self.pipeline_config['encoder_type'],
                'classifier_type': self.pipeline_config['classifier_type'],
                'n_samples': len(new_texts),
                'predicted_at': datetime.now().isoformat()
            },
            'predictions': predictions,
            'embedding_info': {
                'shape': new_embeddings.shape,
                'dtype': str(new_embeddings.dtype)
            }
        }
        
        if save_results:
            self._save_prediction_results(prediction_results)
        
        print(f"âœ… é æ¸¬å®Œæˆ")
        return prediction_results
    
    def get_available_encoders(self) -> Dict[str, Any]:
        """ç²å–å¯ç”¨çš„ç·¨ç¢¼å™¨ä¿¡æ¯"""
        return TextEncoderFactory.get_encoder_info()
    
    def get_available_classifiers(self) -> Dict[str, Any]:
        """ç²å–å¯ç”¨çš„åˆ†é¡å™¨ä¿¡æ¯"""
        return ClassificationMethodFactory.get_method_info()
    
    def get_pipeline_recommendations(self, data_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ ¹æ“šæ•¸æ“šç‰¹é»æ¨è–¦æµç¨‹é…ç½®
        
        Args:
            data_info: æ•¸æ“šä¿¡æ¯ {'n_samples': int, 'has_labels': bool, 'avg_text_length': int, 'language': str}
            
        Returns:
            List[Dict]: æ¨è–¦çš„é…ç½®åˆ—è¡¨
        """
        recommendations = []
        
        n_samples = data_info.get('n_samples', 0)
        has_labels = data_info.get('has_labels', False)
        avg_length = data_info.get('avg_text_length', 100)
        
        # åŸºæ–¼æ•¸æ“šé‡çš„æ¨è–¦
        if n_samples < 1000:
            # å°æ•¸æ“šé›†
            if has_labels:
                recommendations.append({
                    'name': 'å°æ•¸æ“šé›†æƒ…æ„Ÿåˆ†æ',
                    'encoder': 'bert',
                    'classifier': 'sentiment',
                    'reason': 'å°æ•¸æ“šé›†é©åˆä½¿ç”¨é è¨“ç·´BERTé…åˆç°¡å–®åˆ†é¡å™¨',
                    'priority': 'high'
                })
            
            recommendations.append({
                'name': 'å°æ•¸æ“šé›†ä¸»é¡Œç™¼ç¾',
                'encoder': 'bert',
                'classifier': 'lda',
                'reason': 'å°æ•¸æ“šé›†é©åˆLDAé€²è¡Œä¸»é¡Œå»ºæ¨¡',
                'priority': 'medium'
            })
        
        elif n_samples < 10000:
            # ä¸­ç­‰æ•¸æ“šé›†
            if has_labels:
                recommendations.append({
                    'name': 'ä¸­ç­‰æ•¸æ“šé›†æƒ…æ„Ÿåˆ†æ',
                    'encoder': 'bert',
                    'classifier': 'sentiment',
                    'classifier_config': {'model_type': 'xgboost'},
                    'reason': 'ä¸­ç­‰æ•¸æ“šé›†å¯ä»¥ä½¿ç”¨XGBoostç²å¾—æ›´å¥½æ€§èƒ½',
                    'priority': 'high'
                })
            
            recommendations.append({
                'name': 'ä¸­ç­‰æ•¸æ“šé›†ä¸»é¡Œå»ºæ¨¡',
                'encoder': 'bert',
                'classifier': 'bertopic',
                'reason': 'BERTopicåœ¨ä¸­ç­‰æ•¸æ“šé›†ä¸Šè¡¨ç¾å„ªç•°',
                'priority': 'high'
            })
            
        else:
            # å¤§æ•¸æ“šé›†
            recommendations.append({
                'name': 'å¤§æ•¸æ“šé›†å¿«é€Ÿåˆ†æ',
                'encoder': 'cnn',
                'classifier': 'clustering',
                'reason': 'CNNç·¨ç¢¼å™¨é…åˆèšé¡åˆ†æï¼Œé©åˆå¤§æ•¸æ“šé›†å¿«é€Ÿæ¢ç´¢',
                'priority': 'medium'
            })
            
            if has_labels:
                recommendations.append({
                    'name': 'å¤§æ•¸æ“šé›†ç²¾ç¢ºåˆ†æ',
                    'encoder': 't5',
                    'classifier': 'sentiment',
                    'reason': 'T5ç·¨ç¢¼å™¨åœ¨å¤§æ•¸æ“šé›†ä¸Šå¯èƒ½ç²å¾—æ›´å¥½çš„æ³›åŒ–æ€§èƒ½',
                    'priority': 'high'
                })
        
        # åŸºæ–¼æ–‡æœ¬é•·åº¦çš„æ¨è–¦
        if avg_length > 500:
            recommendations.append({
                'name': 'é•·æ–‡æœ¬åˆ†æ',
                'encoder': 'bert',
                'encoder_config': {'model_name': 'bert-base-uncased'},
                'classifier': 'nmf',
                'reason': 'é•·æ–‡æœ¬é©åˆä½¿ç”¨NMFé€²è¡Œä¸»é¡Œåˆ†è§£',
                'priority': 'medium'
            })
        
        # ç„¡ç›£ç£å­¸ç¿’æ¨è–¦
        if not has_labels:
            recommendations.append({
                'name': 'ç„¡æ¨™ç±¤æ¢ç´¢æ€§åˆ†æ',
                'encoder': 'bert',
                'classifier': 'clustering',
                'classifier_config': {'method': 'kmeans', 'n_clusters': 5},
                'reason': 'ç„¡æ¨™ç±¤æ•¸æ“šé©åˆèšé¡åˆ†æç™¼ç¾æ•¸æ“šçµæ§‹',
                'priority': 'high'
            })
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        recommendations.sort(key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['priority']])
        
        return recommendations
    
    def _save_config(self, config_info: Dict[str, Any]):
        """ä¿å­˜é…ç½®ä¿¡æ¯"""
        if self.output_dir:
            config_file = os.path.join(self.output_dir, "pipeline_config.json")
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"é…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    
    def _save_pipeline_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµç¨‹çµæœ"""
        if self.output_dir:
            results_file = os.path.join(self.output_dir, "pipeline_results.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"æµç¨‹çµæœå·²ä¿å­˜åˆ°: {results_file}")
    
    def _save_prediction_results(self, results: Dict[str, Any]):
        """ä¿å­˜é æ¸¬çµæœ"""
        if self.output_dir:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = os.path.join(self.output_dir, f"prediction_results_{timestamp}.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"é æ¸¬çµæœå·²ä¿å­˜åˆ°: {results_file}")

class MultiPipelineComparison:
    """å¤šæµç¨‹æ¯”è¼ƒé¡"""
    
    def __init__(self, output_dir: Optional[str] = None, progress_callback=None):
        """
        åˆå§‹åŒ–å¤šæµç¨‹æ¯”è¼ƒ
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        """
        self.output_dir = output_dir
        self.progress_callback = progress_callback
        self.pipelines = {}
        self.results = {}
        
        if self.output_dir:
            os.makedirs(self.output_dir, exist_ok=True)
    
    def add_pipeline_config(self, 
                           name: str,
                           encoder_type: str,
                           classifier_type: str,
                           encoder_config: Optional[Dict] = None,
                           classifier_config: Optional[Dict] = None):
        """æ·»åŠ æµç¨‹é…ç½®"""
        self.pipelines[name] = {
            'encoder_type': encoder_type,
            'classifier_type': classifier_type,
            'encoder_config': encoder_config or {},
            'classifier_config': classifier_config or {}
        }
        logger.info(f"å·²æ·»åŠ æµç¨‹é…ç½®: {name}")
    
    def run_comparison(self, 
                      texts: pd.Series,
                      labels: Optional[pd.Series] = None,
                      test_split: float = 0.2) -> Dict[str, Any]:
        """
        é‹è¡Œå¤šæµç¨‹æ¯”è¼ƒ
        
        Args:
            texts: è¼¸å…¥æ–‡æœ¬
            labels: æ¨™ç±¤ï¼ˆå¦‚æœæœ‰ï¼‰
            test_split: æ¸¬è©¦é›†æ¯”ä¾‹ï¼ˆç”¨æ–¼æ€§èƒ½è©•ä¼°ï¼‰
            
        Returns:
            Dict: æ¯”è¼ƒçµæœ
        """
        if not self.pipelines:
            raise ValueError("æ²’æœ‰æ·»åŠ ä»»ä½•æµç¨‹é…ç½®")
        
        print(f"\nğŸ”¬ é–‹å§‹å¤šæµç¨‹æ¯”è¼ƒ")
        print(f"ğŸ“Š æµç¨‹æ•¸é‡: {len(self.pipelines)}")
        print(f"ğŸ“Š æ•¸æ“šé‡: {len(texts)}")
        print("="*60)
        
        comparison_results = {
            'comparison_info': {
                'n_pipelines': len(self.pipelines),
                'n_samples': len(texts),
                'test_split': test_split,
                'started_at': datetime.now().isoformat()
            },
            'pipeline_results': {},
            'comparison_metrics': {},
            'recommendations': {}
        }
        
        # åˆ†å‰²æ•¸æ“šï¼ˆå¦‚æœéœ€è¦è©•ä¼°ï¼‰
        if labels is not None and test_split > 0:
            from sklearn.model_selection import train_test_split
            train_texts, test_texts, train_labels, test_labels = train_test_split(
                texts, labels, test_size=test_split, random_state=42, stratify=labels
            )
            use_train_data = True
        else:
            train_texts, test_texts = texts, None
            train_labels, test_labels = labels, None
            use_train_data = False
        
        # é‹è¡Œæ¯å€‹æµç¨‹
        for i, (name, config) in enumerate(self.pipelines.items(), 1):
            print(f"\nğŸ”„ é‹è¡Œæµç¨‹ {i}/{len(self.pipelines)}: {name}")
            print(f"   ç·¨ç¢¼å™¨: {config['encoder_type']}")
            print(f"   åˆ†é¡å™¨: {config['classifier_type']}")
            
            try:
                # å‰µå»ºæµç¨‹
                pipeline_output_dir = os.path.join(self.output_dir, f"pipeline_{name}")
                pipeline = AnalysisPipeline(
                    output_dir=pipeline_output_dir,
                    progress_callback=self.progress_callback
                )
                
                # é…ç½®æµç¨‹
                pipeline.configure_pipeline(
                    encoder_type=config['encoder_type'],
                    classifier_type=config['classifier_type'],
                    encoder_config=config['encoder_config'],
                    classifier_config=config['classifier_config']
                )
                
                # é‹è¡Œæµç¨‹
                start_time = time.time()
                pipeline_result = pipeline.run_pipeline(
                    texts=train_texts,
                    labels=train_labels,
                    save_intermediates=True
                )
                
                processing_time = time.time() - start_time
                
                # è©•ä¼°ï¼ˆå¦‚æœæœ‰æ¸¬è©¦æ•¸æ“šï¼‰
                evaluation_results = {}
                if use_train_data and test_texts is not None:
                    try:
                        prediction_results = pipeline.predict_new_data(test_texts, save_results=True)
                        # é€™è£¡å¯ä»¥æ·»åŠ è©•ä¼°é‚è¼¯
                        evaluation_results = {
                            'test_completed': True,
                            'test_samples': len(test_texts)
                        }
                    except Exception as e:
                        evaluation_results = {
                            'test_completed': False,
                            'test_error': str(e)
                        }
                
                # è¨˜éŒ„çµæœ
                comparison_results['pipeline_results'][name] = {
                    'config': config,
                    'pipeline_result': pipeline_result,
                    'evaluation': evaluation_results,
                    'processing_time': processing_time,
                    'memory_usage': pipeline_result.get('embeddings_info', {}).get('memory_usage_mb', 0),
                    'status': 'success'
                }
                
                print(f"   âœ… å®Œæˆ (ç”¨æ™‚: {processing_time:.2f}ç§’)")
                
            except Exception as e:
                error_msg = str(e)
                comparison_results['pipeline_results'][name] = {
                    'config': config,
                    'status': 'failed',
                    'error': error_msg,
                    'processing_time': 0
                }
                print(f"   âŒ å¤±æ•—: {error_msg}")
                logger.error(f"æµç¨‹ {name} åŸ·è¡Œå¤±æ•—: {error_msg}")
        
        # ç”Ÿæˆæ¯”è¼ƒåˆ†æ
        comparison_results['comparison_metrics'] = self._analyze_comparison(comparison_results)
        comparison_results['recommendations'] = self._generate_recommendations(comparison_results)
        comparison_results['comparison_info']['completed_at'] = datetime.now().isoformat()
        
        # ä¿å­˜æ¯”è¼ƒçµæœ
        self._save_comparison_results(comparison_results)
        
        print(f"\nğŸ“Š æ¯”è¼ƒåˆ†æå®Œæˆ")
        print(f"ğŸ“ çµæœä¿å­˜åœ¨: {self.output_dir}")
        print("="*60)
        
        return comparison_results
    
    def _analyze_comparison(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ¯”è¼ƒçµæœ"""
        pipeline_results = results['pipeline_results']
        successful_pipelines = {k: v for k, v in pipeline_results.items() if v['status'] == 'success'}
        
        if not successful_pipelines:
            return {'error': 'æ²’æœ‰æˆåŠŸçš„æµç¨‹å¯ä¾›æ¯”è¼ƒ'}
        
        # æ€§èƒ½çµ±è¨ˆ
        processing_times = [v['processing_time'] for v in successful_pipelines.values()]
        memory_usages = [v['memory_usage'] for v in successful_pipelines.values()]
        
        comparison_metrics = {
            'successful_pipelines': len(successful_pipelines),
            'failed_pipelines': len(pipeline_results) - len(successful_pipelines),
            'performance_stats': {
                'avg_processing_time': np.mean(processing_times),
                'min_processing_time': np.min(processing_times),
                'max_processing_time': np.max(processing_times),
                'avg_memory_usage_mb': np.mean(memory_usages),
                'total_memory_usage_mb': np.sum(memory_usages)
            },
            'fastest_pipeline': min(successful_pipelines.items(), key=lambda x: x[1]['processing_time'])[0],
            'most_memory_efficient': min(successful_pipelines.items(), key=lambda x: x[1]['memory_usage'])[0]
        }
        
        return comparison_metrics
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨è–¦å»ºè­°"""
        metrics = results.get('comparison_metrics', {})
        
        if 'error' in metrics:
            return {'error': 'ç„¡æ³•ç”Ÿæˆæ¨è–¦ï¼Œæ²’æœ‰æˆåŠŸçš„æµç¨‹'}
        
        recommendations = {
            'best_overall': None,
            'fastest': metrics.get('fastest_pipeline'),
            'most_efficient': metrics.get('most_memory_efficient'),
            'suggestions': []
        }
        
        # ç¶œåˆæ¨è–¦é‚è¼¯ï¼ˆé€™è£¡å¯ä»¥æ ¹æ“šå…·é«”éœ€æ±‚èª¿æ•´ï¼‰
        pipeline_results = results['pipeline_results']
        successful_pipelines = {k: v for k, v in pipeline_results.items() if v['status'] == 'success'}
        
        if successful_pipelines:
            # ç°¡å–®çš„è©•åˆ†æ©Ÿåˆ¶ï¼ˆé€Ÿåº¦ + è¨˜æ†¶é«”æ•ˆç‡ï¼‰
            scores = {}
            for name, result in successful_pipelines.items():
                time_score = 1.0 / (result['processing_time'] + 1)  # æ™‚é–“è¶ŠçŸ­åˆ†æ•¸è¶Šé«˜
                memory_score = 1.0 / (result['memory_usage'] + 1)   # è¨˜æ†¶é«”è¶Šå°‘åˆ†æ•¸è¶Šé«˜
                scores[name] = time_score + memory_score
            
            best_pipeline = max(scores.items(), key=lambda x: x[1])[0]
            recommendations['best_overall'] = best_pipeline
            
            # ç”Ÿæˆå»ºè­°
            if metrics['performance_stats']['avg_processing_time'] > 300:  # 5åˆ†é˜
                recommendations['suggestions'].append("è€ƒæ…®ä½¿ç”¨CNNç·¨ç¢¼å™¨ä»¥æé«˜è™•ç†é€Ÿåº¦")
            
            if metrics['performance_stats']['avg_memory_usage_mb'] > 1000:  # 1GB
                recommendations['suggestions'].append("è€ƒæ…®ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹æˆ–æ‰¹é‡è™•ç†ä»¥æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨")
        
        return recommendations
    
    def _save_comparison_results(self, results: Dict[str, Any]):
        """ä¿å­˜æ¯”è¼ƒçµæœ"""
        if self.output_dir:
            results_file = os.path.join(self.output_dir, "multi_pipeline_comparison.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"æ¯”è¼ƒçµæœå·²ä¿å­˜åˆ°: {results_file}")

# ä¾¿åˆ©å‡½æ•¸
def create_simple_pipeline(encoder_type: str, 
                          classifier_type: str,
                          output_dir: Optional[str] = None,
                          **kwargs) -> AnalysisPipeline:
    """
    å¿«é€Ÿå‰µå»ºç°¡å–®çš„åˆ†ææµç¨‹
    
    Args:
        encoder_type: ç·¨ç¢¼å™¨é¡å‹
        classifier_type: åˆ†é¡å™¨é¡å‹
        output_dir: è¼¸å‡ºç›®éŒ„
        **kwargs: å…¶ä»–é…ç½®åƒæ•¸
        
    Returns:
        AnalysisPipeline: é…ç½®å¥½çš„åˆ†ææµç¨‹
    """
    pipeline = AnalysisPipeline(output_dir=output_dir)
    pipeline.configure_pipeline(
        encoder_type=encoder_type,
        classifier_type=classifier_type,
        **kwargs
    )
    return pipeline

def run_quick_analysis(texts: pd.Series,
                      labels: Optional[pd.Series] = None,
                      encoder_type: str = 'bert',
                      classifier_type: str = 'sentiment',
                      output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    å¿«é€ŸåŸ·è¡Œåˆ†æ
    
    Args:
        texts: è¼¸å…¥æ–‡æœ¬
        labels: æ¨™ç±¤ï¼ˆå¯é¸ï¼‰
        encoder_type: ç·¨ç¢¼å™¨é¡å‹
        classifier_type: åˆ†é¡å™¨é¡å‹
        output_dir: è¼¸å‡ºç›®éŒ„
        
    Returns:
        Dict: åˆ†æçµæœ
    """
    pipeline = create_simple_pipeline(encoder_type, classifier_type, output_dir)
    return pipeline.run_pipeline(texts, labels)