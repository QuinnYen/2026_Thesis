#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æƒ…æ„Ÿåˆ†é¡žå™¨æ¨¡çµ„ - åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µçš„æƒ…æ„Ÿåˆ†é¡ž
"""

import numpy as np
import pandas as pd
import logging
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.naive_bayes import GaussianNB
from tqdm import tqdm
import joblib
import os
import time
import torch
from typing import Dict, Any, Tuple, Optional, List

logger = logging.getLogger(__name__)

class SentimentClassifier:
    """åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µçš„æƒ…æ„Ÿåˆ†é¡žå™¨"""
    
    def __init__(self, output_dir: Optional[str] = None, encoder_type: str = 'bert'):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡žå™¨
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
            encoder_type: ç·¨ç¢¼å™¨é¡žåž‹ (bert, gpt, t5, cnn, elmo)
        """
        self.output_dir = output_dir
        self.encoder_type = encoder_type
        self.model = None
        self.label_encoder = LabelEncoder()
        self.feature_vectors = None
        self.labels = None
        self.model_type = 'xgboost'  # ä¿®æ”¹ï¼šé è¨­ä½¿ç”¨XGBoostï¼ˆé«˜æº–ç¢ºçŽ‡èˆ‡æ€§èƒ½ï¼‰
        
        # è‡ªå‹•åµæ¸¬GPU/CPUç’°å¢ƒ
        self.device_info = self._detect_compute_environment()
        logger.info(f"è¨ˆç®—ç’°å¢ƒ: {self.device_info['description']}")
        
        # æ”¯æŒçš„æ¨¡åž‹é¡žåž‹ - å„ªåŒ–é…ç½®
        self.available_models = self._init_models()
        
        logger.info("æƒ…æ„Ÿåˆ†é¡žå™¨å·²åˆå§‹åŒ–")
        logger.info(f"å¯ç”¨åˆ†é¡žå™¨: {list(self.available_models.keys())}")
    
    def _detect_compute_environment(self) -> Dict[str, Any]:
        """è‡ªå‹•åµæ¸¬è¨ˆç®—ç’°å¢ƒï¼ˆGPU/CPUï¼‰"""
        device_info = {
            'has_gpu': False,
            'gpu_name': None,
            'gpu_memory': None,
            'cuda_available': False,
            'device': 'cpu',
            'description': 'CPU Only'
        }
        
        try:
            # æª¢æ¸¬CUDAå’ŒGPU
            if torch.cuda.is_available():
                device_info['cuda_available'] = True
                device_info['has_gpu'] = True
                device_info['device'] = 'cuda'
                device_info['gpu_name'] = torch.cuda.get_device_name(0)
                device_info['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                device_info['description'] = f"GPU: {device_info['gpu_name']} ({device_info['gpu_memory']:.1f}GB)"
                logger.info(f"æª¢æ¸¬åˆ°GPU: {device_info['gpu_name']}")
            else:
                logger.info("æœªæª¢æ¸¬åˆ°CUDA GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
                
        except Exception as e:
            logger.warning(f"GPUæª¢æ¸¬éŽç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            
        return device_info
    
    def _init_models(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å¯ç”¨çš„æ¨¡åž‹"""
        models = {
            'logistic_regression': LogisticRegression(
                random_state=42, 
                max_iter=1000, 
                C=1.0,
                n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=100, 
                random_state=42,
                n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
            ),
            'svm_linear': SVC(
                kernel='linear', 
                C=1.0, 
                random_state=42, 
                probability=True
            ),
            'naive_bayes': GaussianNB()
        }
        
        # å˜—è©¦è¼‰å…¥XGBoost
        try:
            import xgboost as xgb
            
            # æª¢æŸ¥XGBoostç‰ˆæœ¬ä¸¦è¨˜éŒ„
            xgb_version = xgb.__version__
            logger.info(f"æª¢æ¸¬åˆ°XGBoostç‰ˆæœ¬: {xgb_version}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º2.0.0+ç‰ˆæœ¬ï¼ˆä½¿ç”¨æ–°åƒæ•¸ï¼‰
            xgb_major_version = int(xgb_version.split('.')[0])
            is_new_xgb = xgb_major_version >= 2
            
            if is_new_xgb:
                logger.info("ä½¿ç”¨XGBoost 2.0.0+æ–°ç‰ˆæœ¬åƒæ•¸é…ç½®")
            else:
                logger.info("ä½¿ç”¨XGBoost 1.xç‰ˆæœ¬åƒæ•¸é…ç½®")
            
            # GPUåŠ é€Ÿé…ç½® - æ ¹æ“šç‰ˆæœ¬å’ŒGPUå¯ç”¨æ€§é…ç½®åƒæ•¸
            if self.device_info['has_gpu']:
                if is_new_xgb:
                    # XGBoost 2.0.0+ GPUé…ç½® - ä½¿ç”¨æ–°åƒæ•¸
                    xgb_params = {
                        'tree_method': 'hist',      # æ–°ç‰ˆæœ¬çµ±ä¸€ä½¿ç”¨ 'hist'
                        'device': 'cuda',           # ä½¿ç”¨ 'device' æ›¿ä»£ 'gpu_id'
                        'n_estimators': 100,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                    logger.info("XGBoosté…ç½®ç‚ºGPUæ¨¡å¼ (v2.0+: device='cuda', tree_method='hist')")
                else:
                    # XGBoost 1.x GPUé…ç½® - ä½¿ç”¨èˆŠåƒæ•¸
                    xgb_params = {
                        'tree_method': 'gpu_hist',  # èˆŠç‰ˆæœ¬ä½¿ç”¨ 'gpu_hist'
                        'gpu_id': 0,                # èˆŠç‰ˆæœ¬ä½¿ç”¨ 'gpu_id'
                        'n_estimators': 100,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                    logger.info("XGBoosté…ç½®ç‚ºGPUæ¨¡å¼ (v1.x: gpu_id=0, tree_method='gpu_hist')")
                
                logger.info("ðŸš€ GPUåŠ é€Ÿå·²å•Ÿç”¨ - BERTå’ŒXGBoostéƒ½å°‡ä½¿ç”¨GPUåŠ é€Ÿ")
            else:
                # CPUé…ç½®ï¼ˆå…©å€‹ç‰ˆæœ¬éƒ½ä¸€æ¨£ï¼‰
                xgb_params = {
                    'tree_method': 'hist',      # CPUä¸Šæœ€å¿«çš„æ–¹æ³•
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'random_state': 42,
                    'n_jobs': -1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
                }
                if is_new_xgb:
                    xgb_params['device'] = 'cpu'  # æ–°ç‰ˆæœ¬æ˜Žç¢ºæŒ‡å®šCPUè¨­å‚™
                    logger.info("XGBoosté…ç½®ç‚ºCPUæ¨¡å¼ (v2.0+: device='cpu')")
                else:
                    logger.info("XGBoosté…ç½®ç‚ºCPUæ¨¡å¼ (v1.x: tree_method='hist')")
            
            models['xgboost'] = xgb.XGBClassifier(**xgb_params)
            logger.info("XGBoostå·²æˆåŠŸè¼‰å…¥ä¸¦é…ç½®")
            
        except ImportError:
            logger.warning("XGBoostæœªå®‰è£ï¼Œè«‹ä½¿ç”¨ 'pip install xgboost' å®‰è£")
        except Exception as e:
            logger.error(f"XGBooståˆå§‹åŒ–å¤±æ•—: {str(e)}")
        
        return models
    
    def get_available_models(self) -> List[str]:
        """ç²å–å¯ç”¨çš„æ¨¡åž‹åˆ—è¡¨"""
        return list(self.available_models.keys())
    
    def get_device_info(self) -> Dict[str, Any]:
        """ç²å–è¨­å‚™ä¿¡æ¯"""
        return self.device_info.copy()
    
    def _get_naive_bayes_classifier(self):
        """ç²å–Naive Bayesåˆ†é¡žå™¨"""
        return GaussianNB()
    
    def prepare_features(self, aspect_vectors: Dict, metadata: pd.DataFrame, 
                        original_embeddings: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        æº–å‚™åˆ†é¡žç‰¹å¾µ
        
        Args:
            aspect_vectors: é¢å‘ç‰¹å¾µå‘é‡å­—å…¸
            metadata: åŒ…å«çœŸå¯¦æ¨™ç±¤çš„å…ƒæ•¸æ“š
            original_embeddings: åŽŸå§‹BERTåµŒå…¥å‘é‡ï¼ˆä¿®æ­£ï¼šæ–°å¢žæ­¤åƒæ•¸ï¼‰
            
        Returns:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
        """
        start_time = time.time()
        logger.info("é–‹å§‹æº–å‚™åˆ†é¡žç‰¹å¾µ...")
        
        # æª¢æŸ¥æƒ…æ„Ÿæ¨™ç±¤æ¬„ä½
        if 'sentiment' not in metadata.columns:
            raise ValueError("å…ƒæ•¸æ“šä¸­ç¼ºå°‘ 'sentiment' æ¬„ä½")
        
        # æå–æƒ…æ„Ÿæ¨™ç±¤
        sentiments = metadata['sentiment'].values
        
        # ç·¨ç¢¼æ¨™ç±¤
        encoded_labels = self.label_encoder.fit_transform(sentiments)
        
        # ä¿®æ­£ï¼šç²å–ç·¨ç¢¼å™¨åµŒå…¥å‘é‡ï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
        if original_embeddings is None:
            # å˜—è©¦è¼‰å…¥ç·¨ç¢¼å™¨åµŒå…¥å‘é‡
            if self.output_dir:
                # ä½¿ç”¨é€šç”¨çš„æª”æ¡ˆæª¢æ¸¬é‚è¼¯
                try:
                    from .attention_processor import AttentionProcessor
                    temp_processor = AttentionProcessor(output_dir=self.output_dir, encoder_type=self.encoder_type)
                    embeddings_file = temp_processor._find_existing_embeddings(self.encoder_type)
                    
                    if embeddings_file and os.path.exists(embeddings_file):
                        original_embeddings = np.load(embeddings_file)
                        logger.info(f"å·²è¼‰å…¥ {self.encoder_type.upper()} åµŒå…¥å‘é‡ï¼Œå½¢ç‹€: {original_embeddings.shape}")
                        logger.info(f"æª”æ¡ˆä¾†æº: {embeddings_file}")
                    else:
                        raise ValueError(f"ç„¡æ³•æ‰¾åˆ° {self.encoder_type.upper()} åµŒå…¥å‘é‡æ–‡ä»¶ã€‚è«‹æä¾› original_embeddings åƒæ•¸æˆ–ç¢ºä¿ {self.encoder_type.upper()} ç‰¹å¾µå‘é‡æ–‡ä»¶å­˜åœ¨ã€‚")
                except Exception as e:
                    raise ValueError(f"è¼‰å…¥ {self.encoder_type.upper()} åµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚è«‹æä¾› original_embeddings åƒæ•¸ã€‚")
            else:
                raise ValueError(f"ç„¡æ³•æ‰¾åˆ° {self.encoder_type.upper()} åµŒå…¥å‘é‡ã€‚è«‹æä¾› original_embeddings åƒæ•¸ã€‚")
        
        # ç¢ºä¿åµŒå…¥å‘é‡æ•¸é‡èˆ‡å…ƒæ•¸æ“šåŒ¹é…
        if len(original_embeddings) != len(metadata):
            raise ValueError(f"åµŒå…¥å‘é‡æ•¸é‡ ({len(original_embeddings)}) èˆ‡å…ƒæ•¸æ“šæ•¸é‡ ({len(metadata)}) ä¸åŒ¹é…")
        
        # ä¿®æ­£ï¼šä½¿ç”¨åŽŸå§‹BERTåµŒå…¥å‘é‡ä½œç‚ºä¸»è¦ç‰¹å¾µ
        features = original_embeddings.copy()
        
        # ä¿®æ­£ï¼šè¨ˆç®—èˆ‡é¢å‘å‘é‡çš„ç›¸ä¼¼åº¦ä½œç‚ºé¡å¤–ç‰¹å¾µ
        aspect_names = sorted(aspect_vectors.keys())
        similarity_features = []
        
        logger.info(f"è¨ˆç®—æ¯å€‹æ–‡æª”èˆ‡ {len(aspect_names)} å€‹é¢å‘å‘é‡çš„ç›¸ä¼¼åº¦...")
        
        # æª¢æŸ¥ç¶­åº¦ç›¸å®¹æ€§
        first_embedding = original_embeddings[0]
        first_aspect_vector = aspect_vectors[aspect_names[0]]
        
        if first_embedding.shape[0] != first_aspect_vector.shape[0]:
            error_msg = f"ç¶­åº¦ä¸åŒ¹é…: æ–‡æª”åµŒå…¥å‘é‡ç¶­åº¦ {first_embedding.shape[0]}, é¢å‘å‘é‡ç¶­åº¦ {first_aspect_vector.shape[0]}"
            logger.error(error_msg)
            logger.error(f"é€™é€šå¸¸æ˜¯ç”±æ–¼æ³¨æ„åŠ›åˆ†æžå’Œåˆ†é¡žè©•ä¼°ä½¿ç”¨äº†ä¸åŒçš„ç·¨ç¢¼å™¨é€ æˆçš„")
            logger.error(f"ç•¶å‰ç·¨ç¢¼å™¨é¡žåž‹: {self.encoder_type.upper()}")
            
            # å˜—è©¦ä¿®å¾©ï¼šå¦‚æžœé¢å‘å‘é‡ç¶­åº¦æ˜¯1024è€Œæ–‡æª”å‘é‡æ˜¯768ï¼Œèªªæ˜Žé¢å‘å‘é‡ä¾†è‡ªä¸åŒç·¨ç¢¼å™¨
            if first_aspect_vector.shape[0] == 1024 and first_embedding.shape[0] == 768:
                logger.warning("æª¢æ¸¬åˆ°é¢å‘å‘é‡ä¾†è‡ª1024ç¶­ç·¨ç¢¼å™¨ï¼ˆå¯èƒ½æ˜¯GPT/T5ï¼‰ï¼Œä½†æ–‡æª”å‘é‡æ˜¯768ç¶­ï¼ˆBERTï¼‰")
                logger.warning("å»ºè­°é‡æ–°é‹è¡Œå®Œæ•´çš„æµæ°´ç·šä»¥ç¢ºä¿ç·¨ç¢¼å™¨ä¸€è‡´æ€§")
            elif first_aspect_vector.shape[0] == 768 and first_embedding.shape[0] == 1024:
                logger.warning("æª¢æ¸¬åˆ°é¢å‘å‘é‡ä¾†è‡ª768ç¶­ç·¨ç¢¼å™¨ï¼ˆBERTï¼‰ï¼Œä½†æ–‡æª”å‘é‡æ˜¯1024ç¶­ï¼ˆå¯èƒ½æ˜¯GPT/T5ï¼‰")
                logger.warning("å»ºè­°é‡æ–°é‹è¡Œå®Œæ•´çš„æµæ°´ç·šä»¥ç¢ºä¿ç·¨ç¢¼å™¨ä¸€è‡´æ€§")
            
            raise ValueError(f"{error_msg}ã€‚è«‹ç¢ºä¿æ³¨æ„åŠ›åˆ†æžå’Œåˆ†é¡žè©•ä¼°ä½¿ç”¨ç›¸åŒçš„ç·¨ç¢¼å™¨é¡žåž‹ã€‚")
        
        for i, embedding in enumerate(original_embeddings):
            doc_similarities = []
            for aspect_name in aspect_names:
                aspect_vector = aspect_vectors[aspect_name]
                # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(embedding, aspect_vector) / (
                    np.linalg.norm(embedding) * np.linalg.norm(aspect_vector) + 1e-8
                )
                doc_similarities.append(similarity)
            similarity_features.append(doc_similarities)
        
        similarity_features = np.array(similarity_features)
        
        # çµ„åˆåŽŸå§‹ç‰¹å¾µå’Œç›¸ä¼¼åº¦ç‰¹å¾µ
        features = np.concatenate([original_embeddings, similarity_features], axis=1)
        
        prepare_time = time.time() - start_time
        logger.info(f"ç‰¹å¾µæº–å‚™å®Œæˆï¼Œè€—æ™‚: {prepare_time:.2f} ç§’")
        logger.info(f"æº–å‚™äº† {features.shape[0]} å€‹æ¨£æœ¬ï¼Œ{features.shape[1]} ç¶­ç‰¹å¾µ")
        logger.info(f"  - åŽŸå§‹BERTç‰¹å¾µ: {original_embeddings.shape[1]} ç¶­")
        logger.info(f"  - é¢å‘ç›¸ä¼¼åº¦ç‰¹å¾µ: {similarity_features.shape[1]} ç¶­")
        logger.info(f"ä½¿ç”¨çš„é¢å‘: {aspect_names}")
        logger.info(f"æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(sentiments, return_counts=True)))}")
        
        self.feature_vectors = features
        self.labels = encoded_labels
        
        return features, encoded_labels
    
    def train(self, features: np.ndarray, labels: np.ndarray, 
              model_type: str = None, test_size: float = 0.2,
              original_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è¨“ç·´åˆ†é¡žæ¨¡åž‹
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
            model_type: æ¨¡åž‹é¡žåž‹
            test_size: æ¸¬è©¦é›†æ¯”ä¾‹
            original_data: åŽŸå§‹æ•¸æ“šDataFrameï¼Œç”¨æ–¼ä¿å­˜æ¸¬è©¦é›†æ–‡æœ¬
            
        Returns:
            è©•ä¼°çµæžœå­—å…¸
        """
        # å¦‚æžœæ²’æœ‰æŒ‡å®šæ¨¡åž‹é¡žåž‹ï¼Œä½¿ç”¨é è¨­å€¼
        if model_type is None:
            model_type = self.model_type
            
        if model_type not in self.available_models:
            available_models = list(self.available_models.keys())
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡åž‹é¡žåž‹: {model_type}ã€‚å¯ç”¨çš„æ¨¡åž‹: {available_models}")
        
        start_time = time.time()
        logger.info(f"é–‹å§‹è¨“ç·´ {model_type} æ¨¡åž‹...")
        logger.info(f"è¨ˆç®—ç’°å¢ƒ: {self.device_info['description']}")
        
        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦é›†
        split_start = time.time()
        if original_data is not None:
            # å¦‚æžœæœ‰åŽŸå§‹æ•¸æ“šï¼ŒåŒæ™‚åˆ†å‰²åŽŸå§‹æ•¸æ“šä»¥ä¿æŒå°æ‡‰é—œä¿‚
            X_train, X_test, y_train, y_test, data_train, data_test = train_test_split(
                features, labels, original_data, test_size=test_size, random_state=42, stratify=labels
            )
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                features, labels, test_size=test_size, random_state=42, stratify=labels
            )
            data_test = None
        
        split_time = time.time() - split_start
        logger.info(f"æ•¸æ“šåˆ†å‰²å®Œæˆï¼Œè€—æ™‚: {split_time:.2f} ç§’")
        logger.info(f"è¨“ç·´é›†å¤§å°: {X_train.shape[0]}, æ¸¬è©¦é›†å¤§å°: {X_test.shape[0]}")
        
        # é¸æ“‡æ¨¡åž‹
        self.model = self.available_models[model_type]
        self.model_type = model_type
        
        # è¨“ç·´æ¨¡åž‹
        train_start = time.time()
        logger.info(f"é–‹å§‹è¨“ç·´æ¨¡åž‹...")
        
        # é‡å°XGBoosté¡¯ç¤ºç‰¹æ®Šè¨Šæ¯å’Œè¨­å‚™é…ç½®
        if model_type == 'xgboost':
            if self.device_info['has_gpu']:
                logger.info("ðŸš€ ä½¿ç”¨GPUåŠ é€ŸXGBoostè¨“ç·´...")
                # æ™ºèƒ½è¨­å‚™ç®¡ç† - ç¢ºä¿æ•¸æ“šåœ¨æ­£ç¢ºè¨­å‚™ä¸Š
                try:
                    import torch
                    if torch.cuda.is_available():
                        # å°‡numpyæ•¸çµ„è½‰æ›ç‚ºGPUå¼µé‡å†è½‰å›žnumpyï¼ˆç¢ºä¿æ•¸æ“šæ ¼å¼æ­£ç¢ºï¼‰
                        logger.info("æ­£åœ¨å„ªåŒ–æ•¸æ“šæ ¼å¼ä»¥æ”¯æ´GPUåŠ é€Ÿ...")
                        X_train_gpu = torch.tensor(X_train, dtype=torch.float32).cuda()
                        X_test_gpu = torch.tensor(X_test, dtype=torch.float32).cuda()
                        X_train = X_train_gpu.cpu().numpy()
                        X_test = X_test_gpu.cpu().numpy()
                        logger.info("âœ… æ•¸æ“šå·²å„ªåŒ–ç‚ºGPUå…¼å®¹æ ¼å¼")
                except Exception as device_error:
                    logger.warning(f"è¨­å‚™å„ªåŒ–å¤±æ•—ï¼Œä½¿ç”¨åŽŸå§‹æ•¸æ“š: {device_error}")
                
                # ç¢ºä¿XGBoostä½¿ç”¨GPUé…ç½®ï¼ˆä¸è¦†è“‹åˆå§‹åŒ–æ™‚çš„è¨­ç½®ï¼‰
                try:
                    # æª¢æŸ¥XGBoostç‰ˆæœ¬
                    import xgboost as xgb
                    xgb_version = xgb.__version__
                    xgb_major_version = int(xgb_version.split('.')[0])
                    
                    if xgb_major_version >= 2:
                        # XGBoost 2.0+ ç¢ºèªGPUè¨­ç½®
                        current_device = getattr(self.model, 'device', None)
                        if current_device != 'cuda':
                            self.model.set_params(device='cuda')
                        logger.info("XGBoost 2.0+: ç¢ºèªGPUæ¨¡å¼å·²å•Ÿç”¨ (device='cuda')")
                    else:
                        # XGBoost 1.x ç¢ºèªGPUè¨­ç½®
                        current_tree_method = getattr(self.model, 'tree_method', None)
                        if current_tree_method != 'gpu_hist':
                            self.model.set_params(tree_method='gpu_hist', gpu_id=0)
                        logger.info("XGBoost 1.x: ç¢ºèªGPUæ¨¡å¼å·²å•Ÿç”¨ (tree_method='gpu_hist')")
                except Exception as e:
                    logger.warning(f"XGBoost GPUé…ç½®ç¢ºèªå¤±æ•—ï¼Œå°‡ä½¿ç”¨åˆå§‹åŒ–è¨­ç½®: {e}")
            else:
                logger.info("ä½¿ç”¨CPUå¤šæ ¸å¿ƒXGBoostè¨“ç·´...")
        
        self.model.fit(X_train, y_train)
        
        train_time = time.time() - train_start
        logger.info(f"æ¨¡åž‹è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {train_time:.2f} ç§’")
        
        # é æ¸¬
        predict_start = time.time()
        logger.info("é–‹å§‹é æ¸¬...")
        
        train_pred = self.model.predict(X_train)
        test_pred = self.model.predict(X_test)
        test_pred_proba = self.model.predict_proba(X_test)
        
        predict_time = time.time() - predict_start
        logger.info(f"é æ¸¬å®Œæˆï¼Œè€—æ™‚: {predict_time:.2f} ç§’")
        
        # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
        results = self._calculate_metrics(
            y_train, train_pred, y_test, test_pred, test_pred_proba
        )
        
        # æ·»åŠ æ™‚é–“ä¿¡æ¯
        total_time = time.time() - start_time
        timing_info = {
            'total_time': total_time,
            'split_time': split_time,
            'train_time': train_time,
            'predict_time': predict_time,
            'model_type': model_type,
            'device_info': self.device_info
        }
        
        results['timing_info'] = timing_info
        
        # ä¿å­˜é æ¸¬çµæžœè©³ç´°ä¿¡æ¯
        # å°‡ç·¨ç¢¼çš„æ¨™ç±¤è½‰æ›å›žåŽŸå§‹æ¨™ç±¤åç¨±
        # ä¿®æ­£ï¼šæª¢æŸ¥LabelEncoderæ˜¯å¦å·²ç¶“fitted
        if hasattr(self.label_encoder, 'classes_') and self.label_encoder.classes_ is not None:
            true_label_names = self.label_encoder.inverse_transform(y_test)
            predicted_label_names = self.label_encoder.inverse_transform(test_pred)
            class_names_list = self.label_encoder.classes_.tolist()
        else:
            # å¦‚æžœæ²’æœ‰fittedï¼Œä½¿ç”¨æ•¸å­—æ¨™ç±¤
            true_label_names = [f"label_{label}" for label in y_test]
            predicted_label_names = [f"label_{label}" for label in test_pred]
            unique_labels = np.unique(np.concatenate([y_train, y_test]))
            class_names_list = [f"label_{label}" for label in unique_labels]
        
        # ä¿å­˜æ¸¬è©¦é›†çš„æ–‡æœ¬ä¿¡æ¯ä»¥ä¾¿å¾ŒçºŒåŒ¹é…
        test_texts = []
        if data_test is not None:
            # ç²å–æ¸¬è©¦é›†çš„æ–‡æœ¬
            text_column = None
            for col in ['processed_text', 'text', 'review', 'content']:
                if col in data_test.columns:
                    text_column = col
                    break
            
            if text_column:
                test_texts = data_test[text_column].tolist()
        
        results['prediction_details'] = {
            'test_indices': None,  # ç”±æ–¼è¨“ç·´æ™‚æ²’æœ‰åŽŸå§‹ç´¢å¼•ï¼Œé€™è£¡è¨­ç‚ºNone
            'true_labels': y_test.tolist(),  # ç·¨ç¢¼å¾Œçš„æ¨™ç±¤
            'predicted_labels': test_pred.tolist(),  # ç·¨ç¢¼å¾Œçš„é æ¸¬æ¨™ç±¤
            'true_label_names': true_label_names.tolist() if hasattr(true_label_names, 'tolist') else list(true_label_names),  # åŽŸå§‹æ¨™ç±¤åç¨±
            'predicted_label_names': predicted_label_names.tolist() if hasattr(predicted_label_names, 'tolist') else list(predicted_label_names),  # é æ¸¬æ¨™ç±¤åç¨±
            'predicted_probabilities': test_pred_proba.tolist(),
            'class_names': class_names_list,
            'test_texts': test_texts  # æ¸¬è©¦é›†æ–‡æœ¬ï¼Œç”¨æ–¼å¾ŒçºŒåŒ¹é…
        }
        
        # ä¿å­˜æ¨¡åž‹
        if self.output_dir:
            self._save_model()
        
        # è¼¸å‡ºæ™‚é–“çµ±è¨ˆ
        logger.info(f"ðŸ• æ¨¡åž‹è¨“ç·´å®Œæ•´çµ±è¨ˆ:")
        logger.info(f"   â€¢ ç¸½è€—æ™‚: {total_time:.2f} ç§’")
        logger.info(f"   â€¢ æ•¸æ“šåˆ†å‰²: {split_time:.2f} ç§’")
        logger.info(f"   â€¢ æ¨¡åž‹è¨“ç·´: {train_time:.2f} ç§’")
        logger.info(f"   â€¢ é æ¸¬æ™‚é–“: {predict_time:.2f} ç§’")
        logger.info(f"   â€¢ æ¸¬è©¦æº–ç¢ºçŽ‡: {results['test_accuracy']:.4f}")
        logger.info(f"   â€¢ è¨ˆç®—ç’°å¢ƒ: {self.device_info['description']}")
        
        return results
    
    def predict(self, features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        é æ¸¬æƒ…æ„Ÿæ¨™ç±¤
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            
        Returns:
            predictions: é æ¸¬æ¨™ç±¤
            probabilities: é æ¸¬æ©ŸçŽ‡
        """
        if self.model is None:
            raise ValueError("æ¨¡åž‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ train() æ–¹æ³•")
        
        predictions = self.model.predict(features)
        probabilities = self.model.predict_proba(features)
        
        return predictions, probabilities
    
    def predict_with_details(self, features: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        é æ¸¬æƒ…æ„Ÿæ¨™ç±¤ä¸¦è¿”å›žè©³ç´°çµæžœ
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            original_data: åŽŸå§‹æ•¸æ“šï¼ˆåŒ…å«æ–‡æœ¬å’ŒçœŸå¯¦æ¨™ç±¤ï¼‰
            
        Returns:
            åŒ…å«é æ¸¬çµæžœå’Œè©³ç´°ä¿¡æ¯çš„å­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡åž‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ train() æ–¹æ³•")
        
        predictions = self.model.predict(features)
        probabilities = self.model.predict_proba(features)
        
        # è§£ç¢¼é æ¸¬æ¨™ç±¤
        predicted_labels = self.label_encoder.inverse_transform(predictions)
        
        result = {
            'predictions': predictions,
            'predicted_labels': predicted_labels.tolist(),
            'probabilities': probabilities.tolist(),
            'class_names': self.label_encoder.classes_.tolist()
        }
        
        # å¦‚æžœæœ‰åŽŸå§‹æ•¸æ“šï¼Œæ·»åŠ è©³ç´°æ¯”å°ä¿¡æ¯
        if original_data is not None:
            result['detailed_comparison'] = self._create_detailed_comparison(
                original_data, predicted_labels, probabilities
            )
        
        return result
    
    def evaluate_attention_mechanisms(self, attention_results: Dict[str, Any], 
                                    metadata: pd.DataFrame,
                                    original_embeddings: np.ndarray = None,
                                    model_type: str = None) -> Dict[str, Dict]:
        """
        è©•ä¼°ä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½
        
        Args:
            attention_results: æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æžçµæžœ
            metadata: åŒ…å«çœŸå¯¦æ¨™ç±¤çš„å…ƒæ•¸æ“š
            original_embeddings: åŽŸå§‹BERTåµŒå…¥å‘é‡ï¼ˆä¿®æ­£ï¼šæ–°å¢žæ­¤åƒæ•¸ï¼‰
            model_type: æŒ‡å®šåˆ†é¡žå™¨é¡žåž‹ï¼Œå¦‚æžœNoneå‰‡ä½¿ç”¨é è¨­å€¼
            
        Returns:
            å„æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½çµæžœ
        """
        evaluation_results = {}
        
        # ä¿®æ­£ï¼šå¦‚æžœæ²’æœ‰æä¾›original_embeddingsï¼Œå˜—è©¦è¼‰å…¥ï¼ˆæ”¯æ´å¤šç¨®ç·¨ç¢¼å™¨ï¼‰
        if original_embeddings is None:
            if self.output_dir:
                # ä½¿ç”¨é€šç”¨çš„æª”æ¡ˆæª¢æ¸¬é‚è¼¯
                try:
                    from .attention_processor import AttentionProcessor
                    temp_processor = AttentionProcessor(output_dir=self.output_dir, encoder_type=self.encoder_type)
                    embeddings_file = temp_processor._find_existing_embeddings(self.encoder_type)
                    
                    if embeddings_file and os.path.exists(embeddings_file):
                        original_embeddings = np.load(embeddings_file)
                        logger.info(f"å·²è¼‰å…¥ {self.encoder_type.upper()} åµŒå…¥å‘é‡ç”¨æ–¼åˆ†é¡žè©•ä¼°ï¼Œå½¢ç‹€: {original_embeddings.shape}")
                        logger.info(f"æª”æ¡ˆä¾†æº: {embeddings_file}")
                    else:
                        logger.warning(f"æœªæ‰¾åˆ° {self.encoder_type.upper()} åµŒå…¥å‘é‡æ–‡ä»¶ï¼Œå°‡å˜—è©¦å¾žprepare_featuresæ–¹æ³•ä¸­è¼‰å…¥")
                except Exception as e:
                    logger.warning(f"è¼‰å…¥ {self.encoder_type.upper()} åµŒå…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œå°‡å˜—è©¦å¾žprepare_featuresæ–¹æ³•ä¸­è¼‰å…¥")
        
        # éŽæ¿¾å‡ºæœ‰æ•ˆçš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        valid_mechanisms = []
        for mechanism_name, mechanism_result in attention_results.items():
            if mechanism_name != 'comparison' and 'aspect_vectors' in mechanism_result:
                valid_mechanisms.append((mechanism_name, mechanism_result))
        
        print(f"ðŸ“Š é–‹å§‹è©•ä¼° {len(valid_mechanisms)} ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½...")
        
        for mechanism_name, mechanism_result in tqdm(valid_mechanisms, desc="è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶"):
            print(f"   ðŸ” æ­£åœ¨è©•ä¼° {mechanism_name} æ³¨æ„åŠ›æ©Ÿåˆ¶...")
            logger.info(f"è©•ä¼° {mechanism_name} æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½...")
            
            try:
                # æº–å‚™ç‰¹å¾µï¼ˆä¿®æ­£ï¼šå‚³éžåŽŸå§‹åµŒå…¥å‘é‡ï¼‰
                print(f"      ðŸ“‹ æº–å‚™ç‰¹å¾µå‘é‡...")
                aspect_vectors = mechanism_result['aspect_vectors']
                features, labels = self.prepare_features(aspect_vectors, metadata, original_embeddings)
                
                # è¨“ç·´å’Œè©•ä¼°
                print(f"      ðŸ¤– è¨“ç·´åˆ†é¡žå™¨...")
                # ä½¿ç”¨æŒ‡å®šçš„model_typeæˆ–é è¨­å€¼
                train_model_type = model_type if model_type is not None else self.model_type
                results = self.train(features, labels, model_type=train_model_type, original_data=metadata)
                
                # æ·»åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å®šä¿¡æ¯
                results['attention_mechanism'] = mechanism_name
                results['mechanism_metrics'] = mechanism_result.get('metrics', {})
                
                evaluation_results[mechanism_name] = results
                
                print(f"      âœ… {mechanism_name} è©•ä¼°å®Œæˆ - æº–ç¢ºçŽ‡: {results['test_accuracy']:.4f}, "
                      f"F1åˆ†æ•¸: {results['test_f1']:.4f}")
                
            except Exception as e:
                print(f"      âŒ è©•ä¼° {mechanism_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                logger.error(f"è©•ä¼° {mechanism_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        # æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½
        print(f"   ðŸ“ˆ æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½...")
        comparison = self._compare_mechanisms(evaluation_results)
        evaluation_results['comparison'] = comparison
        
        print(f"âœ… åˆ†é¡žæ€§èƒ½è©•ä¼°å®Œæˆï¼")
        if comparison and 'best_mechanism' in comparison:
            print(f"ðŸ† æœ€ä½³åˆ†é¡žæ€§èƒ½æ©Ÿåˆ¶: {comparison['best_mechanism']}")
            # å®‰å…¨è¨ªå• summary éµ
            summary = comparison.get('summary', {})
            if summary:
                print(f"ðŸ“Š æœ€ä½³æº–ç¢ºçŽ‡: {summary.get('best_accuracy', 0):.4f}")
                print(f"ðŸ“Š æœ€ä½³F1åˆ†æ•¸: {summary.get('best_f1', 0):.4f}")
            else:
                print("ðŸ“Š è­¦å‘Šï¼šç„¡æ³•ç²å–æ€§èƒ½çµ±è¨ˆæ‘˜è¦")
        else:
            print("âš ï¸  è­¦å‘Šï¼šç„¡æ³•é€²è¡Œæ€§èƒ½æ¯”è¼ƒ")
        
        return evaluation_results
    
    def _calculate_metrics(self, y_train: np.ndarray, train_pred: np.ndarray,
                          y_test: np.ndarray, test_pred: np.ndarray, 
                          test_pred_proba: np.ndarray) -> Dict[str, Any]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        # æº–ç¢ºçŽ‡
        train_accuracy = accuracy_score(y_train, train_pred)
        test_accuracy = accuracy_score(y_test, test_pred)
        
        # ç²¾ç¢ºçŽ‡ã€å¬å›žçŽ‡ã€F1åˆ†æ•¸
        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(
            y_train, train_pred, average='weighted'
        )
        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
            y_test, test_pred, average='weighted'
        )
        
        # æ··æ·†çŸ©é™£
        confusion_mat = confusion_matrix(y_test, test_pred)
        
        # åˆ†é¡žå ±å‘Š
        # ä¿®æ­£ï¼šæª¢æŸ¥LabelEncoderæ˜¯å¦å·²ç¶“fittedï¼Œé¿å…AttributeError
        if hasattr(self.label_encoder, 'classes_') and self.label_encoder.classes_ is not None:
            class_names = self.label_encoder.classes_
        else:
            # å¦‚æžœæ²’æœ‰é¡žåˆ¥åç¨±ï¼Œä½¿ç”¨å”¯ä¸€å€¼
            unique_labels = np.unique(np.concatenate([y_train, y_test]))
            class_names = [f"class_{label}" for label in unique_labels]
        
        classification_rep = classification_report(
            y_test, test_pred, target_names=class_names, output_dict=True
        )
        
        return {
            'train_accuracy': float(train_accuracy),
            'test_accuracy': float(test_accuracy),
            'train_precision': float(train_precision),
            'test_precision': float(test_precision),
            'train_recall': float(train_recall),
            'test_recall': float(test_recall),
            'train_f1': float(train_f1),
            'test_f1': float(test_f1),
            'confusion_matrix': confusion_mat.tolist(),
            'classification_report': classification_rep,
            'class_names': class_names.tolist() if hasattr(class_names, 'tolist') else list(class_names),
            'model_type': self.model_type
        }
    
    def _compare_mechanisms(self, evaluation_results: Dict[str, Dict]) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½"""
        # å¦‚æžœæ²’æœ‰è©•ä¼°çµæžœï¼Œè¿”å›žç©ºçš„ä½†çµæ§‹å®Œæ•´çš„æ¯”è¼ƒçµæžœ
        if not evaluation_results:
            return {
                'best_mechanism': None,
                'accuracy_ranking': [],
                'f1_ranking': [],
                'summary': {
                    'best_accuracy': 0,
                    'best_f1': 0,
                    'mechanisms_tested': 0,
                    'error': 'No evaluation results available'
                }
            }
        
        # æå–æ€§èƒ½æŒ‡æ¨™
        mechanisms = []
        accuracies = []
        f1_scores = []
        precisions = []
        recalls = []
        
        for mechanism, results in evaluation_results.items():
            if mechanism == 'comparison':
                continue
            
            # æ·»åŠ å®‰å…¨æª¢æŸ¥ï¼Œç¢ºä¿çµæžœåŒ…å«å¿…è¦çš„éµ
            try:
                mechanisms.append(mechanism)
                accuracies.append(results.get('test_accuracy', 0))
                f1_scores.append(results.get('test_f1', 0))
                precisions.append(results.get('test_precision', 0))
                recalls.append(results.get('test_recall', 0))
            except (KeyError, TypeError) as e:
                logger.warning(f"æ©Ÿåˆ¶ {mechanism} çš„çµæžœä¸å®Œæ•´ï¼Œè·³éŽ: {str(e)}")
                continue
        
        # ç¢ºä¿æœ‰æœ‰æ•ˆçš„çµæžœæ‰é€²è¡ŒæŽ’åº
        if not mechanisms:
            return {
                'best_mechanism': None,
                'accuracy_ranking': [],
                'f1_ranking': [],
                'summary': {
                    'best_accuracy': 0,
                    'best_f1': 0,
                    'mechanisms_tested': 0,
                    'error': 'No valid mechanism results found'
                }
            }
        
        # æŽ’åº
        accuracy_ranking = sorted(zip(mechanisms, accuracies), key=lambda x: x[1], reverse=True)
        f1_ranking = sorted(zip(mechanisms, f1_scores), key=lambda x: x[1], reverse=True)
        
        # æ‰¾å‡ºæœ€ä½³æ©Ÿåˆ¶
        best_mechanism = accuracy_ranking[0][0] if accuracy_ranking else None
        
        return {
            'best_mechanism': best_mechanism,
            'accuracy_ranking': accuracy_ranking,
            'f1_ranking': f1_ranking,
            'summary': {
                'best_accuracy': accuracy_ranking[0][1] if accuracy_ranking else 0,
                'best_f1': f1_ranking[0][1] if f1_ranking else 0,
                'mechanisms_tested': len(mechanisms)
            }
        }
    
    def _save_model(self):
        """ä¿å­˜è¨“ç·´å¥½çš„æ¨¡åž‹åˆ°runç›®éŒ„æ ¹ç›®éŒ„"""
        if self.model is None or self.output_dir is None:
            return

        # ç¢ºä¿ä¿å­˜åˆ°runç›®éŒ„çš„æ ¹ç›®éŒ„
        if any(subdir in self.output_dir for subdir in ["01_preprocessing", "02_bert_encoding", "03_attention_testing", "04_analysis"]):
            # å¦‚æžœè¼¸å‡ºç›®éŒ„æ˜¯å­ç›®éŒ„ï¼Œæ”¹ç‚ºçˆ¶ç›®éŒ„ï¼ˆrunç›®éŒ„æ ¹ç›®éŒ„ï¼‰
            run_dir = os.path.dirname(self.output_dir)
        else:
            run_dir = self.output_dir

        model_path = os.path.join(run_dir, f"sentiment_classifier_{self.model_type}.pkl")
        encoder_path = os.path.join(run_dir, "label_encoder.pkl")

        joblib.dump(self.model, model_path)
        joblib.dump(self.label_encoder, encoder_path)

        logger.info(f"æ¨¡åž‹å·²ä¿å­˜è‡³: {model_path}")
        logger.info(f"æ¨™ç±¤ç·¨ç¢¼å™¨å·²ä¿å­˜è‡³: {encoder_path}")
    
    def load_model(self, model_path: str, encoder_path: str):
        """è¼‰å…¥é è¨“ç·´æ¨¡åž‹"""
        self.model = joblib.load(model_path)
        self.label_encoder = joblib.load(encoder_path)
        
        logger.info(f"æ¨¡åž‹å·²è¼‰å…¥: {model_path}")
        logger.info(f"æ¨™ç±¤ç·¨ç¢¼å™¨å·²è¼‰å…¥: {encoder_path}")
    
    def _create_detailed_comparison(self, original_data: pd.DataFrame, 
                                  predicted_labels: np.ndarray, 
                                  probabilities: np.ndarray) -> List[Dict]:
        """å‰µå»ºè©³ç´°çš„æ¯”å°çµæžœ"""
        detailed_results = []
        
        for i in range(len(original_data)):
            row = original_data.iloc[i]
            
            # ç²å–åŽŸå§‹æ–‡æœ¬å’Œæ¨™ç±¤
            original_text = str(row.get('processed_text', row.get('text', row.get('review', ''))))
            original_label = str(row.get('sentiment', 'unknown'))
            
            # æˆªæ–·éŽé•·çš„æ–‡æœ¬
            if len(original_text) > 100:
                display_text = original_text[:97] + "..."
            else:
                display_text = original_text
            
            predicted_label = predicted_labels[i]
            is_correct = predicted_label == original_label
            confidence = float(np.max(probabilities[i]))
            
            detailed_results.append({
                'index': i,
                'original_text': display_text,
                'original_label': original_label,
                'predicted_label': predicted_label,
                'is_correct': is_correct,
                'confidence': confidence
            })
        
        return detailed_results 