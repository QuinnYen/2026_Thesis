#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æƒ…æ„Ÿåˆ†é¡žå™¨æ¨¡çµ„ - åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µçš„æƒ…æ„Ÿåˆ†é¡ž
"""

import numpy as np
import pandas as pd
import logging
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from tqdm import tqdm
import joblib
import os
from typing import Dict, Any, Tuple, Optional, List

logger = logging.getLogger(__name__)

class SentimentClassifier:
    """åŸºæ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µçš„æƒ…æ„Ÿåˆ†é¡žå™¨"""
    
    def __init__(self, output_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡žå™¨
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        """
        self.output_dir = output_dir
        self.model = None
        self.label_encoder = LabelEncoder()
        self.feature_vectors = None
        self.labels = None
        self.model_type = 'random_forest'  # é è¨­ä½¿ç”¨éš¨æ©Ÿæ£®æž—
        
        # æ”¯æŒçš„æ¨¡åž‹é¡žåž‹
        self.available_models = {
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),
            'svm': SVC(random_state=42, probability=True)
        }
        
        logger.info("æƒ…æ„Ÿåˆ†é¡žå™¨å·²åˆå§‹åŒ–")
    
    def prepare_features(self, aspect_vectors: Dict, metadata: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        æº–å‚™åˆ†é¡žç‰¹å¾µ
        
        Args:
            aspect_vectors: é¢å‘ç‰¹å¾µå‘é‡å­—å…¸
            metadata: åŒ…å«çœŸå¯¦æ¨™ç±¤çš„å…ƒæ•¸æ“š
            
        Returns:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
        """
        # æª¢æŸ¥æƒ…æ„Ÿæ¨™ç±¤æ¬„ä½
        if 'sentiment' not in metadata.columns:
            raise ValueError("å…ƒæ•¸æ“šä¸­ç¼ºå°‘ 'sentiment' æ¬„ä½")
        
        # æå–æƒ…æ„Ÿæ¨™ç±¤
        sentiments = metadata['sentiment'].values
        
        # ç·¨ç¢¼æ¨™ç±¤
        encoded_labels = self.label_encoder.fit_transform(sentiments)
        
        # éœ€è¦åŽŸå§‹çš„BERTåµŒå…¥å‘é‡ä¾†è¨ˆç®—èˆ‡é¢å‘å‘é‡çš„ç›¸ä¼¼åº¦
        # é€™è£¡å‡è¨­åœ¨æŸè™•å¯ä»¥ç²å–åŽŸå§‹åµŒå…¥å‘é‡
        # ç‚ºäº†å®Œæ•´æ€§ï¼Œæˆ‘å€‘å°‡é¢å‘å‘é‡ä¸²è¯ä½œç‚ºç‰¹å¾µ
        
        # å°‡é¢å‘å‘é‡åˆä½µç‚ºå–®ä¸€ç‰¹å¾µå‘é‡ï¼ˆæ¯å€‹æ–‡æª”éƒ½ä½¿ç”¨ç›¸åŒçš„é¢å‘å‘é‡ï¼‰
        aspect_names = sorted(aspect_vectors.keys())  # ä¿è­‰é †åºä¸€è‡´
        concatenated_features = []
        for aspect_name in aspect_names:
            concatenated_features.extend(aspect_vectors[aspect_name])
        
        # ç‚ºæ¯å€‹æ–‡æª”è¤‡è£½ç›¸åŒçš„é¢å‘ç‰¹å¾µï¼ˆé€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„æ–¹æ³•ï¼‰
        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œæ‡‰è©²è¨ˆç®—æ¯å€‹æ–‡æª”èˆ‡å„é¢å‘å‘é‡çš„ç›¸ä¼¼åº¦
        features = np.tile(concatenated_features, (len(metadata), 1))
        
        logger.info(f"æº–å‚™äº† {features.shape[0]} å€‹æ¨£æœ¬ï¼Œ{features.shape[1]} ç¶­ç‰¹å¾µ")
        logger.info(f"ä½¿ç”¨çš„é¢å‘: {aspect_names}")
        logger.info(f"æ¨™ç±¤åˆ†å¸ƒ: {dict(zip(*np.unique(sentiments, return_counts=True)))}")
        
        self.feature_vectors = features
        self.labels = encoded_labels
        
        return features, encoded_labels
    
    def train(self, features: np.ndarray, labels: np.ndarray, 
              model_type: str = 'random_forest', test_size: float = 0.2,
              original_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è¨“ç·´åˆ†é¡žæ¨¡åž‹
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
            model_type: æ¨¡åž‹é¡žåž‹
            test_size: æ¸¬è©¦é›†æ¯”ä¾‹
            original_data: åŽŸå§‹æ•¸æ“šDataFrameï¼Œç”¨æ–¼ä¿å­˜æ¸¬è©¦é›†æ–‡æœ¬
            
        Returns:
            è©•ä¼°çµæžœå­—å…¸
        """
        if model_type not in self.available_models:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡åž‹é¡žåž‹: {model_type}")
        
        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦é›†
        if original_data is not None:
            # å¦‚æžœæœ‰åŽŸå§‹æ•¸æ“šï¼ŒåŒæ™‚åˆ†å‰²åŽŸå§‹æ•¸æ“šä»¥ä¿æŒå°æ‡‰é—œä¿‚
            X_train, X_test, y_train, y_test, data_train, data_test = train_test_split(
                features, labels, original_data, test_size=test_size, random_state=42, stratify=labels
            )
        else:
            X_train, X_test, y_train, y_test = train_test_split(
                features, labels, test_size=test_size, random_state=42, stratify=labels
            )
            data_test = None
        
        # é¸æ“‡æ¨¡åž‹
        self.model = self.available_models[model_type]
        self.model_type = model_type
        
        logger.info(f"é–‹å§‹è¨“ç·´ {model_type} æ¨¡åž‹...")
        logger.info(f"è¨“ç·´é›†å¤§å°: {X_train.shape[0]}, æ¸¬è©¦é›†å¤§å°: {X_test.shape[0]}")
        
        # è¨“ç·´æ¨¡åž‹
        self.model.fit(X_train, y_train)
        
        # é æ¸¬
        train_pred = self.model.predict(X_train)
        test_pred = self.model.predict(X_test)
        test_pred_proba = self.model.predict_proba(X_test)
        
        # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
        results = self._calculate_metrics(
            y_train, train_pred, y_test, test_pred, test_pred_proba
        )
        
        # ä¿å­˜é æ¸¬çµæžœè©³ç´°ä¿¡æ¯
        # å°‡ç·¨ç¢¼çš„æ¨™ç±¤è½‰æ›å›žåŽŸå§‹æ¨™ç±¤åç¨±
        true_label_names = self.label_encoder.inverse_transform(y_test)
        predicted_label_names = self.label_encoder.inverse_transform(test_pred)
        
        # ä¿å­˜æ¸¬è©¦é›†çš„æ–‡æœ¬ä¿¡æ¯ä»¥ä¾¿å¾ŒçºŒåŒ¹é…
        test_texts = []
        if data_test is not None:
            # ç²å–æ¸¬è©¦é›†çš„æ–‡æœ¬
            text_column = None
            for col in ['processed_text', 'text', 'review', 'content']:
                if col in data_test.columns:
                    text_column = col
                    break
            
            if text_column:
                test_texts = data_test[text_column].tolist()
        
        results['prediction_details'] = {
            'test_indices': None,  # ç”±æ–¼è¨“ç·´æ™‚æ²’æœ‰åŽŸå§‹ç´¢å¼•ï¼Œé€™è£¡è¨­ç‚ºNone
            'true_labels': y_test.tolist(),  # ç·¨ç¢¼å¾Œçš„æ¨™ç±¤
            'predicted_labels': test_pred.tolist(),  # ç·¨ç¢¼å¾Œçš„é æ¸¬æ¨™ç±¤
            'true_label_names': true_label_names.tolist(),  # åŽŸå§‹æ¨™ç±¤åç¨±
            'predicted_label_names': predicted_label_names.tolist(),  # é æ¸¬æ¨™ç±¤åç¨±
            'predicted_probabilities': test_pred_proba.tolist(),
            'class_names': self.label_encoder.classes_.tolist(),
            'test_texts': test_texts  # æ¸¬è©¦é›†æ–‡æœ¬ï¼Œç”¨æ–¼å¾ŒçºŒåŒ¹é…
        }
        
        # ä¿å­˜æ¨¡åž‹
        if self.output_dir:
            self._save_model()
        
        logger.info(f"æ¨¡åž‹è¨“ç·´å®Œæˆï¼æ¸¬è©¦æº–ç¢ºçŽ‡: {results['test_accuracy']:.4f}")
        
        return results
    
    def predict(self, features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        é æ¸¬æƒ…æ„Ÿæ¨™ç±¤
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            
        Returns:
            predictions: é æ¸¬æ¨™ç±¤
            probabilities: é æ¸¬æ©ŸçŽ‡
        """
        if self.model is None:
            raise ValueError("æ¨¡åž‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ train() æ–¹æ³•")
        
        predictions = self.model.predict(features)
        probabilities = self.model.predict_proba(features)
        
        return predictions, probabilities
    
    def predict_with_details(self, features: np.ndarray, original_data: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """
        é æ¸¬æƒ…æ„Ÿæ¨™ç±¤ä¸¦è¿”å›žè©³ç´°çµæžœ
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            original_data: åŽŸå§‹æ•¸æ“šï¼ˆåŒ…å«æ–‡æœ¬å’ŒçœŸå¯¦æ¨™ç±¤ï¼‰
            
        Returns:
            åŒ…å«é æ¸¬çµæžœå’Œè©³ç´°ä¿¡æ¯çš„å­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡åž‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ train() æ–¹æ³•")
        
        predictions = self.model.predict(features)
        probabilities = self.model.predict_proba(features)
        
        # è§£ç¢¼é æ¸¬æ¨™ç±¤
        predicted_labels = self.label_encoder.inverse_transform(predictions)
        
        result = {
            'predictions': predictions,
            'predicted_labels': predicted_labels.tolist(),
            'probabilities': probabilities.tolist(),
            'class_names': self.label_encoder.classes_.tolist()
        }
        
        # å¦‚æžœæœ‰åŽŸå§‹æ•¸æ“šï¼Œæ·»åŠ è©³ç´°æ¯”å°ä¿¡æ¯
        if original_data is not None:
            result['detailed_comparison'] = self._create_detailed_comparison(
                original_data, predicted_labels, probabilities
            )
        
        return result
    
    def evaluate_attention_mechanisms(self, attention_results: Dict[str, Any], 
                                    metadata: pd.DataFrame) -> Dict[str, Dict]:
        """
        è©•ä¼°ä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½
        
        Args:
            attention_results: æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æžçµæžœ
            metadata: åŒ…å«çœŸå¯¦æ¨™ç±¤çš„å…ƒæ•¸æ“š
            
        Returns:
            å„æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½çµæžœ
        """
        evaluation_results = {}
        
        # éŽæ¿¾å‡ºæœ‰æ•ˆçš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        valid_mechanisms = []
        for mechanism_name, mechanism_result in attention_results.items():
            if mechanism_name != 'comparison' and 'aspect_vectors' in mechanism_result:
                valid_mechanisms.append((mechanism_name, mechanism_result))
        
        print(f"ðŸ“Š é–‹å§‹è©•ä¼° {len(valid_mechanisms)} ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½...")
        
        for mechanism_name, mechanism_result in tqdm(valid_mechanisms, desc="è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶"):
            print(f"   ðŸ” æ­£åœ¨è©•ä¼° {mechanism_name} æ³¨æ„åŠ›æ©Ÿåˆ¶...")
            logger.info(f"è©•ä¼° {mechanism_name} æ³¨æ„åŠ›æ©Ÿåˆ¶çš„åˆ†é¡žæ€§èƒ½...")
            
            try:
                # æº–å‚™ç‰¹å¾µ
                print(f"      ðŸ“‹ æº–å‚™ç‰¹å¾µå‘é‡...")
                aspect_vectors = mechanism_result['aspect_vectors']
                features, labels = self.prepare_features(aspect_vectors, metadata)
                
                # è¨“ç·´å’Œè©•ä¼°
                print(f"      ðŸ¤– è¨“ç·´åˆ†é¡žå™¨...")
                results = self.train(features, labels, model_type=self.model_type, original_data=metadata)
                
                # æ·»åŠ æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å®šä¿¡æ¯
                results['attention_mechanism'] = mechanism_name
                results['mechanism_metrics'] = mechanism_result.get('metrics', {})
                
                evaluation_results[mechanism_name] = results
                
                print(f"      âœ… {mechanism_name} è©•ä¼°å®Œæˆ - æº–ç¢ºçŽ‡: {results['test_accuracy']:.4f}, "
                      f"F1åˆ†æ•¸: {results['test_f1']:.4f}")
                
            except Exception as e:
                print(f"      âŒ è©•ä¼° {mechanism_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                logger.error(f"è©•ä¼° {mechanism_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                continue
        
        # æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½
        print(f"   ðŸ“ˆ æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½...")
        comparison = self._compare_mechanisms(evaluation_results)
        evaluation_results['comparison'] = comparison
        
        print(f"âœ… åˆ†é¡žæ€§èƒ½è©•ä¼°å®Œæˆï¼")
        if comparison and 'best_mechanism' in comparison:
            print(f"ðŸ† æœ€ä½³åˆ†é¡žæ€§èƒ½æ©Ÿåˆ¶: {comparison['best_mechanism']}")
            print(f"ðŸ“Š æœ€ä½³æº–ç¢ºçŽ‡: {comparison['summary']['best_accuracy']:.4f}")
            print(f"ðŸ“Š æœ€ä½³F1åˆ†æ•¸: {comparison['summary']['best_f1']:.4f}")
        
        return evaluation_results
    
    def _calculate_metrics(self, y_train: np.ndarray, train_pred: np.ndarray,
                          y_test: np.ndarray, test_pred: np.ndarray, 
                          test_pred_proba: np.ndarray) -> Dict[str, Any]:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        # æº–ç¢ºçŽ‡
        train_accuracy = accuracy_score(y_train, train_pred)
        test_accuracy = accuracy_score(y_test, test_pred)
        
        # ç²¾ç¢ºçŽ‡ã€å¬å›žçŽ‡ã€F1åˆ†æ•¸
        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(
            y_train, train_pred, average='weighted'
        )
        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
            y_test, test_pred, average='weighted'
        )
        
        # æ··æ·†çŸ©é™£
        confusion_mat = confusion_matrix(y_test, test_pred)
        
        # åˆ†é¡žå ±å‘Š
        class_names = self.label_encoder.classes_
        classification_rep = classification_report(
            y_test, test_pred, target_names=class_names, output_dict=True
        )
        
        return {
            'train_accuracy': float(train_accuracy),
            'test_accuracy': float(test_accuracy),
            'train_precision': float(train_precision),
            'test_precision': float(test_precision),
            'train_recall': float(train_recall),
            'test_recall': float(test_recall),
            'train_f1': float(train_f1),
            'test_f1': float(test_f1),
            'confusion_matrix': confusion_mat.tolist(),
            'classification_report': classification_rep,
            'class_names': class_names.tolist(),
            'model_type': self.model_type
        }
    
    def _compare_mechanisms(self, evaluation_results: Dict[str, Dict]) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½"""
        if not evaluation_results:
            return {}
        
        # æå–æ€§èƒ½æŒ‡æ¨™
        mechanisms = []
        accuracies = []
        f1_scores = []
        precisions = []
        recalls = []
        
        for mechanism, results in evaluation_results.items():
            if mechanism == 'comparison':
                continue
            
            mechanisms.append(mechanism)
            accuracies.append(results['test_accuracy'])
            f1_scores.append(results['test_f1'])
            precisions.append(results['test_precision'])
            recalls.append(results['test_recall'])
        
        # æŽ’åº
        accuracy_ranking = sorted(zip(mechanisms, accuracies), key=lambda x: x[1], reverse=True)
        f1_ranking = sorted(zip(mechanisms, f1_scores), key=lambda x: x[1], reverse=True)
        
        # æ‰¾å‡ºæœ€ä½³æ©Ÿåˆ¶
        best_mechanism = accuracy_ranking[0][0] if accuracy_ranking else None
        
        return {
            'best_mechanism': best_mechanism,
            'accuracy_ranking': accuracy_ranking,
            'f1_ranking': f1_ranking,
            'summary': {
                'best_accuracy': accuracy_ranking[0][1] if accuracy_ranking else 0,
                'best_f1': f1_ranking[0][1] if f1_ranking else 0,
                'mechanisms_tested': len(mechanisms)
            }
        }
    
    def _save_model(self):
        """ä¿å­˜è¨“ç·´å¥½çš„æ¨¡åž‹"""
        if self.model is None or self.output_dir is None:
            return
        
        model_path = os.path.join(self.output_dir, f"sentiment_classifier_{self.model_type}.pkl")
        encoder_path = os.path.join(self.output_dir, "label_encoder.pkl")
        
        joblib.dump(self.model, model_path)
        joblib.dump(self.label_encoder, encoder_path)
        
        logger.info(f"æ¨¡åž‹å·²ä¿å­˜è‡³: {model_path}")
        logger.info(f"æ¨™ç±¤ç·¨ç¢¼å™¨å·²ä¿å­˜è‡³: {encoder_path}")
    
    def load_model(self, model_path: str, encoder_path: str):
        """è¼‰å…¥é è¨“ç·´æ¨¡åž‹"""
        self.model = joblib.load(model_path)
        self.label_encoder = joblib.load(encoder_path)
        
        logger.info(f"æ¨¡åž‹å·²è¼‰å…¥: {model_path}")
        logger.info(f"æ¨™ç±¤ç·¨ç¢¼å™¨å·²è¼‰å…¥: {encoder_path}")
    
    def _create_detailed_comparison(self, original_data: pd.DataFrame, 
                                  predicted_labels: np.ndarray, 
                                  probabilities: np.ndarray) -> List[Dict]:
        """å‰µå»ºè©³ç´°çš„æ¯”å°çµæžœ"""
        detailed_results = []
        
        for i in range(len(original_data)):
            row = original_data.iloc[i]
            
            # ç²å–åŽŸå§‹æ–‡æœ¬å’Œæ¨™ç±¤
            original_text = str(row.get('processed_text', row.get('text', row.get('review', ''))))
            original_label = str(row.get('sentiment', 'unknown'))
            
            # æˆªæ–·éŽé•·çš„æ–‡æœ¬
            if len(original_text) > 100:
                display_text = original_text[:97] + "..."
            else:
                display_text = original_text
            
            predicted_label = predicted_labels[i]
            is_correct = predicted_label == original_label
            confidence = float(np.max(probabilities[i]))
            
            detailed_results.append({
                'index': i,
                'original_text': display_text,
                'original_label': original_label,
                'predicted_label': predicted_label,
                'is_correct': is_correct,
                'confidence': confidence
            })
        
        return detailed_results 