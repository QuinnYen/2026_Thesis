#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº¤å‰é©—è­‰æ¨¡çµ„ - æ”¯æ´ K æŠ˜äº¤å‰é©—è­‰
ç‚º BERT æƒ…æ„Ÿåˆ†æç³»çµ±æä¾›ç©©å¥çš„æ¨¡å‹è©•ä¼°
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import logging
import json
import os
from datetime import datetime
import time

logger = logging.getLogger(__name__)

class CrossValidationEvaluator:
    """K æŠ˜äº¤å‰é©—è­‰è©•ä¼°å™¨"""
    
    def __init__(self, output_dir: Optional[str] = None, n_folds: int = 5, random_state: int = 42):
        """
        åˆå§‹åŒ–äº¤å‰é©—è­‰è©•ä¼°å™¨
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            n_folds: æŠ˜æ•¸ (æ¨è–¦ 5 æˆ– 10)
            random_state: éš¨æ©Ÿç¨®å­
        """
        self.output_dir = output_dir
        self.n_folds = n_folds
        self.random_state = random_state
        self.cv_results = {}
        
        # å‰µå»ºåˆ†å±¤ K æŠ˜äº¤å‰é©—è­‰å™¨
        self.skf = StratifiedKFold(
            n_splits=n_folds, 
            shuffle=True, 
            random_state=random_state
        )
        
        logger.info(f"äº¤å‰é©—è­‰è©•ä¼°å™¨å·²åˆå§‹åŒ– - {n_folds} æŠ˜äº¤å‰é©—è­‰")
    
    def evaluate_single_model(self, 
                            features: np.ndarray, 
                            labels: np.ndarray, 
                            model, 
                            model_name: str,
                            label_encoder = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ K æŠ˜äº¤å‰é©—è­‰è©•ä¼°å–®å€‹æ¨¡å‹
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
            model: sklearn å…¼å®¹çš„æ¨¡å‹
            model_name: æ¨¡å‹åç¨±
            label_encoder: æ¨™ç±¤ç·¨ç¢¼å™¨
            
        Returns:
            äº¤å‰é©—è­‰çµæœå­—å…¸
        """
        print(f"\nğŸ”„ é–‹å§‹ {self.n_folds} æŠ˜äº¤å‰é©—è­‰: {model_name}")
        logger.info(f"é–‹å§‹äº¤å‰é©—è­‰: {model_name}")
        
        start_time = time.time()
        
        # å®šç¾©è©•ä¼°æŒ‡æ¨™
        scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']
        
        # åŸ·è¡Œäº¤å‰é©—è­‰
        cv_results = cross_validate(
            model, features, labels, 
            cv=self.skf, 
            scoring=scoring,
            return_train_score=True,
            n_jobs=-1  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        )
        
        # æ‰‹å‹•è¨ˆç®—æ¯å€‹ fold çš„è©³ç´°çµæœ
        fold_details = []
        fold_num = 1
        
        for train_idx, test_idx in self.skf.split(features, labels):
            print(f"   ğŸ“Š Fold {fold_num}/{self.n_folds}")
            
            # åˆ†å‰²æ•¸æ“š
            X_train, X_test = features[train_idx], features[test_idx]
            y_train, y_test = labels[train_idx], labels[test_idx]
            
            # è¨“ç·´æ¨¡å‹
            model_copy = self._clone_model(model)
            model_copy.fit(X_train, y_train)
            
            # é æ¸¬
            train_pred = model_copy.predict(X_train)
            test_pred = model_copy.predict(X_test)
            test_pred_proba = model_copy.predict_proba(X_test)
            
            # è¨ˆç®—æŒ‡æ¨™
            fold_metrics = self._calculate_fold_metrics(
                y_train, train_pred, y_test, test_pred, test_pred_proba, label_encoder
            )
            fold_metrics['fold'] = fold_num
            fold_metrics['train_size'] = len(X_train)
            fold_metrics['test_size'] = len(X_test)
            
            fold_details.append(fold_metrics)
            fold_num += 1
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        summary_stats = self._calculate_summary_statistics(cv_results, fold_details)
        
        total_time = time.time() - start_time
        
        result = {
            'model_name': model_name,
            'n_folds': self.n_folds,
            'cv_scores': {
                'test_accuracy': cv_results['test_accuracy'].tolist(),
                'test_precision': cv_results['test_precision_weighted'].tolist(),
                'test_recall': cv_results['test_recall_weighted'].tolist(),
                'test_f1': cv_results['test_f1_weighted'].tolist(),
                'train_accuracy': cv_results['train_accuracy'].tolist(),
                'train_precision': cv_results['train_precision_weighted'].tolist(),
                'train_recall': cv_results['train_recall_weighted'].tolist(),
                'train_f1': cv_results['train_f1_weighted'].tolist()
            },
            'summary_statistics': summary_stats,
            'fold_details': fold_details,
            'total_evaluation_time': total_time,
            'timestamp': datetime.now().isoformat()
        }
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        self._print_cv_summary(model_name, summary_stats)
        
        return result
    
    def evaluate_multiple_models(self, 
                                features: np.ndarray, 
                                labels: np.ndarray, 
                                models_dict: Dict[str, Any],
                                label_encoder = None) -> Dict[str, Dict]:
        """
        è©•ä¼°å¤šå€‹æ¨¡å‹çš„äº¤å‰é©—è­‰æ€§èƒ½
        
        Args:
            features: ç‰¹å¾µçŸ©é™£
            labels: æ¨™ç±¤æ•¸çµ„
            models_dict: æ¨¡å‹å­—å…¸ {åç¨±: æ¨¡å‹å¯¦ä¾‹}
            label_encoder: æ¨™ç±¤ç·¨ç¢¼å™¨
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„äº¤å‰é©—è­‰çµæœ
        """
        print(f"\nğŸ”¬ é–‹å§‹å¤šæ¨¡å‹ {self.n_folds} æŠ˜äº¤å‰é©—è­‰æ¯”è¼ƒ")
        print(f"   â€¢ æ¨¡å‹æ•¸é‡: {len(models_dict)}")
        print(f"   â€¢ æ•¸æ“šè¦æ¨¡: {features.shape[0]} æ¨£æœ¬, {features.shape[1]} ç‰¹å¾µ")
        print(f"   â€¢ é¡åˆ¥æ•¸é‡: {len(np.unique(labels))}")
        
        all_results = {}
        
        for model_name, model in models_dict.items():
            try:
                result = self.evaluate_single_model(
                    features, labels, model, model_name, label_encoder
                )
                all_results[model_name] = result
            except Exception as e:
                logger.error(f"è©•ä¼°æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                all_results[model_name] = {
                    'error': str(e),
                    'model_name': model_name,
                    'timestamp': datetime.now().isoformat()
                }
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        comparison = self._compare_models(all_results)
        
        final_results = {
            'individual_results': all_results,
            'comparison': comparison,
            'evaluation_config': {
                'n_folds': self.n_folds,
                'random_state': self.random_state,
                'total_samples': features.shape[0],
                'n_features': features.shape[1],
                'n_classes': len(np.unique(labels))
            }
        }
        
        # ä¿å­˜çµæœ
        if self.output_dir:
            self._save_cv_results(final_results)
        
        return final_results
    
    def evaluate_attention_mechanisms_cv(self, 
                                       attention_results: Dict[str, Any],
                                       metadata: pd.DataFrame,
                                       original_embeddings: np.ndarray,
                                       models_dict: Dict[str, Any],
                                       label_encoder = None) -> Dict[str, Dict]:
        """
        ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°ä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ•ˆæœ
        
        Args:
            attention_results: æ³¨æ„åŠ›æ©Ÿåˆ¶è™•ç†çµæœ
            metadata: å…ƒæ•¸æ“šï¼ˆåŒ…å«æ¨™ç±¤ï¼‰
            original_embeddings: åŸå§‹åµŒå…¥å‘é‡
            models_dict: æ¨¡å‹å­—å…¸
            label_encoder: æ¨™ç±¤ç·¨ç¢¼å™¨
            
        Returns:
            å„æ³¨æ„åŠ›æ©Ÿåˆ¶çš„äº¤å‰é©—è­‰çµæœ
        """
        print(f"\nğŸ¯ é–‹å§‹æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰è©•ä¼°")
        
        # æª¢æŸ¥æƒ…æ„Ÿæ¨™ç±¤æ¬„ä½ï¼Œå¦‚æœæ²’æœ‰å‰‡æ ¹æ“šreview_starsç”Ÿæˆ
        if 'sentiment' not in metadata.columns:
            if 'review_stars' in metadata.columns:
                print("æœªæ‰¾åˆ° 'sentiment' æ¬„ä½ï¼Œæ ¹æ“š 'review_stars' ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤...")
                # æ ¹æ“šè©•åˆ†ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤ï¼š1-2æ˜Ÿ=è² é¢, 3æ˜Ÿ=ä¸­æ€§, 4-5æ˜Ÿ=æ­£é¢
                def map_stars_to_sentiment(stars):
                    if stars <= 2:
                        return 'negative'
                    elif stars == 3:
                        return 'neutral'
                    else:
                        return 'positive'
                
                metadata = metadata.copy()
                metadata['sentiment'] = metadata['review_stars'].apply(map_stars_to_sentiment)
                print(f"ç”Ÿæˆçš„æƒ…æ„Ÿæ¨™ç±¤åˆ†ä½ˆï¼š{metadata['sentiment'].value_counts().to_dict()}")
            else:
                raise ValueError("å…ƒæ•¸æ“šä¸­ç¼ºå°‘ 'sentiment' æ¬„ä½ï¼Œä¸”ç„¡æ³•æ‰¾åˆ° 'review_stars' æ¬„ä½ä¾†ç”Ÿæˆæƒ…æ„Ÿæ¨™ç±¤")
        
        # ç·¨ç¢¼æ¨™ç±¤
        if label_encoder is None:
            from sklearn.preprocessing import LabelEncoder
            label_encoder = LabelEncoder()
            labels = label_encoder.fit_transform(metadata['sentiment'].values)
        else:
            labels = label_encoder.transform(metadata['sentiment'].values)
        
        attention_cv_results = {}
        
        # ç‚ºæ¯å€‹æ³¨æ„åŠ›æ©Ÿåˆ¶é€²è¡Œäº¤å‰é©—è­‰
        for attention_type, attention_data in attention_results.items():
            if attention_type in ['processing_info', 'comparison']:
                continue
                
            print(f"\nğŸ” è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶: {attention_type}")
            
            try:
                # æº–å‚™ç‰¹å¾µ
                if attention_type == 'no':
                    # ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œä½¿ç”¨åŸå§‹åµŒå…¥å‘é‡
                    features = original_embeddings
                else:
                    # ä½¿ç”¨é¢å‘ç‰¹å¾µå‘é‡
                    aspect_vectors = attention_data.get('aspect_vectors', {})
                    features = self._prepare_attention_features(
                        aspect_vectors, original_embeddings
                    )
                
                # å°æ¯å€‹æ¨¡å‹é€²è¡Œäº¤å‰é©—è­‰
                attention_model_results = {}
                for model_name, model in models_dict.items():
                    print(f"   ğŸ“Š {model_name} + {attention_type}")
                    
                    cv_result = self.evaluate_single_model(
                        features, labels, model, 
                        f"{model_name}_{attention_type}", label_encoder
                    )
                    attention_model_results[model_name] = cv_result
                
                attention_cv_results[attention_type] = {
                    'model_results': attention_model_results,
                    'feature_shape': features.shape,
                    'attention_type': attention_type
                }
                
            except Exception as e:
                logger.error(f"è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶ {attention_type} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                attention_cv_results[attention_type] = {
                    'error': str(e),
                    'attention_type': attention_type
                }
        
        # ç”Ÿæˆæ³¨æ„åŠ›æ©Ÿåˆ¶æ¯”è¼ƒ
        attention_comparison = self._compare_attention_mechanisms(attention_cv_results)
        
        final_results = {
            'attention_results': attention_cv_results,
            'attention_comparison': attention_comparison,
            'evaluation_config': {
                'n_folds': self.n_folds,
                'random_state': self.random_state,
                'n_attention_types': len([k for k in attention_results.keys() 
                                        if k not in ['processing_info', 'comparison']]),
                'n_models': len(models_dict)
            }
        }
        
        # ä¿å­˜çµæœ
        if self.output_dir:
            self._save_attention_cv_results(final_results)
        
        return final_results
    
    def _clone_model(self, model):
        """è¤‡è£½æ¨¡å‹å¯¦ä¾‹"""
        from sklearn.base import clone
        return clone(model)
    
    def _calculate_fold_metrics(self, y_train, train_pred, y_test, test_pred, 
                              test_pred_proba, label_encoder) -> Dict[str, Any]:
        """è¨ˆç®—å–®å€‹ fold çš„è©³ç´°æŒ‡æ¨™"""
        # åŸºæœ¬æŒ‡æ¨™
        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        
        # ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1
        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(
            y_train, train_pred, average='weighted'
        )
        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(
            y_test, test_pred, average='weighted'
        )
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_test, test_pred)
        
        # åˆ†é¡å ±å‘Š
        if label_encoder and hasattr(label_encoder, 'classes_'):
            target_names = label_encoder.classes_
        else:
            target_names = None
        
        class_report = classification_report(
            y_test, test_pred, target_names=target_names, output_dict=True
        )
        
        return {
            'train_accuracy': float(train_acc),
            'test_accuracy': float(test_acc),
            'train_precision': float(train_precision),
            'test_precision': float(test_precision),
            'train_recall': float(train_recall),
            'test_recall': float(test_recall),
            'train_f1': float(train_f1),
            'test_f1': float(test_f1),
            'confusion_matrix': cm.tolist(),
            'classification_report': class_report
        }
    
    def _calculate_summary_statistics(self, cv_results, fold_details) -> Dict[str, Any]:
        """è¨ˆç®—äº¤å‰é©—è­‰çš„çµ±è¨ˆæ‘˜è¦"""
        stats = {}
        
        # å°æ¯å€‹æŒ‡æ¨™è¨ˆç®—å¹³å‡å€¼å’Œæ¨™æº–å·®
        for metric in ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']:
            test_scores = cv_results[f'test_{metric}']
            train_scores = cv_results[f'train_{metric}']
            
            metric_name = metric.replace('_weighted', '')
            stats[f'test_{metric_name}_mean'] = float(np.mean(test_scores))
            stats[f'test_{metric_name}_std'] = float(np.std(test_scores))
            stats[f'test_{metric_name}_min'] = float(np.min(test_scores))
            stats[f'test_{metric_name}_max'] = float(np.max(test_scores))
            
            stats[f'train_{metric_name}_mean'] = float(np.mean(train_scores))
            stats[f'train_{metric_name}_std'] = float(np.std(train_scores))
        
        # éæ“¬åˆæŒ‡æ¨™ (è¨“ç·´åˆ†æ•¸ - æ¸¬è©¦åˆ†æ•¸)
        train_test_gap = np.mean(cv_results['train_accuracy']) - np.mean(cv_results['test_accuracy'])
        stats['overfitting_score'] = float(train_test_gap)
        
        # ç©©å®šæ€§æŒ‡æ¨™ (æ¨™æº–å·®)
        stats['stability_score'] = float(np.std(cv_results['test_accuracy']))
        
        return stats
    
    def _prepare_attention_features(self, aspect_vectors, original_embeddings) -> np.ndarray:
        """æº–å‚™æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µ"""
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›å¯èƒ½éœ€è¦æ›´è¤‡é›œçš„ç‰¹å¾µçµ„åˆ
        if not aspect_vectors:
            return original_embeddings
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„é¢å‘å‘é‡
        first_aspect = list(aspect_vectors.keys())[0]
        aspect_features = aspect_vectors[first_aspect]
        
        # çµ„åˆåŸå§‹åµŒå…¥å’Œé¢å‘ç‰¹å¾µ
        if len(aspect_features) == len(original_embeddings):
            combined_features = np.hstack([original_embeddings, aspect_features])
            return combined_features
        else:
            return original_embeddings
    
    def _compare_models(self, all_results) -> Dict[str, Any]:
        """æ¯”è¼ƒå¤šå€‹æ¨¡å‹çš„æ€§èƒ½"""
        comparison = {
            'ranking': [],
            'best_model': None,
            'performance_summary': {}
        }
        
        # æ”¶é›†å„æ¨¡å‹çš„å¹³å‡æ¸¬è©¦æº–ç¢ºç‡
        model_scores = {}
        for model_name, result in all_results.items():
            if 'error' not in result:
                acc_mean = result['summary_statistics']['test_accuracy_mean']
                f1_mean = result['summary_statistics']['test_f1_mean']
                stability = result['summary_statistics']['stability_score']
                
                # ç¶œåˆåˆ†æ•¸ (æº–ç¢ºç‡ 70% + F1åˆ†æ•¸ 20% + ç©©å®šæ€§ 10%)
                composite_score = acc_mean * 0.7 + f1_mean * 0.2 + (1 - stability) * 0.1
                
                model_scores[model_name] = {
                    'accuracy_mean': acc_mean,
                    'f1_mean': f1_mean,
                    'stability_score': stability,
                    'composite_score': composite_score
                }
        
        # æ’åº
        sorted_models = sorted(model_scores.items(), 
                             key=lambda x: x[1]['composite_score'], 
                             reverse=True)
        
        comparison['ranking'] = [
            {
                'rank': i + 1,
                'model_name': model_name,
                **scores
            }
            for i, (model_name, scores) in enumerate(sorted_models)
        ]
        
        if sorted_models:
            comparison['best_model'] = sorted_models[0][0]
            comparison['performance_summary'] = {
                'best_accuracy': max(s['accuracy_mean'] for s in model_scores.values()),
                'best_f1': max(s['f1_mean'] for s in model_scores.values()),
                'most_stable': min(s['stability_score'] for s in model_scores.values()),
                'n_successful_models': len(model_scores)
            }
        
        return comparison
    
    def _compare_attention_mechanisms(self, attention_cv_results) -> Dict[str, Any]:
        """æ¯”è¼ƒä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ€§èƒ½"""
        comparison = {
            'attention_ranking': [],
            'best_attention_model_combo': None,
            'mechanism_summary': {}
        }
        
        # æ”¶é›†å„æ³¨æ„åŠ›æ©Ÿåˆ¶+æ¨¡å‹çµ„åˆçš„åˆ†æ•¸
        combo_scores = {}
        
        for attention_type, attention_data in attention_cv_results.items():
            if 'error' in attention_data:
                continue
                
            model_results = attention_data.get('model_results', {})
            for model_name, result in model_results.items():
                if 'error' not in result:
                    combo_name = f"{attention_type}+{model_name}"
                    acc_mean = result['summary_statistics']['test_accuracy_mean']
                    f1_mean = result['summary_statistics']['test_f1_mean']
                    stability = result['summary_statistics']['stability_score']
                    
                    combo_scores[combo_name] = {
                        'attention_type': attention_type,
                        'model_name': model_name,
                        'accuracy_mean': acc_mean,
                        'f1_mean': f1_mean,
                        'stability_score': stability,
                        'composite_score': acc_mean * 0.7 + f1_mean * 0.2 + (1 - stability) * 0.1
                    }
        
        # æ’åºçµ„åˆ
        sorted_combos = sorted(combo_scores.items(), 
                             key=lambda x: x[1]['composite_score'], 
                             reverse=True)
        
        comparison['attention_ranking'] = [
            {
                'rank': i + 1,
                'combination': combo_name,
                **scores
            }
            for i, (combo_name, scores) in enumerate(sorted_combos)
        ]
        
        if sorted_combos:
            comparison['best_attention_model_combo'] = sorted_combos[0][0]
        
        return comparison
    
    def _print_cv_summary(self, model_name: str, stats: Dict[str, Any]):
        """é¡¯ç¤ºäº¤å‰é©—è­‰çµæœæ‘˜è¦"""
        print(f"\nğŸ“Š {model_name} äº¤å‰é©—è­‰çµæœ:")
        print(f"   â€¢ å¹³å‡æº–ç¢ºç‡: {stats['test_accuracy_mean']:.4f} Â± {stats['test_accuracy_std']:.4f}")
        print(f"   â€¢ å¹³å‡ F1 åˆ†æ•¸: {stats['test_f1_mean']:.4f} Â± {stats['test_f1_std']:.4f}")
        print(f"   â€¢ ç©©å®šæ€§ (Ïƒ): {stats['stability_score']:.4f}")
        print(f"   â€¢ éæ“¬åˆæŒ‡æ¨™: {stats['overfitting_score']:.4f}")
    
    def _save_cv_results(self, results: Dict[str, Any]):
        """ä¿å­˜äº¤å‰é©—è­‰çµæœ"""
        if not self.output_dir:
            return
            
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = os.path.join(self.output_dir, f"cv_{self.n_folds}fold_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"äº¤å‰é©—è­‰çµæœå·²ä¿å­˜: {results_file}")
    
    def _save_attention_cv_results(self, results: Dict[str, Any]):
        """ä¿å­˜æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰çµæœ"""
        if not self.output_dir:
            return
            
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰çµæœ
        results_file = os.path.join(self.output_dir, f"attention_cv_{self.n_folds}fold_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ³¨æ„åŠ›æ©Ÿåˆ¶äº¤å‰é©—è­‰çµæœå·²ä¿å­˜: {results_file}") 