"""
èåˆæµç¨‹ç®¡ç·š - æ•´åˆæ–°çš„æƒ…æ„Ÿåˆ†ææ¶æ§‹
åŒ…å«æ–‡å­—é è™•ç†ã€ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶ä¸¦è¡Œè¨ˆç®—ã€é–€æ§èåˆå’Œåˆ†é¡é æ¸¬
"""

import os
import numpy as np
import pandas as pd
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import json

# å°å…¥å¿…è¦çš„æ¨¡çµ„
from .text_preprocessor import TextPreprocessor
from .attention_fusion_network import AttentionFusionProcessor
from .sentiment_classifier import SentimentClassifier
from .run_manager import RunManager

# åŒ¯å…¥éŒ¯èª¤è™•ç†å·¥å…·
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.error_handler import handle_error, handle_warning, handle_info
from utils.storage_manager import StorageManager

logger = logging.getLogger(__name__)

class FusionPipeline:
    """èåˆæµç¨‹ç®¡ç·š - å®Œæ•´çš„æ–°æ¶æ§‹å¯¦ç¾"""
    
    def __init__(self, output_dir: Optional[str] = None, encoder_type: str = 'bert', config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–èåˆæµç¨‹ç®¡ç·š
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            encoder_type: ç·¨ç¢¼å™¨é¡å‹
            config: é…ç½®åƒæ•¸
        """
        self.output_dir = output_dir
        self.encoder_type = encoder_type
        self.config = config or {}
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.text_preprocessor = TextPreprocessor(output_dir)
        self.attention_fusion = AttentionFusionProcessor()
        self.classifier = SentimentClassifier(output_dir, encoder_type)
        
        # åˆå§‹åŒ–ç®¡ç†çµ„ä»¶
        self.run_manager = RunManager(output_dir) if output_dir else None
        self.storage_manager = StorageManager(output_dir) if output_dir else None
        
        logger.info("èåˆæµç¨‹ç®¡ç·šå·²åˆå§‹åŒ–")
        logger.info(f"ç·¨ç¢¼å™¨é¡å‹: {encoder_type.upper()}")
        logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    def run_complete_pipeline(self, 
                            input_data: pd.DataFrame,
                            text_column: str,
                            test_size: float = 0.2,
                            save_results: bool = True) -> Dict[str, Any]:
        """
        é‹è¡Œå®Œæ•´çš„æ–°æ¶æ§‹æµç¨‹
        
        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            text_column: æ–‡æœ¬æ¬„ä½åç¨±
            test_size: æ¸¬è©¦é›†æ¯”ä¾‹
            save_results: æ˜¯å¦ä¿å­˜çµæœ
            
        Returns:
            å®Œæ•´çš„æµç¨‹çµæœ
        """
        pipeline_start = datetime.now()
        logger.info("="*60)
        logger.info("é–‹å§‹æ–°æ¶æ§‹èåˆæµç¨‹")
        logger.info("="*60)
        
        results = {
            'pipeline_info': {
                'start_time': pipeline_start.isoformat(),
                'architecture': 'attention_fusion_network',
                'encoder_type': self.encoder_type,
                'stages': []
            }
        }
        
        try:
            # éšæ®µ 1: æ–‡å­—é è™•ç†å’Œæ¨™ç±¤è½‰æ›
            stage_start = datetime.now()
            print("ğŸ“ éšæ®µ 1: æ–‡å­—é è™•ç†...")
            
            processed_data = self.text_preprocessor.preprocess(input_data, text_column)
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 1,
                'name': 'æ–‡å­—é è™•ç†å’Œæ¨™ç±¤è½‰æ›',
                'duration_seconds': stage_time,
                'output_shape': processed_data.shape,
                'sentiment_distribution': processed_data['sentiment_numeric'].value_counts().to_dict() if 'sentiment_numeric' in processed_data.columns else {}
            })
            
            print(f"   âœ… å®Œæˆ - {processed_data.shape[0]} æ¢è¨˜éŒ„ ({stage_time:.1f}s)")
            
            # éšæ®µ 2: ç²å–æ–‡æœ¬åµŒå…¥
            stage_start = datetime.now()
            print(f"ğŸ¤– éšæ®µ 2: ç”Ÿæˆ{self.encoder_type.upper()}åµŒå…¥å‘é‡...")
            
            embeddings = self._get_text_embeddings(processed_data)
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 2,
                'name': f'{self.encoder_type.upper()}æ–‡æœ¬åµŒå…¥',
                'duration_seconds': stage_time,
                'embeddings_shape': embeddings.shape
            })
            
            print(f"   âœ… å®Œæˆ - å½¢ç‹€{embeddings.shape} ({stage_time:.1f}s)")
            
            # éšæ®µ 3: ä¸¦è¡Œè¨ˆç®—ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µ
            stage_start = datetime.now()
            print("âš¡ éšæ®µ 3: ä¸¦è¡Œè¨ˆç®—ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶...")
            
            similarity_features, keyword_features, self_attention_features, attention_info = \
                self.attention_fusion.compute_parallel_attention_features(embeddings, processed_data)
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 3,
                'name': 'ä¸¦è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶è¨ˆç®—',
                'duration_seconds': stage_time,
                'attention_info': attention_info,
                'feature_shapes': {
                    'similarity': similarity_features.shape,
                    'keyword': keyword_features.shape,
                    'self_attention': self_attention_features.shape
                }
            })
            
            print(f"   âœ… å®Œæˆ - ä¸‰ç¨®æ³¨æ„åŠ›ç‰¹å¾µ ({stage_time:.1f}s)")
            
            # éšæ®µ 4: é–€æ§èåˆç¶²è·¯
            stage_start = datetime.now()
            print("ğŸ”€ éšæ®µ 4: é–€æ§èåˆç¶²è·¯...")
            
            fused_features, fusion_info = self.attention_fusion.fuse_attention_features(
                similarity_features, keyword_features, self_attention_features
            )
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 4,
                'name': 'é–€æ§èåˆç¶²è·¯',
                'duration_seconds': stage_time,
                'fusion_info': fusion_info,
                'fused_features_shape': fused_features.shape
            })
            
            # é¡¯ç¤ºé‡è¦çš„æ¬Šé‡ä¿¡æ¯
            avg_weights = fusion_info.get('average_weights', {})
            weights_str = f"ç›¸ä¼¼åº¦:{avg_weights.get('similarity', 0):.3f} | é—œéµè©:{avg_weights.get('keyword', 0):.3f} | è‡ªæ³¨æ„åŠ›:{avg_weights.get('self_attention', 0):.3f}"
            print(f"   âœ… å®Œæˆ - æ¬Šé‡åˆ†é…: {weights_str} ({stage_time:.1f}s)")
            
            # éšæ®µ 5: åˆ†é¡å™¨è¨“ç·´å’Œé æ¸¬
            stage_start = datetime.now()
            print("ğŸ¯ éšæ®µ 5: è¨“ç·´åˆ†é¡å™¨...")
            
            # æº–å‚™åˆ†é¡ç‰¹å¾µï¼ˆä½¿ç”¨èåˆç‰¹å¾µï¼‰
            features, labels = self.classifier.prepare_features(
                aspect_vectors={},  # ä¸éœ€è¦aspect_vectorsï¼Œå› ç‚ºä½¿ç”¨èåˆç‰¹å¾µ
                metadata=processed_data,
                fused_features=fused_features
            )
            
            # è¨“ç·´åˆ†é¡å™¨
            classification_results = self.classifier.train(
                features, labels, 
                test_size=test_size,
                original_data=processed_data
            )
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 5,
                'name': 'åˆ†é¡å™¨è¨“ç·´å’Œé æ¸¬',
                'duration_seconds': stage_time,
                'classification_results': {
                    'test_accuracy': classification_results['test_accuracy'],
                    'test_f1': classification_results['test_f1'],
                    'test_precision': classification_results['test_precision'],
                    'test_recall': classification_results['test_recall']
                }
            })
            
            print(f"   âœ… å®Œæˆ - æº–ç¢ºç‡:{classification_results['test_accuracy']:.4f} ({stage_time:.1f}s)")
            
            # éšæ®µ 6: èˆ‡åŸå§‹æ¨™ç±¤æ¯”è¼ƒæº–ç¢ºç‡
            stage_start = datetime.now()
            print("ğŸ“Š éšæ®µ 6: çµæœåˆ†æ...")
            
            accuracy_analysis = self._analyze_accuracy(classification_results, processed_data)
            
            stage_time = (datetime.now() - stage_start).total_seconds()
            results['pipeline_info']['stages'].append({
                'stage': 6,
                'name': 'æº–ç¢ºç‡åˆ†æ',
                'duration_seconds': stage_time,
                'accuracy_analysis': accuracy_analysis
            })
            
            print(f"   âœ… å®Œæˆ - åˆ†æå ±å‘Šç”Ÿæˆ ({stage_time:.1f}s)")
            
            # æ•´åˆæ‰€æœ‰çµæœ
            pipeline_end = datetime.now()
            total_time = (pipeline_end - pipeline_start).total_seconds()
            
            results.update({
                'preprocessing_results': {
                    'original_shape': input_data.shape,
                    'processed_shape': processed_data.shape,
                    'sentiment_encoding': {
                        0: 'è² é¢',
                        1: 'ä¸­æ€§', 
                        2: 'æ­£é¢'
                    }
                },
                'attention_results': {
                    'similarity_features': similarity_features,
                    'keyword_features': keyword_features,
                    'self_attention_features': self_attention_features,
                    'attention_info': attention_info
                },
                'fusion_results': {
                    'fused_features': fused_features,
                    'fusion_info': fusion_info,
                    'gate_weights': fusion_info.get('average_weights', {})
                },
                'classification_results': classification_results,
                'accuracy_analysis': accuracy_analysis,
                'pipeline_info': {
                    **results['pipeline_info'],
                    'end_time': pipeline_end.isoformat(),
                    'total_duration_seconds': total_time,
                    'success': True
                }
            })
            
            # é¡¯ç¤ºç°¡æ½”çš„æœ€çµ‚çµæœ
            print("\n" + "="*50)
            print("ğŸ‰ æ–°èåˆæ¶æ§‹å®Œæˆï¼")
            print("="*50)
            print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.1f} ç§’")
            print(f"ğŸ¯ æœ€çµ‚æº–ç¢ºç‡: {classification_results['test_accuracy']:.4f}")
            print(f"ğŸ“ˆ F1åˆ†æ•¸: {classification_results['test_f1']:.4f}")
            
            # é¡¯ç¤ºé–€æ§æ¬Šé‡åˆ†é…
            avg_weights = fusion_info.get('average_weights', {})
            print(f"âš–ï¸  æ¬Šé‡åˆ†é…:")
            print(f"   ç›¸ä¼¼åº¦æ³¨æ„åŠ›: {avg_weights.get('similarity', 0):.3f}")
            print(f"   é—œéµè©æ³¨æ„åŠ›: {avg_weights.get('keyword', 0):.3f}")
            print(f"   è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶: {avg_weights.get('self_attention', 0):.3f}")
            
            # é¡¯ç¤ºå„é¡åˆ¥æ€§èƒ½
            per_class = accuracy_analysis.get('per_class_metrics', {})
            if per_class:
                print(f"ğŸ“Š å„é¡åˆ¥F1åˆ†æ•¸:")
                for class_name, metrics in per_class.items():
                    print(f"   {class_name}: {metrics.get('f1_score', 0):.3f}")
            
            print("="*50)
            
            # ä¿å­˜çµæœ
            if save_results and self.output_dir:
                self._save_pipeline_results(results)
            
            return results
            
        except Exception as e:
            error_time = datetime.now()
            error_info = {
                'error': str(e),
                'error_time': error_time.isoformat(),
                'error_stage': len(results['pipeline_info']['stages']) + 1
            }
            results['pipeline_info'].update(error_info)
            
            logger.error(f"æµç¨‹åœ¨éšæ®µ {error_info['error_stage']} ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            handle_error(e, "èåˆæµç¨‹ç®¡ç·š")
            raise
    
    def _get_text_embeddings(self, processed_data: pd.DataFrame) -> np.ndarray:
        """ç²å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        # å˜—è©¦å¾å­˜å„²ç®¡ç†å™¨ç²å–å·²å­˜åœ¨çš„åµŒå…¥
        if self.storage_manager:
            existing_path = self.storage_manager.check_existing_embeddings(self.encoder_type)
            if existing_path:
                logger.info(f"è¼‰å…¥å·²å­˜åœ¨çš„{self.encoder_type.upper()}åµŒå…¥å‘é‡: {existing_path}")
                return np.load(existing_path)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œç”Ÿæˆæ–°çš„åµŒå…¥
        logger.info(f"ç”Ÿæˆæ–°çš„{self.encoder_type.upper()}åµŒå…¥å‘é‡...")
        
        if self.encoder_type == 'bert':
            from .bert_encoder import BertEncoder
            encoder = BertEncoder(output_dir=self.output_dir)
        else:
            # ä½¿ç”¨æ¨¡çµ„åŒ–ç·¨ç¢¼å™¨
            try:
                from .text_encoders import TextEncoderFactory
                encoder = TextEncoderFactory.create_encoder(
                    self.encoder_type, 
                    output_dir=self.output_dir
                )
            except Exception as e:
                logger.warning(f"ç„¡æ³•å‰µå»º{self.encoder_type}ç·¨ç¢¼å™¨ï¼Œå›é€€åˆ°BERT: {e}")
                from .bert_encoder import BertEncoder
                encoder = BertEncoder(output_dir=self.output_dir)
                self.encoder_type = 'bert'
        
        embeddings = encoder.encode(processed_data['processed_text'])
        
        # ä¿å­˜åµŒå…¥å‘é‡
        if self.storage_manager:
            self.storage_manager.save_embeddings(embeddings, self.encoder_type)
        
        return embeddings
    
    def _analyze_accuracy(self, classification_results: Dict, processed_data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æåˆ†é¡æº–ç¢ºç‡"""
        analysis = {
            'overall_metrics': {
                'test_accuracy': classification_results['test_accuracy'],
                'test_precision': classification_results['test_precision'],
                'test_recall': classification_results['test_recall'],
                'test_f1': classification_results['test_f1']
            },
            'confusion_matrix': classification_results['confusion_matrix'],
            'classification_report': classification_results['classification_report'],
            'label_mapping': {
                0: 'è² é¢',
                1: 'ä¸­æ€§',
                2: 'æ­£é¢'
            }
        }
        
        # åˆ†ææ¯å€‹é¡åˆ¥çš„æ€§èƒ½
        per_class_analysis = {}
        class_names = ['è² é¢', 'ä¸­æ€§', 'æ­£é¢']
        
        for i, class_name in enumerate(class_names):
            if i < len(classification_results.get('per_class_precision', [])):
                per_class_analysis[class_name] = {
                    'precision': classification_results['per_class_precision'][i],
                    'recall': classification_results['per_class_recall'][i],
                    'f1_score': classification_results['per_class_f1'][i],
                    'support': classification_results['per_class_support'][i]
                }
        
        analysis['per_class_metrics'] = per_class_analysis
        
        # é æ¸¬è©³æƒ…åˆ†æ
        if 'prediction_details' in classification_results:
            pred_details = classification_results['prediction_details']
            
            # è¨ˆç®—é æ¸¬åˆ†ä½ˆ
            pred_counts = {}
            true_counts = {}
            
            for true_label, pred_label in zip(pred_details['true_labels'], pred_details['predicted_labels']):
                true_name = class_names[true_label] if true_label < len(class_names) else f'é¡åˆ¥_{true_label}'
                pred_name = class_names[pred_label] if pred_label < len(class_names) else f'é¡åˆ¥_{pred_label}'
                
                true_counts[true_name] = true_counts.get(true_name, 0) + 1
                pred_counts[pred_name] = pred_counts.get(pred_name, 0) + 1
            
            analysis['prediction_distribution'] = {
                'true_labels': true_counts,
                'predicted_labels': pred_counts
            }
        
        return analysis
    
    def _save_pipeline_results(self, results: Dict[str, Any]):
        """ä¿å­˜æµç¨‹çµæœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if self.storage_manager:
                # ä½¿ç”¨å­˜å„²ç®¡ç†å™¨ä¿å­˜çµæœ
                self.storage_manager.save_analysis_results(
                    results,
                    "fusion_pipeline",
                    f"fusion_pipeline_results_{timestamp}.json"
                )
                
                # å–®ç¨ä¿å­˜èåˆç‰¹å¾µ
                if 'fusion_results' in results and 'fused_features' in results['fusion_results']:
                    fused_features = results['fusion_results']['fused_features']
                    self.storage_manager.save_features(
                        fused_features,
                        f"fused_features_{timestamp}.npy"
                    )
                
                logger.info("æµç¨‹çµæœå·²é€šéå­˜å„²ç®¡ç†å™¨ä¿å­˜")
            else:
                # ç›´æ¥ä¿å­˜åˆ°è¼¸å‡ºç›®éŒ„
                results_file = os.path.join(self.output_dir, f"fusion_pipeline_results_{timestamp}.json")
                
                # æº–å‚™å¯åºåˆ—åŒ–çš„çµæœ
                serializable_results = self._make_json_serializable(results)
                
                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(serializable_results, f, ensure_ascii=False, indent=2)
                
                logger.info(f"æµç¨‹çµæœå·²ä¿å­˜åˆ°: {results_file}")
                
        except Exception as e:
            logger.error(f"ä¿å­˜æµç¨‹çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def _make_json_serializable(self, obj):
        """å°‡å°è±¡è½‰æ›ç‚ºJSONå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, np.ndarray):
            return {
                '__type__': 'numpy_array',
                'shape': obj.shape,
                'dtype': str(obj.dtype),
                'data': obj.tolist()
            }
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: self._make_json_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        else:
            return obj
    
    def compare_with_baseline(self, 
                            input_data: pd.DataFrame,
                            text_column: str,
                            baseline_methods: List[str] = None) -> Dict[str, Any]:
        """
        èˆ‡åŸºæº–æ–¹æ³•æ¯”è¼ƒæ€§èƒ½
        
        Args:
            input_data: è¼¸å…¥æ•¸æ“š
            text_column: æ–‡æœ¬æ¬„ä½
            baseline_methods: åŸºæº–æ–¹æ³•åˆ—è¡¨
            
        Returns:
            æ¯”è¼ƒçµæœ
        """
        if baseline_methods is None:
            baseline_methods = ['no', 'similarity', 'keyword', 'self_attention', 'combined']
        
        logger.info("é–‹å§‹èˆ‡åŸºæº–æ–¹æ³•æ¯”è¼ƒ...")
        
        # é‹è¡Œæ–°æ¶æ§‹
        logger.info("é‹è¡Œæ–°èåˆæ¶æ§‹...")
        fusion_results = self.run_complete_pipeline(input_data, text_column, save_results=False)
        
        # é‹è¡ŒåŸºæº–æ–¹æ³•
        baseline_results = {}
        
        # é€™è£¡å¯ä»¥é›†æˆç¾æœ‰çš„æ³¨æ„åŠ›åˆ†ææ–¹æ³•
        from .attention_processor import AttentionProcessor
        processor = AttentionProcessor(self.output_dir, encoder_type=self.encoder_type)
        
        # æº–å‚™è¼¸å…¥æ–‡ä»¶
        temp_file = os.path.join(self.output_dir, "temp_comparison_data.csv")
        processed_data = self.text_preprocessor.preprocess(input_data, text_column)
        processed_data.to_csv(temp_file, index=False)
        
        try:
            # é‹è¡Œå‚³çµ±æ³¨æ„åŠ›åˆ†æ
            attention_results = processor.process_with_attention(
                input_file=temp_file,
                attention_types=baseline_methods,
                save_results=False
            )
            
            # è©•ä¼°åŸºæº–æ–¹æ³•çš„åˆ†é¡æ€§èƒ½
            classifier_baseline = SentimentClassifier(self.output_dir, self.encoder_type)
            baseline_classification = classifier_baseline.evaluate_attention_mechanisms(
                attention_results, processed_data
            )
            
            baseline_results = baseline_classification
            
        finally:
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
        
        # æ¯”è¼ƒçµæœ
        comparison = {
            'fusion_architecture': {
                'accuracy': fusion_results['classification_results']['test_accuracy'],
                'f1_score': fusion_results['classification_results']['test_f1'],
                'precision': fusion_results['classification_results']['test_precision'],
                'recall': fusion_results['classification_results']['test_recall'],
                'training_time': fusion_results['classification_results']['training_time']
            },
            'baseline_methods': {},
            'performance_improvement': {}
        }
        
        # æå–åŸºæº–æ–¹æ³•çµæœ
        for method, results in baseline_results.items():
            if method != 'comparison' and isinstance(results, dict) and 'test_accuracy' in results:
                comparison['baseline_methods'][method] = {
                    'accuracy': results['test_accuracy'],
                    'f1_score': results['test_f1'],
                    'precision': results['test_precision'],
                    'recall': results['test_recall'],
                    'training_time': results.get('training_time', 0)
                }
        
        # è¨ˆç®—æ”¹å–„å¹…åº¦
        fusion_acc = comparison['fusion_architecture']['accuracy']
        for method, metrics in comparison['baseline_methods'].items():
            baseline_acc = metrics['accuracy']
            improvement = ((fusion_acc - baseline_acc) / baseline_acc) * 100 if baseline_acc > 0 else 0
            comparison['performance_improvement'][method] = {
                'accuracy_improvement_percent': improvement,
                'absolute_improvement': fusion_acc - baseline_acc
            }
        
        logger.info("æ€§èƒ½æ¯”è¼ƒå®Œæˆ")
        logger.info(f"èåˆæ¶æ§‹æº–ç¢ºç‡: {fusion_acc:.4f}")
        
        # æ‰¾å‡ºæœ€ä½³åŸºæº–æ–¹æ³•
        if comparison['baseline_methods']:
            best_baseline = max(comparison['baseline_methods'].items(), key=lambda x: x[1]['accuracy'])
            best_method, best_metrics = best_baseline
            logger.info(f"æœ€ä½³åŸºæº–æ–¹æ³•: {best_method} (æº–ç¢ºç‡: {best_metrics['accuracy']:.4f})")
            logger.info(f"ç›¸å°æ”¹å–„: {comparison['performance_improvement'][best_method]['accuracy_improvement_percent']:.2f}%")
        
        return {
            'comparison_results': comparison,
            'fusion_results': fusion_results,
            'baseline_results': baseline_results
        }


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'text': [
            'This product is absolutely amazing! I love it so much.',
            'The quality is terrible and I hate this purchase.',
            'It\'s okay, nothing special but not bad either.',
            'Outstanding quality and excellent customer service!',
            'Worst experience ever, very disappointed.',
            'Average product, meets basic expectations.'
        ],
        'review_stars': [5, 1, 3, 5, 1, 3]
    })
    
    # æ¸¬è©¦èåˆæµç¨‹
    pipeline = FusionPipeline(encoder_type='bert')
    
    try:
        results = pipeline.run_complete_pipeline(
            input_data=test_data,
            text_column='text',
            test_size=0.3,
            save_results=False
        )
        
        print("èåˆæµç¨‹æ¸¬è©¦å®Œæˆï¼")
        print(f"æº–ç¢ºç‡: {results['classification_results']['test_accuracy']:.4f}")
        print(f"é–€æ§æ¬Šé‡: {results['fusion_results']['gate_weights']}")
        
    except Exception as e:
        print(f"æ¸¬è©¦å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()