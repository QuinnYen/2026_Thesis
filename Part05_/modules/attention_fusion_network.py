"""
æ³¨æ„åŠ›èåˆç¶²è·¯ - å¯¦ç¾ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çš„é–€æ§èåˆ
åŒ…å«ç‰¹å¾µå°é½Šã€é–€æ§æ¬Šé‡è¨ˆç®—ã€æ¬Šé‡æ­¸ä¸€åŒ–å’ŒåŠ æ¬Šèåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
from typing import Dict, List, Tuple, Optional, Any
import math

logger = logging.getLogger(__name__)

class FeatureAligner(nn.Module):
    """ç‰¹å¾µå°é½Šæ¨¡çµ„ï¼Œç¢ºä¿ä¸åŒæ³¨æ„åŠ›æ©Ÿåˆ¶è¼¸å‡ºç¶­åº¦ä¸€è‡´"""
    
    def __init__(self, input_dims: List[int], target_dim: int = 768):
        """
        åˆå§‹åŒ–ç‰¹å¾µå°é½Šå™¨
        
        Args:
            input_dims: å„æ³¨æ„åŠ›æ©Ÿåˆ¶çš„è¼¸å…¥ç¶­åº¦åˆ—è¡¨
            target_dim: ç›®æ¨™å°é½Šç¶­åº¦
        """
        super(FeatureAligner, self).__init__()
        self.target_dim = target_dim
        self.aligners = nn.ModuleDict()
        
        # ç‚ºæ¯å€‹è¼¸å…¥ç¶­åº¦å‰µå»ºå°é½Šå±¤
        for i, dim in enumerate(input_dims):
            if dim != target_dim:
                # ä½¿ç”¨ç·šæ€§å±¤é€²è¡Œç¶­åº¦å°é½Š
                self.aligners[f'aligner_{i}'] = nn.Sequential(
                    nn.Linear(dim, target_dim),
                    nn.LayerNorm(target_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )
            else:
                # ç¶­åº¦å·²åŒ¹é…ï¼Œä½¿ç”¨æ†ç­‰æ˜ å°„
                self.aligners[f'aligner_{i}'] = nn.Identity()
    
    def forward(self, features_list: List[torch.Tensor]) -> List[torch.Tensor]:
        """
        å°é½Šç‰¹å¾µç¶­åº¦
        
        Args:
            features_list: å¾…å°é½Šçš„ç‰¹å¾µåˆ—è¡¨
            
        Returns:
            å°é½Šå¾Œçš„ç‰¹å¾µåˆ—è¡¨
        """
        aligned_features = []
        for i, features in enumerate(features_list):
            aligner = self.aligners[f'aligner_{i}']
            aligned = aligner(features)
            aligned_features.append(aligned)
        
        return aligned_features


class GatedFusionNetwork(nn.Module):
    """é–€æ§èåˆç¶²è·¯ - ä¸‰ç¨®é–€æ§æ©Ÿåˆ¶çš„å¯¦ç¾"""
    
    def __init__(self, feature_dim: int = 768, hidden_dim: int = 256):
        """
        åˆå§‹åŒ–é–€æ§èåˆç¶²è·¯
        
        Args:
            feature_dim: ç‰¹å¾µç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        super(GatedFusionNetwork, self).__init__()
        self.feature_dim = feature_dim
        self.hidden_dim = hidden_dim
        
        # ç›¸ä¼¼åº¦é–€æ§ï¼šè¨ˆç®—ç‰¹å¾µé–“çš„ç›¸ä¼¼æ€§
        self.similarity_gate = nn.Sequential(
            nn.Linear(feature_dim * 3, hidden_dim),  # ä¸‰å€‹æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç‰¹å¾µ
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3),  # è¼¸å‡ºä¸‰å€‹æ¬Šé‡
            nn.Softmax(dim=-1)
        )
        
        # é—œéµè©é–€æ§ï¼šåŸºæ–¼æ–‡æœ¬å…§å®¹çš„é—œéµè©å¯†åº¦
        self.keyword_gate = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 3),
            nn.Softmax(dim=-1)
        )
        
        # è‡ªæ³¨æ„åŠ›é–€æ§ï¼šè¨ˆç®—ç‰¹å¾µçš„è‡ªç›¸é—œæ€§
        self.self_attention_gate = nn.MultiheadAttention(
            embed_dim=feature_dim,
            num_heads=8,
            dropout=0.1
        )
        
        # é–€æ§æ¬Šé‡è½‰æ›
        self.gate_transform = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3),
            nn.Softmax(dim=-1)
        )
        
        # æœ€çµ‚èåˆå±¤
        self.fusion_layer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
    
    def forward(self, similarity_features: torch.Tensor,
                keyword_features: torch.Tensor,
                self_attention_features: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘å‚³æ’­
        
        Args:
            similarity_features: ç›¸ä¼¼åº¦æ³¨æ„åŠ›ç‰¹å¾µ [batch_size, feature_dim]
            keyword_features: é—œéµè©æ³¨æ„åŠ›ç‰¹å¾µ [batch_size, feature_dim]
            self_attention_features: è‡ªæ³¨æ„åŠ›ç‰¹å¾µ [batch_size, feature_dim]
            
        Returns:
            fused_features: èåˆå¾Œçš„ç‰¹å¾µ
            gate_weights: å„é–€æ§çš„æ¬Šé‡
        """
        batch_size = similarity_features.size(0)
        
        # 1. ç›¸ä¼¼åº¦é–€æ§è¨ˆç®—
        concat_features = torch.cat([similarity_features, keyword_features, self_attention_features], dim=-1)
        similarity_weights = self.similarity_gate(concat_features)  # [batch_size, 3]
        
        # 2. é—œéµè©é–€æ§è¨ˆç®—ï¼ˆä½¿ç”¨å¹³å‡ç‰¹å¾µï¼‰
        avg_features = (similarity_features + keyword_features + self_attention_features) / 3
        keyword_weights = self.keyword_gate(avg_features)  # [batch_size, 3]
        
        # 3. è‡ªæ³¨æ„åŠ›é–€æ§è¨ˆç®—
        # é‡å¡‘ç‚ºåºåˆ—æ ¼å¼ï¼š[seq_len=3, batch_size, feature_dim]
        stacked_features = torch.stack([similarity_features, keyword_features, self_attention_features], dim=0)
        attended_features, attention_weights = self.self_attention_gate(
            stacked_features, stacked_features, stacked_features
        )
        # å¹³å‡æ³¨æ„åŠ›æ¬Šé‡ä½œç‚ºé–€æ§æ¬Šé‡
        self_attention_weights = torch.mean(attention_weights, dim=1)  # [batch_size, 3]
        
        # 4. æ¬Šé‡æ­¸ä¸€åŒ– - ç¢ºä¿ä¸‰å€‹é–€æ§æ¬Šé‡ç¸½å’Œç‚º1
        combined_weights = similarity_weights + keyword_weights + self_attention_weights
        normalized_weights = F.softmax(combined_weights, dim=-1)  # [batch_size, 3]
        
        # 5. åŠ æ¬Šèåˆ
        features_stack = torch.stack([similarity_features, keyword_features, self_attention_features], dim=1)  # [batch_size, 3, feature_dim]
        weighted_features = torch.sum(features_stack * normalized_weights.unsqueeze(-1), dim=1)  # [batch_size, feature_dim]
        
        # 6. æœ€çµ‚èåˆå±¤
        fused_features = self.fusion_layer(weighted_features)
        
        # æº–å‚™è¼¸å‡ºçš„æ¬Šé‡ä¿¡æ¯
        gate_weights = {
            'similarity_weights': similarity_weights,
            'keyword_weights': keyword_weights,
            'self_attention_weights': self_attention_weights,
            'normalized_weights': normalized_weights,
            'final_weights': {
                'similarity': normalized_weights[:, 0],
                'keyword': normalized_weights[:, 1],
                'self_attention': normalized_weights[:, 2]
            }
        }
        
        return fused_features, gate_weights


class AttentionFusionProcessor:
    """æ³¨æ„åŠ›èåˆè™•ç†å™¨ - æ•´åˆä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶ä¸¦é€²è¡Œèåˆ"""
    
    def __init__(self, feature_dim: int = 768, device: str = 'auto'):
        """
        åˆå§‹åŒ–èåˆè™•ç†å™¨
        
        Args:
            feature_dim: ç‰¹å¾µç¶­åº¦
            device: è¨ˆç®—è¨­å‚™
        """
        self.feature_dim = feature_dim
        
        # è¨­å‚™é¸æ“‡
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        logger.info(f"æ³¨æ„åŠ›èåˆè™•ç†å™¨ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # åˆå§‹åŒ–ç‰¹å¾µå°é½Šå™¨å’Œèåˆç¶²è·¯
        self.feature_aligner = None
        self.fusion_network = None
        
        # GPUè¨˜æ†¶é«”å„ªåŒ–
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
    
    def _initialize_networks(self, input_dims: List[int]):
        """åˆå§‹åŒ–ç¶²è·¯çµæ§‹"""
        if self.feature_aligner is None:
            self.feature_aligner = FeatureAligner(input_dims, self.feature_dim).to(self.device)
            self.fusion_network = GatedFusionNetwork(self.feature_dim).to(self.device)
            
            # è¨­ç‚ºè©•ä¼°æ¨¡å¼ä»¥ç²å¾—ä¸€è‡´çµæœ
            self.feature_aligner.eval()
            self.fusion_network.eval()
            
            logger.info(f"ç¶²è·¯çµæ§‹å·²åˆå§‹åŒ–ä¸¦ç§»è‡³ {self.device}")
    
    def fuse_attention_features(self, 
                              similarity_vectors: np.ndarray,
                              keyword_vectors: np.ndarray,
                              self_attention_vectors: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        èåˆä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç‰¹å¾µå‘é‡
        
        Args:
            similarity_vectors: ç›¸ä¼¼åº¦æ³¨æ„åŠ›å‘é‡ [batch_size, dim1]
            keyword_vectors: é—œéµè©æ³¨æ„åŠ›å‘é‡ [batch_size, dim2]  
            self_attention_vectors: è‡ªæ³¨æ„åŠ›å‘é‡ [batch_size, dim3]
            
        Returns:
            fused_features: èåˆå¾Œçš„ç‰¹å¾µ [batch_size, feature_dim]
            fusion_info: èåˆéç¨‹çš„è©³ç´°ä¿¡æ¯
        """
        # æª¢æŸ¥è¼¸å…¥ç¶­åº¦
        input_dims = [
            similarity_vectors.shape[-1],
            keyword_vectors.shape[-1], 
            self_attention_vectors.shape[-1]
        ]
        
        # åˆå§‹åŒ–ç¶²è·¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._initialize_networks(input_dims)
        
        try:
            with torch.no_grad():
                # è½‰æ›ç‚ºå¼µé‡
                sim_tensor = torch.tensor(similarity_vectors, dtype=torch.float32).to(self.device)
                key_tensor = torch.tensor(keyword_vectors, dtype=torch.float32).to(self.device)
                self_tensor = torch.tensor(self_attention_vectors, dtype=torch.float32).to(self.device)
                
                # ç‰¹å¾µå°é½Š
                aligned_features = self.feature_aligner([sim_tensor, key_tensor, self_tensor])
                
                # é–€æ§èåˆ
                fused_features, gate_weights = self.fusion_network(
                    aligned_features[0], aligned_features[1], aligned_features[2]
                )
                
                # è½‰æ›å›numpyæ ¼å¼
                fused_np = fused_features.cpu().numpy()
                
                # æº–å‚™èåˆä¿¡æ¯
                fusion_info = {
                    'input_dims': input_dims,
                    'output_dim': fused_np.shape[-1],
                    'gate_weights': {
                        'similarity': gate_weights['final_weights']['similarity'].cpu().numpy(),
                        'keyword': gate_weights['final_weights']['keyword'].cpu().numpy(), 
                        'self_attention': gate_weights['final_weights']['self_attention'].cpu().numpy()
                    },
                    'average_weights': {
                        'similarity': float(torch.mean(gate_weights['final_weights']['similarity']).cpu()),
                        'keyword': float(torch.mean(gate_weights['final_weights']['keyword']).cpu()),
                        'self_attention': float(torch.mean(gate_weights['final_weights']['self_attention']).cpu())
                    }
                }
                
                logger.debug(f"èåˆå®Œæˆ - å½¢ç‹€:{fused_np.shape} æ¬Šé‡:(ç›¸ä¼¼åº¦:{fusion_info['average_weights']['similarity']:.3f} é—œéµè©:{fusion_info['average_weights']['keyword']:.3f} è‡ªæ³¨æ„åŠ›:{fusion_info['average_weights']['self_attention']:.3f})")
                
                return fused_np, fusion_info
                
        except Exception as e:
            logger.error(f"æ³¨æ„åŠ›èåˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # GPUè¨˜æ†¶é«”æ¸…ç†
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            raise
        
        finally:
            # æ¸…ç†GPUè¨˜æ†¶é«”
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
    
    def compute_parallel_attention_features(self, 
                                          embeddings: np.ndarray,
                                          metadata,
                                          **kwargs) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:
        """
        ä¸¦è¡Œè¨ˆç®—ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç‰¹å¾µ
        
        Args:
            embeddings: æ–‡æœ¬åµŒå…¥å‘é‡
            metadata: å…ƒæ•¸æ“š
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            similarity_features: ç›¸ä¼¼åº¦æ³¨æ„åŠ›ç‰¹å¾µ
            keyword_features: é—œéµè©æ³¨æ„åŠ›ç‰¹å¾µ
            self_attention_features: è‡ªæ³¨æ„åŠ›ç‰¹å¾µ
            attention_info: æ³¨æ„åŠ›è¨ˆç®—ä¿¡æ¯
        """
        from .attention_mechanism import SimilarityAttention, KeywordGuidedAttention, SelfAttention
        
        logger.info("ğŸ”„ é–‹å§‹ä¸¦è¡Œè¨ˆç®—ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µ...")
        
        # åˆå§‹åŒ–ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶
        similarity_attn = SimilarityAttention()
        keyword_attn = KeywordGuidedAttention()
        self_attn = SelfAttention()
        
        attention_results = {}
        
        # ä¸¦è¡Œè¨ˆç®—ï¼ˆåœ¨å¯¦éš›å¯¦ç¾ä¸­å¯ä»¥ä½¿ç”¨å¤šç·šç¨‹ï¼‰
        mechanisms = [
            ('similarity', similarity_attn),
            ('keyword', keyword_attn), 
            ('self_attention', self_attn)
        ]
        
        features_list = []
        
        for name, mechanism in mechanisms:
            logger.debug(f"è¨ˆç®— {name} æ³¨æ„åŠ›æ©Ÿåˆ¶")
            
            try:
                # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡å’Œé¢å‘å‘é‡
                aspect_vectors, attention_data = mechanism.compute_aspect_vectors(
                    embeddings, metadata, **kwargs
                )
                
                # å°‡aspect vectorsè½‰æ›ç‚ºdocument-level features
                if aspect_vectors:
                    # ç‚ºæ¯å€‹æ–‡æª”è¨ˆç®—èˆ‡å„aspectçš„ç›¸ä¼¼åº¦ç‰¹å¾µ
                    num_docs = len(embeddings)
                    doc_features = []
                    
                    # ç²å–æ‰€æœ‰aspectåç¨±ä¸¦æ’åºä»¥ç¢ºä¿ä¸€è‡´æ€§
                    sorted_aspects = sorted(aspect_vectors.keys())
                    
                    for doc_idx in range(num_docs):
                        doc_embedding = embeddings[doc_idx]
                        
                        # è¨ˆç®—æ–‡æª”èˆ‡æ¯å€‹aspectçš„ç›¸ä¼¼åº¦åˆ†æ•¸
                        similarity_scores = []
                        for aspect_name in sorted_aspects:
                            aspect_vector = aspect_vectors[aspect_name]
                            
                            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                            similarity = np.dot(doc_embedding, aspect_vector) / (
                                np.linalg.norm(doc_embedding) * np.linalg.norm(aspect_vector) + 1e-8
                            )
                            similarity_scores.append(similarity)
                        
                        # ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ•¸ä½œç‚ºdocument-levelç‰¹å¾µ
                        # åŒæ™‚åŠ å…¥åŸå§‹embeddingçš„çµ±è¨ˆç‰¹å¾µ
                        doc_stats = [
                            np.mean(doc_embedding),    # å¹³å‡å€¼
                            np.std(doc_embedding),     # æ¨™æº–å·®  
                            np.max(doc_embedding),     # æœ€å¤§å€¼
                            np.min(doc_embedding)      # æœ€å°å€¼
                        ]
                        
                        # çµ„åˆç‰¹å¾µï¼šç›¸ä¼¼åº¦åˆ†æ•¸ + æ–‡æª”çµ±è¨ˆç‰¹å¾µ
                        combined_feature = np.array(similarity_scores + doc_stats)
                        doc_features.append(combined_feature)
                    
                    features = np.array(doc_features)  # [num_docs, num_aspects + 4]
                    features_list.append(features)
                    
                    attention_results[name] = {
                        'aspect_vectors': aspect_vectors,
                        'attention_data': attention_data,
                        'features_shape': features.shape
                    }
                    
                    logger.debug(f"{name} æ³¨æ„åŠ›ç‰¹å¾µå½¢ç‹€: {features.shape}")
                else:
                    # å¦‚æœæ²’æœ‰aspect vectorsï¼Œå‰µå»ºé è¨­ç‰¹å¾µ
                    num_docs = len(embeddings)
                    # ä½¿ç”¨æ–‡æª”embeddingçš„çµ±è¨ˆç‰¹å¾µä½œç‚ºé è¨­ç‰¹å¾µ
                    doc_features = []
                    for doc_idx in range(num_docs):
                        doc_embedding = embeddings[doc_idx]
                        # è¨ˆç®—æ–‡æª”çµ±è¨ˆç‰¹å¾µ
                        doc_stats = [
                            np.mean(doc_embedding),    # å¹³å‡å€¼
                            np.std(doc_embedding),     # æ¨™æº–å·®  
                            np.max(doc_embedding),     # æœ€å¤§å€¼
                            np.min(doc_embedding),     # æœ€å°å€¼
                            0.0, 0.0, 0.0              # 3å€‹é è¨­ç›¸ä¼¼åº¦åˆ†æ•¸
                        ]
                        doc_features.append(np.array(doc_stats))
                    
                    features = np.array(doc_features)  # [num_docs, 7]
                    features_list.append(features)
                    logger.warning(f"{name} æ³¨æ„åŠ›æ²’æœ‰ç”¢ç”Ÿæœ‰æ•ˆaspect vectorsï¼Œä½¿ç”¨æ–‡æª”çµ±è¨ˆç‰¹å¾µ")
                    
            except Exception as e:
                logger.error(f"è¨ˆç®— {name} æ³¨æ„åŠ›æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                # å›é€€åˆ°çµ±è¨ˆç‰¹å¾µ
                num_docs = len(embeddings)
                doc_features = []
                for doc_idx in range(num_docs):
                    doc_embedding = embeddings[doc_idx]
                    # ä½¿ç”¨æ–‡æª”çµ±è¨ˆç‰¹å¾µä½œç‚ºå›é€€æ–¹æ¡ˆ
                    doc_stats = [
                        np.mean(doc_embedding),    # å¹³å‡å€¼
                        np.std(doc_embedding),     # æ¨™æº–å·®  
                        np.max(doc_embedding),     # æœ€å¤§å€¼
                        np.min(doc_embedding),     # æœ€å°å€¼
                        0.0, 0.0, 0.0              # 3å€‹é è¨­ç›¸ä¼¼åº¦åˆ†æ•¸
                    ]
                    doc_features.append(np.array(doc_stats))
                
                fallback_features = np.array(doc_features)  # [num_docs, 7]
                features_list.append(fallback_features)
        
        # æª¢æŸ¥ç‰¹å¾µç¶­åº¦ä¸¦é€²è¡Œå°é½Š
        if len(features_list) >= 3:
            logger.debug("æª¢æŸ¥ç‰¹å¾µç¶­åº¦ä¸€è‡´æ€§...")
            dims = [f.shape for f in features_list]
            logger.debug(f"å„æ©Ÿåˆ¶ç‰¹å¾µå½¢ç‹€: {dims}")
            
            # ç¢ºä¿æ‰€æœ‰ç‰¹å¾µéƒ½æœ‰ç›¸åŒçš„æ¨£æœ¬æ•¸
            min_samples = min(f.shape[0] for f in features_list)
            features_list = [f[:min_samples] for f in features_list]
            
            # æª¢æŸ¥ç‰¹å¾µç¶­åº¦ä¸¦é€²è¡Œå°é½Š  
            feature_dims = [f.shape[1] for f in features_list]
            if len(set(feature_dims)) > 1:
                logger.info(f"æª¢æ¸¬åˆ°ä¸åŒç‰¹å¾µç¶­åº¦: {feature_dims}ï¼Œé€²è¡Œç‰¹å¾µå°é½Š...")
                # çµ±ä¸€åˆ°æœ€å¤§ç¶­åº¦
                max_dim = max(feature_dims) 
                aligned_features = []
                for i, features in enumerate(features_list):
                    if features.shape[1] < max_dim:
                        # é›¶å¡«å……åˆ°æœ€å¤§ç¶­åº¦
                        padding = np.zeros((features.shape[0], max_dim - features.shape[1]))
                        aligned_features.append(np.concatenate([features, padding], axis=1))
                    else:
                        aligned_features.append(features)
                features_list = aligned_features
                logger.info(f"ç‰¹å¾µå°é½Šå®Œæˆ - æ‰€æœ‰æ©Ÿåˆ¶ç‰¹å¾µç¶­åº¦çµ±ä¸€ç‚º: {max_dim}")
            else:
                logger.info(f"æ‰€æœ‰æ³¨æ„åŠ›æ©Ÿåˆ¶çš„ç‰¹å¾µç¶­åº¦å·²ä¸€è‡´: {feature_dims[0]}")
        
        attention_info = {
            'mechanisms_computed': len(features_list),
            'feature_shapes': [f.shape for f in features_list],
            'attention_results': attention_results
        }
        
        logger.info("âœ… ä¸‰ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶ç‰¹å¾µè¨ˆç®—å®Œæˆï¼Œç‰¹å¾µå·²å°é½Šç‚ºdocument-levelæ ¼å¼")
        
        return features_list[0], features_list[1], features_list[2], attention_info


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(level=logging.INFO)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    batch_size = 10
    feature_dim = 768
    
    # æ¨¡æ“¬ä¸‰ç¨®æ³¨æ„åŠ›ç‰¹å¾µï¼ˆä¸åŒç¶­åº¦ä»¥æ¸¬è©¦å°é½Šï¼‰
    similarity_features = np.random.randn(batch_size, feature_dim)
    keyword_features = np.random.randn(batch_size, 512)  # ä¸åŒç¶­åº¦
    self_attention_features = np.random.randn(batch_size, 256)  # ä¸åŒç¶­åº¦
    
    # æ¸¬è©¦èåˆè™•ç†å™¨
    fusion_processor = AttentionFusionProcessor(feature_dim=feature_dim)
    
    # åŸ·è¡Œèåˆ
    fused_features, fusion_info = fusion_processor.fuse_attention_features(
        similarity_features, keyword_features, self_attention_features
    )
    
    print(f"èåˆçµæœå½¢ç‹€: {fused_features.shape}")
    print(f"å¹³å‡æ¬Šé‡: {fusion_info['average_weights']}")
    print("æ³¨æ„åŠ›èåˆç¶²è·¯æ¸¬è©¦å®Œæˆï¼")