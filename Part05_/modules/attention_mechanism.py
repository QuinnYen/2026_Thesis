"""
æ³¨æ„åŠ›æ©Ÿåˆ¶æ¨¡çµ„ - æä¾›å„ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ç¾
åŒ…å«ç›¸ä¼¼åº¦æ³¨æ„åŠ›ã€é—œéµè©æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›å’Œçµ„åˆæ³¨æ„åŠ›
"""

import os
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import json
import logging
import time
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Dict, List, Tuple, Optional, Any

# å›ºå®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# å›ºå®šPyTorchéš¨æ©Ÿç¨®å­
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class AttentionMechanism:
    """æ³¨æ„åŠ›æ©Ÿåˆ¶åŸºé¡"""
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶
        
        Args:
            config: é…ç½®åƒæ•¸å­—å…¸
        """
        self.config = config or {}
        self.logger = logger
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        
        Args:
            embeddings: æ–‡æª”åµŒå…¥å‘é‡ï¼Œå½¢ç‹€ç‚º [n_docs, embed_dim]
            metadata: æ–‡æª”å…ƒæ•¸æ“šï¼ŒåŒ…å«é¢å‘æ¨™ç±¤
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            weights: æ³¨æ„åŠ›æ¬Šé‡ï¼Œå½¢ç‹€ç‚º [n_topics, n_docs]
            topic_indices: æ¯å€‹ä¸»é¡ŒåŒ…å«çš„æ–‡æª”ç´¢å¼•å­—å…¸
        """
        raise NotImplementedError("å­é¡å¿…é ˆå¯¦ç¾æ­¤æ–¹æ³•")
    
    def compute_aspect_vectors(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[Dict, Dict]:
        """è¨ˆç®—é¢å‘å‘é‡
        
        Args:
            embeddings: æ–‡æª”åµŒå…¥å‘é‡
            metadata: æ–‡æª”å…ƒæ•¸æ“š
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            aspect_vectors: é¢å‘å‘é‡å­—å…¸
            attention_data: æ³¨æ„åŠ›æ¬Šé‡å’Œç›¸é—œæ•¸æ“š
        """
        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        weights, topic_indices = self.compute_attention(embeddings, metadata, **kwargs)
        
        # è¨ˆç®—é¢å‘å‘é‡
        # éæ¿¾æ‰éä¸»é¡Œçš„ç‰¹æ®Šéµ
        excluded_keys = {'dynamic_weights', 'is_dynamic', 'topic_indices'}
        if isinstance(topic_indices, dict) and 'topic_indices' in topic_indices:
            # å¦‚æœæ˜¯åŒ…è£æ ¼å¼ï¼Œæå–å¯¦éš›çš„ä¸»é¡Œç´¢å¼•
            actual_topic_indices = topic_indices['topic_indices']
            topics = list(actual_topic_indices.keys())
            topic_indices_data = actual_topic_indices
        else:
            # ç›´æ¥æ ¼å¼ï¼Œéæ¿¾ç‰¹æ®Šéµ
            topics = [k for k in topic_indices.keys() if k not in excluded_keys]
            topic_indices_data = topic_indices
            
        aspect_vectors = {}
        
        for topic_idx, topic in enumerate(topics):
            # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
            doc_indices = topic_indices_data[topic]
            
            if not doc_indices:
                self.logger.warning(f"ä¸»é¡Œ {topic} æ²’æœ‰æ–‡æª”ï¼Œå°‡ä½¿ç”¨é›¶å‘é‡")
                aspect_vectors[topic] = np.zeros(embeddings.shape[1])
                continue
            
            # ç¢ºä¿ç´¢å¼•æ˜¯æ•´æ•¸åˆ—è¡¨
            if isinstance(doc_indices, np.ndarray):
                doc_indices = doc_indices.astype(int).tolist()
            elif not isinstance(doc_indices, list):
                doc_indices = list(doc_indices)
                
            # é©—è­‰ç´¢å¼•æ˜¯å¦ç‚ºæ•´æ•¸é¡å‹
            doc_indices = [int(idx) for idx in doc_indices if isinstance(idx, (int, np.integer))]
            
            if not doc_indices:
                self.logger.warning(f"ä¸»é¡Œ {topic} æ²’æœ‰æœ‰æ•ˆçš„æ–‡æª”ç´¢å¼•ï¼Œå°‡ä½¿ç”¨é›¶å‘é‡")
                aspect_vectors[topic] = np.zeros(embeddings.shape[1])
                continue
            
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[doc_indices]
            
            # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”æ¬Šé‡
            topic_weights = weights[topic_idx][doc_indices]
            
            # æ­£è¦åŒ–æ¬Šé‡ç¢ºä¿ç¸½å’Œç‚º1
            if np.sum(topic_weights) > 0:
                topic_weights = topic_weights / np.sum(topic_weights)
            else:
                topic_weights = np.ones(len(doc_indices)) / len(doc_indices)
            
            # è¨ˆç®—åŠ æ¬Šå¹³å‡
            weighted_vector = np.zeros(embeddings.shape[1])
            for i, embed in enumerate(topic_embeddings):
                weighted_vector += topic_weights[i] * embed
                
            aspect_vectors[topic] = weighted_vector
        
        # è¿”å›é¢å‘å‘é‡å’Œæ³¨æ„åŠ›æ•¸æ“š
        attention_data = {
            "weights": weights,
            "topic_indices": topic_indices,
            "topics": topics
        }
        
        return aspect_vectors, attention_data
    
    def evaluate(self, aspect_vectors: Dict, embeddings: np.ndarray, metadata: pd.DataFrame) -> Dict:
        """è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ•ˆèƒ½
        
        Args:
            aspect_vectors: é¢å‘å‘é‡å­—å…¸
            embeddings: æ–‡æª”åµŒå…¥å‘é‡
            metadata: æ–‡æª”å…ƒæ•¸æ“š
            
        Returns:
            metrics: è©•ä¼°æŒ‡æ¨™å­—å…¸
        """
        topics = list(aspect_vectors.keys())
        self.logger.debug(f"é–‹å§‹è©•ä¼°ï¼Œå…±æœ‰ {len(topics)} å€‹é¢å‘")
        
        # è¨ˆç®—é¢å‘å…§èšåº¦
        coherence = 0.0
        doc_count = 0
        topic_coherence_dict = {}
        
        self.logger.debug("é–‹å§‹è¨ˆç®—é¢å‘å…§èšåº¦...")
        
        for topic in topics:
            # ç²å–è©²é¢å‘çš„æ–‡æª”
            topic_docs = metadata[metadata['sentiment'] == topic] if 'sentiment' in metadata.columns else metadata[metadata.index.isin([topic])]
            
            if len(topic_docs) == 0:
                self.logger.debug(f"é¢å‘ '{topic}' æ²’æœ‰å°æ‡‰çš„æ–‡æª”")
                topic_coherence_dict[topic] = 0.0
                continue
            
            # ç²å–è©²é¢å‘çš„åµŒå…¥å‘é‡ç´¢å¼•
            doc_indices = topic_docs.index.tolist()
            
            # ç¢ºä¿ç´¢å¼•æ˜¯æ•´æ•¸åˆ—è¡¨ä¸¦ä¸”æœ‰æ•ˆ
            if isinstance(doc_indices, np.ndarray):
                doc_indices = doc_indices.astype(int).tolist()
            elif not isinstance(doc_indices, list):
                doc_indices = list(doc_indices)
                
            # é©—è­‰ç´¢å¼•æ˜¯å¦ç‚ºæ•´æ•¸é¡å‹ä¸¦ä¸”åœ¨æœ‰æ•ˆç¯„åœå…§
            valid_doc_indices = []
            for idx in doc_indices:
                try:
                    idx_int = int(idx)
                    if 0 <= idx_int < len(embeddings):
                        valid_doc_indices.append(idx_int)
                    else:
                        self.logger.warning(f"ç´¢å¼• {idx_int} è¶…å‡ºç¯„åœ [0, {len(embeddings)-1}]")
                except (ValueError, TypeError):
                    self.logger.warning(f"ç„¡æ•ˆçš„ç´¢å¼•é¡å‹: {type(idx)} - {idx}")
                    
            if not valid_doc_indices:
                self.logger.debug(f"é¢å‘ '{topic}' æ²’æœ‰æœ‰æ•ˆçš„æ–‡æª”ç´¢å¼•")
                topic_coherence_dict[topic] = 0.0
                continue
                
            topic_embeddings = embeddings[valid_doc_indices]
            aspect_vector = aspect_vectors[topic]
            
            # è¨ˆç®—è©²é¢å‘çš„å…§èšåº¦
            topic_coherence = 0.0
            for embed in topic_embeddings:
                similarity = np.dot(aspect_vector, embed) / (np.linalg.norm(aspect_vector) * np.linalg.norm(embed))
                topic_coherence += similarity
            
            if len(topic_embeddings) > 0:
                topic_coherence /= len(topic_embeddings)
                topic_coherence_dict[topic] = float(topic_coherence)
                coherence += topic_coherence * len(topic_embeddings)
                doc_count += len(topic_embeddings)
                
                self.logger.debug(f"é¢å‘ '{topic}' çš„å…§èšåº¦: {topic_coherence:.4f} (æ–‡æª”æ•¸: {len(topic_embeddings)})")
            else:
                topic_coherence_dict[topic] = 0.0
        
        if doc_count > 0:
            coherence /= doc_count
            
        # ç§»é™¤è©³ç´°è¼¸å‡º
        
        # è¨ˆç®—é¢å‘åˆ†é›¢åº¦
        separation = 0.0
        pair_count = 0
        topic_separation_dict = {}
        aspect_separation_values = {topic: [] for topic in topics}
        
        self.logger.debug("é–‹å§‹è¨ˆç®—é¢å‘é–“çš„åˆ†é›¢åº¦...")
        
        for i in range(len(topics)):
            for j in range(i+1, len(topics)):
                topic_i = topics[i]
                topic_j = topics[j]
                vec_i = aspect_vectors[topic_i]
                vec_j = aspect_vectors[topic_j]
                
                # è¨ˆç®—é¤˜å¼¦è·é›¢ (1 - ç›¸ä¼¼åº¦)
                similarity = np.dot(vec_i, vec_j) / (np.linalg.norm(vec_i) * np.linalg.norm(vec_j))
                distance = 1.0 - similarity
                
                separation += distance
                pair_count += 1
                
                # å­˜å„²è©²å°é¢å‘çš„åˆ†é›¢åº¦
                pair_key = f"{topic_i}_{topic_j}"
                topic_separation_dict[pair_key] = float(distance)
                
                # ç‚ºå…©å€‹é¢å‘éƒ½æ·»åŠ é€™å€‹åˆ†é›¢åº¦å€¼
                aspect_separation_values[topic_i].append(distance)
                aspect_separation_values[topic_j].append(distance)
                
                self.logger.debug(f"é¢å‘ '{topic_i}' å’Œ '{topic_j}' çš„åˆ†é›¢åº¦: {distance:.4f}")
        
        if pair_count > 0:
            separation /= pair_count
            
        # ç§»é™¤è©³ç´°è¼¸å‡º
            
        # è¨ˆç®—æ¯å€‹é¢å‘çš„å¹³å‡åˆ†é›¢åº¦
        aspect_avg_separation = {}
        for topic in topics:
            if aspect_separation_values[topic]:
                aspect_avg_separation[topic] = float(np.mean(aspect_separation_values[topic]))
                self.logger.debug(f"é¢å‘ '{topic}' çš„å¹³å‡åˆ†é›¢åº¦: {aspect_avg_separation[topic]:.4f}")
            else:
                aspect_avg_separation[topic] = 0.0
            
        # è¨ˆç®—ç¶œåˆå¾—åˆ†
        coherence_weight = self.config.get('coherence_weight', 0.5)
        separation_weight = self.config.get('separation_weight', 0.5)
        combined_score = coherence_weight * coherence + separation_weight * separation
        
        # ç§»é™¤è©³ç´°è¼¸å‡º
            
        # è¿”å›è©•ä¼°æŒ‡æ¨™
        metrics = {
            # ç¸½é«”æŒ‡æ¨™
            "coherence": float(coherence),
            "separation": float(separation),
            "combined_score": float(combined_score),
            
            # é¢å‘ç‰¹å®šæŒ‡æ¨™
            "topic_coherence": topic_coherence_dict,
            "topic_separation": topic_separation_dict,
            "aspect_separation": aspect_avg_separation,
            
            # è©³ç´°æŒ‡æ¨™
            "metrics_details": {
                "weights": {
                    "coherence_weight": coherence_weight,
                    "separation_weight": separation_weight
                },
                "per_aspect": {
                    "coherence": topic_coherence_dict,
                    "separation": aspect_avg_separation,
                    "pairwise_separation": topic_separation_dict
                }
            }
        }
        
        return metrics


class NoAttention(AttentionMechanism):
    """ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆå¹³å‡ï¼‰"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—å‡ç­‰æ¬Šé‡ï¼ˆç›¸ç•¶æ–¼å¹³å‡ï¼‰"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ•¸å€¼åŒ–æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment_numeric' in metadata.columns:
            topics = sorted(metadata['sentiment_numeric'].unique())
            topic_column = 'sentiment_numeric'
        elif 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å‰µå»ºå‡è¨­çš„æ•¸å€¼æƒ…æ„Ÿæ¨™ç±¤ï¼š0:è² é¢ã€1:ä¸­æ€§ã€2:æ­£é¢
            topics = [0, 1, 2]
            topic_column = 'sentiment_numeric'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£ï¼ˆæ‰€æœ‰æ¬Šé‡ç›¸ç­‰ï¼‰
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
                raw_indices = metadata.index[metadata[topic_column] == topic]
                doc_indices = [int(idx) for idx in raw_indices.tolist()]
            else:
                # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œå¹³å‡åˆ†é…
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            # é©—è­‰ç´¢å¼•æœ‰æ•ˆæ€§ä¸¦éæ¿¾ç„¡æ•ˆç´¢å¼•
            valid_doc_indices = []
            for idx in doc_indices:
                if 0 <= idx < len(embeddings):
                    valid_doc_indices.append(idx)
                else:
                    self.logger.warning(f"æ–‡æª”ç´¢å¼• {idx} è¶…å‡ºåµŒå…¥å‘é‡ç¯„åœ [0, {len(embeddings)-1}]")
            
            topic_indices[topic] = valid_doc_indices
            
            # è¨­ç½®å‡ç­‰æ¬Šé‡
            if valid_doc_indices:
                uniform_weight = 1.0 / len(valid_doc_indices)
                for doc_idx in valid_doc_indices:
                    weights[topic_idx][doc_idx] = uniform_weight
        
        return weights, topic_indices


class SimilarityAttention(AttentionMechanism):
    """åŸºæ–¼ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—åŸºæ–¼ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ•¸å€¼åŒ–æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment_numeric' in metadata.columns:
            topics = sorted(metadata['sentiment_numeric'].unique())
            topic_column = 'sentiment_numeric'
        elif 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å‰µå»ºå‡è¨­çš„æ•¸å€¼æƒ…æ„Ÿæ¨™ç±¤ï¼š0:è² é¢ã€1:ä¸­æ€§ã€2:æ­£é¢
            topics = [0, 1, 2]
            topic_column = 'sentiment_numeric'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
                raw_indices = metadata.index[metadata[topic_column] == topic]
                doc_indices = [int(idx) for idx in raw_indices.tolist()]
            else:
                # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œå¹³å‡åˆ†é…
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            if not doc_indices:
                continue
            
            # é©—è­‰ç´¢å¼•æœ‰æ•ˆæ€§ä¸¦éæ¿¾ç„¡æ•ˆç´¢å¼•
            valid_doc_indices = []
            for idx in doc_indices:
                if 0 <= idx < len(embeddings):
                    valid_doc_indices.append(idx)
                else:
                    self.logger.warning(f"æ–‡æª”ç´¢å¼• {idx} è¶…å‡ºåµŒå…¥å‘é‡ç¯„åœ [0, {len(embeddings)-1}]")
            
            # æ›´æ–°æœ‰æ•ˆçš„æ–‡æª”ç´¢å¼•
            topic_indices[topic] = valid_doc_indices
            
            if not valid_doc_indices:
                continue
                
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[valid_doc_indices]
            
            # è¨ˆç®—ä¸­å¿ƒå‘é‡
            center = np.mean(topic_embeddings, axis=0)
            
            # è¨ˆç®—æ¯å€‹æ–‡æª”èˆ‡ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
            similarities = []
            for embed in topic_embeddings:
                # æ·»åŠ æ•¸å€¼ç©©å®šæ€§æª¢æŸ¥
                center_norm = np.linalg.norm(center)
                embed_norm = np.linalg.norm(embed)
                
                if center_norm > 1e-8 and embed_norm > 1e-8:
                    similarity = np.dot(center, embed) / (center_norm * embed_norm)
                else:
                    similarity = 0.0
                similarities.append(similarity)
            
            # è½‰æ›ç‚º softmax æ¬Šé‡
            similarities = np.array(similarities)
            
            # å¦‚æœæ‰€æœ‰ç›¸ä¼¼åº¦éƒ½ç›¸åŒï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
            if len(similarities) == 0 or np.all(similarities == similarities[0]):
                topic_weights = np.ones(len(valid_doc_indices)) / len(valid_doc_indices)
            else:
                # é¿å…æ•¸å€¼ä¸ç©©å®š
                similarities = similarities - np.max(similarities)
                exp_sim = np.exp(similarities)
                exp_sum = np.sum(exp_sim)
                if exp_sum > 1e-8:
                    topic_weights = exp_sim / exp_sum
                else:
                    topic_weights = np.ones(len(valid_doc_indices)) / len(valid_doc_indices)
            
            # é€å€‹å¡«å……æ¬Šé‡çŸ©é™£
            for i, doc_idx in enumerate(valid_doc_indices):
                weights[topic_idx][doc_idx] = topic_weights[i]
        
        return weights, topic_indices


class KeywordGuidedAttention(AttentionMechanism):
    """åŸºæ–¼é—œéµè©çš„æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—åŸºæ–¼é—œéµè©çš„æ³¨æ„åŠ›æ¬Šé‡"""
        # æª¢æŸ¥æ˜¯å¦æä¾›äº†é—œéµè©
        topic_keywords = kwargs.get('topic_keywords')
        if topic_keywords is None:
            # å˜—è©¦å¾topics_pathåŠ è¼‰
            topics_path = kwargs.get('topics_path')
            if topics_path and os.path.exists(topics_path):
                try:
                    with open(topics_path, 'r', encoding='utf-8') as f:
                        topic_keywords = json.load(f)
                except Exception as e:
                    self.logger.warning(f"ç„¡æ³•å¾ {topics_path} åŠ è¼‰ä¸»é¡Œé—œéµè©: {str(e)}")
        
        if topic_keywords is None:
            # ä½¿ç”¨é è¨­é—œéµè©ï¼Œæ”¯æ´æ•¸å€¼åŒ–æ¨™ç±¤
            topic_keywords = {
                0: ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointing', 'poor', 'å£', 'ç³Ÿç³•', 'å¯æ€•'],  # è² é¢
                1: ['okay', 'average', 'normal', 'fine', 'neutral', 'acceptable', 'é‚„å¯ä»¥', 'æ™®é€š', 'ä¸€èˆ¬'],  # ä¸­æ€§
                2: ['good', 'great', 'excellent', 'amazing', 'wonderful', 'love', 'best', 'perfect', 'å¥½', 'æ£’', 'å„ªç§€'],  # æ­£é¢
                # åŒæ™‚æ”¯æ´æ–‡å­—æ¨™ç±¤ä»¥å‘å¾Œå…¼å®¹
                'negative': ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointing', 'poor', 'å£', 'ç³Ÿç³•', 'å¯æ€•'],
                'neutral': ['okay', 'average', 'normal', 'fine', 'neutral', 'acceptable', 'é‚„å¯ä»¥', 'æ™®é€š', 'ä¸€èˆ¬'],
                'positive': ['good', 'great', 'excellent', 'amazing', 'wonderful', 'love', 'best', 'perfect', 'å¥½', 'æ£’', 'å„ªç§€']
            }
            # ç§»é™¤è©³ç´°è¼¸å‡º
        
        # ç²å–æ‰€æœ‰ä¸»é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ•¸å€¼åŒ–æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment_numeric' in metadata.columns:
            topics = sorted(metadata['sentiment_numeric'].unique())
            topic_column = 'sentiment_numeric'
        elif 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å¦‚æœä½¿ç”¨é—œéµè©ä½†æ²’æœ‰æ¨™ç±¤ï¼Œå„ªå…ˆä½¿ç”¨æ•¸å€¼åŒ–æ¨™ç±¤
            if topic_keywords and any(isinstance(k, (int, str)) and str(k).isdigit() for k in topic_keywords.keys()):
                topics = [0, 1, 2]  # æ•¸å€¼åŒ–æƒ…æ„Ÿæ¨™ç±¤
                topic_column = 'sentiment_numeric'
            else:
                topics = list(topic_keywords.keys())
                topic_column = 'sentiment'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        # ç²å–åŸå§‹æ–‡æœ¬
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in metadata.columns:
                text_column = col
                break
        
        if text_column is None:
            self.logger.warning("æœªæ‰¾åˆ°æ–‡æœ¬æ¬„ä½ï¼Œå°‡ä½¿ç”¨ç›¸ä¼¼åº¦æ³¨æ„åŠ›")
            similarity_attn = SimilarityAttention(self.config)
            return similarity_attn.compute_attention(embeddings, metadata, **kwargs)
        
        texts = metadata[text_column].fillna('').tolist()
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                raw_indices = metadata.index[metadata[topic_column] == topic]
                doc_indices = [int(idx) for idx in raw_indices.tolist()]
            else:
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            if not doc_indices:
                continue
            
            # ç²å–è©²ä¸»é¡Œçš„é—œéµè©
            keywords = topic_keywords.get(topic, [])
            if not keywords:
                # å¦‚æœæ²’æœ‰é—œéµè©ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
                for doc_idx in doc_indices:
                    if 0 <= doc_idx < len(embeddings):
                        weights[topic_idx][doc_idx] = 1.0 / len(doc_indices)
                continue
            
            # è¨ˆç®—é—œéµè©åˆ†æ•¸
            keyword_scores = []
            valid_doc_indices = []
            
            for idx in doc_indices:
                # æª¢æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
                if not (0 <= idx < len(texts)):
                    self.logger.warning(f"æ–‡æª”ç´¢å¼• {idx} è¶…å‡ºç¯„åœ [0, {len(texts)-1}]")
                    continue
                    
                valid_doc_indices.append(idx)
                text = texts[idx].lower()
                score = 0
                
                # è¨ˆç®—é—œéµè©å‡ºç¾æ¬¡æ•¸çš„åŠ æ¬Šå’Œ
                for i, keyword in enumerate(keywords):
                    # é—œéµè©æ¬Šé‡éš¨æ’åéæ¸›
                    keyword_weight = 1.0 / (i + 1)
                    count = text.count(keyword.lower())
                    score += count * keyword_weight
                
                keyword_scores.append(score)
            
            # æ›´æ–°æœ‰æ•ˆçš„æ–‡æª”ç´¢å¼•
            topic_indices[topic] = valid_doc_indices
            
            if not valid_doc_indices:
                continue
            
            # å¦‚æœæ‰€æœ‰åˆ†æ•¸ç‚ºé›¶ï¼Œä½¿ç”¨å¹³å‡æ¬Šé‡
            if sum(keyword_scores) == 0:
                uniform_weight = 1.0 / len(valid_doc_indices)
                for doc_idx in valid_doc_indices:
                    if 0 <= doc_idx < len(embeddings):
                        weights[topic_idx][doc_idx] = uniform_weight
            else:
                # æ­£è¦åŒ–åˆ†æ•¸
                keyword_scores = np.array(keyword_scores)
                topic_weights = keyword_scores / np.sum(keyword_scores)
                
                # é€å€‹è¨­ç½®æ¬Šé‡
                for i, doc_idx in enumerate(valid_doc_indices):
                    if 0 <= doc_idx < len(embeddings):
                        weights[topic_idx][doc_idx] = topic_weights[i]
        
        return weights, topic_indices


class SelfAttention(AttentionMechanism):
    """è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.logger.info(f"ğŸ”§ SelfAttention ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # å„ªåŒ–GPUè¨˜æ†¶é«”ä½¿ç”¨
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—è‡ªæ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œï¼Œå„ªå…ˆä½¿ç”¨æ•¸å€¼åŒ–æƒ…æ„Ÿæ¨™ç±¤
        if 'sentiment_numeric' in metadata.columns:
            topics = sorted(metadata['sentiment_numeric'].unique())
            topic_column = 'sentiment_numeric'
        elif 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å‰µå»ºå‡è¨­çš„æ•¸å€¼æƒ…æ„Ÿæ¨™ç±¤ï¼š0:è² é¢ã€1:ä¸­æ€§ã€2:æ­£é¢
            topics = [0, 1, 2]
            topic_column = 'sentiment_numeric'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                raw_indices = metadata.index[metadata[topic_column] == topic]
                doc_indices = [int(idx) for idx in raw_indices.tolist()]
            else:
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            # é©—è­‰ç´¢å¼•æœ‰æ•ˆæ€§ä¸¦éæ¿¾ç„¡æ•ˆç´¢å¼•
            valid_doc_indices = []
            for idx in doc_indices:
                if 0 <= idx < len(embeddings):
                    valid_doc_indices.append(idx)
                else:
                    self.logger.warning(f"æ–‡æª”ç´¢å¼• {idx} è¶…å‡ºåµŒå…¥å‘é‡ç¯„åœ [0, {len(embeddings)-1}]")
            
            topic_indices[topic] = valid_doc_indices
            
            if not valid_doc_indices or len(valid_doc_indices) < 2:
                # å¦‚æœæ–‡æª”æ•¸å°‘æ–¼2ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
                if valid_doc_indices:
                    uniform_weight = 1.0 / len(valid_doc_indices)
                    for doc_idx in valid_doc_indices:
                        weights[topic_idx][doc_idx] = uniform_weight
                continue
                
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[valid_doc_indices]
            
            # è½‰æ›ç‚ºå¼µé‡ä¸¦ç¢ºä¿åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
            try:
                topic_embeddings_tensor = torch.tensor(topic_embeddings, dtype=torch.float32).to(self.device)
                
                # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
                # ä½¿ç”¨ç¸®æ”¾é»ç©æ³¨æ„åŠ›
                d_k = topic_embeddings_tensor.size(-1)
                scores = torch.matmul(topic_embeddings_tensor, topic_embeddings_tensor.transpose(-2, -1)) / math.sqrt(d_k)
                
                # ä½¿ç”¨softmaxè½‰æ›ç‚ºæ¬Šé‡
                attn_weights = F.softmax(scores, dim=-1)
                
                # è¨ˆç®—æ¯å€‹æ–‡æª”çš„å¹³å‡æ³¨æ„åŠ›å¾—åˆ†
                avg_weights = torch.mean(attn_weights, dim=-1).cpu().numpy()
                
                # é€å€‹å¡«å……æ¬Šé‡çŸ©é™£
                for i, doc_idx in enumerate(valid_doc_indices):
                    weights[topic_idx][doc_idx] = avg_weights[i]
                    
            except Exception as e:
                self.logger.warning(f"è‡ªæ³¨æ„åŠ›è¨ˆç®—å¤±æ•—ï¼Œå›é€€åˆ°å‡ç­‰æ¬Šé‡: {str(e)}")
                # å›é€€åˆ°å‡ç­‰æ¬Šé‡
                uniform_weight = 1.0 / len(valid_doc_indices)
                for doc_idx in valid_doc_indices:
                    weights[topic_idx][doc_idx] = uniform_weight
        
        return weights, topic_indices


class GatedFusionNetwork(nn.Module):
    """é–€æ§å‹•æ…‹èåˆç¥ç¶“ç¶²è·¯
    
    æ ¹æ“šè¼¸å…¥æ–‡æœ¬ç‰¹å¾µå‹•æ…‹è¨ˆç®—æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ¬Šé‡ï¼Œ
    è§£æ±ºéœæ…‹æ¬Šé‡åˆ†é…çš„å•é¡Œ
    """
    
    def __init__(self, input_dim: int, num_mechanisms: int, hidden_dim: int = 128):
        """åˆå§‹åŒ–é–€æ§èåˆç¶²è·¯
        
        Args:
            input_dim: è¼¸å…¥ç‰¹å¾µç¶­åº¦ï¼ˆæ–‡æœ¬åµŒå…¥ç¶­åº¦ï¼‰
            num_mechanisms: æ³¨æ„åŠ›æ©Ÿåˆ¶æ•¸é‡
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        super(GatedFusionNetwork, self).__init__()
        self.input_dim = input_dim
        self.num_mechanisms = num_mechanisms
        self.hidden_dim = hidden_dim
        
        # ç‰¹å¾µæå–ç¶²è·¯
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # é–€æ§æ¬Šé‡ç”Ÿæˆç¶²è·¯
        self.gate_network = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, num_mechanisms)
            # ç§»é™¤ Softmaxï¼Œå°‡åœ¨ forward æ–¹æ³•ä¸­æ‰‹å‹•æ‡‰ç”¨
        )
        
        # äº’è£œæ€§å»ºæ¨¡å±¤
        self.complementarity_layer = nn.Linear(num_mechanisms, num_mechanisms)
        
    def forward(self, text_features: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­
        
        Args:
            text_features: æ–‡æœ¬ç‰¹å¾µå¼µé‡ [batch_size, input_dim]
            
        Returns:
            dynamic_weights: å‹•æ…‹æ¬Šé‡ [batch_size, num_mechanisms]
        """
        # ç‰¹å¾µæå–
        extracted_features = self.feature_extractor(text_features)
        
        # ç”ŸæˆåŸºç¤é–€æ§æ¬Šé‡ (ç§»é™¤é€™è£¡çš„ Softmaxï¼Œé¿å…é›™é‡æ­¸ä¸€åŒ–)
        base_logits = self.gate_network(extracted_features)  # ç¾åœ¨ç›´æ¥ä½¿ç”¨ï¼Œå› ç‚ºå·²ç§»é™¤ Softmax å±¤
        
        # äº’è£œæ€§èª¿æ•´
        complementarity_weights = torch.sigmoid(self.complementarity_layer(
            torch.softmax(base_logits, dim=1)  # æš«æ™‚æ‡‰ç”¨ softmax ç”¨æ–¼äº’è£œæ€§è¨ˆç®—
        ))
        
        # çµåˆåŸºç¤æ¬Šé‡å’Œäº’è£œæ€§æ¬Šé‡
        # ä½¿ç”¨åŠ æ³•è€Œä¸æ˜¯ä¹˜æ³•ï¼Œé¿å…æ¬Šé‡éå°
        dynamic_logits = base_logits + torch.log(complementarity_weights + 1e-8)
        
        # æœ€çµ‚æ­¸ä¸€åŒ–
        dynamic_weights = F.softmax(dynamic_logits, dim=1)
        
        return dynamic_weights


class DynamicCombinedAttention(AttentionMechanism):
    """å‹•æ…‹çµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶
    
    ä½¿ç”¨é–€æ§èåˆç¶²è·¯å‹•æ…‹è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡ï¼Œ
    æ ¹æ“šæ–‡æœ¬å…§å®¹è‡ªå‹•èª¿æ•´æ³¨æ„åŠ›çµ„åˆ
    """
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–å‹•æ…‹çµ„åˆæ³¨æ„åŠ›æ©Ÿåˆ¶"""
        super().__init__(config)
        self.fusion_network = None
        # è¨­å‚™é¸æ“‡å’Œç®¡ç†
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # ç°¡åŒ–è¼¸å‡ºï¼šåªåœ¨éœ€è¦æ™‚é¡¯ç¤ºè¨­å‚™è³‡è¨Š
        if self.device.type == 'cuda':
            print(f"ğŸ”§ GNFå‹•æ…‹æ³¨æ„åŠ›ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            torch.cuda.empty_cache()
        self.mechanism_names = ['similarity', 'keyword', 'self']
        
    def _initialize_fusion_network(self, input_dim: int):
        """åˆå§‹åŒ–èåˆç¶²è·¯"""
        if self.fusion_network is None:
            try:
                self.fusion_network = GatedFusionNetwork(
                    input_dim=input_dim,
                    num_mechanisms=len(self.mechanism_names),
                    hidden_dim=128
                ).to(self.device)
                
                # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼ä»¥ç²å¾—ä¸€è‡´çš„çµæœ
                self.fusion_network.eval()
                    
            except Exception as e:
                self.logger.error(f"   âŒ GatedFusionNetwork åˆå§‹åŒ–å¤±æ•—: {str(e)}")
                # å›é€€åˆ°CPU
                if self.device.type == 'cuda':
                    self.logger.warning("   âš ï¸ å›é€€åˆ°CPUè¨­å‚™")
                    self.device = torch.device('cpu')
                    self.fusion_network = GatedFusionNetwork(
                        input_dim=input_dim,
                        num_mechanisms=len(self.mechanism_names),
                        hidden_dim=128
                    ).to(self.device)
                    self.fusion_network.eval()
                else:
                    raise
    
    def _extract_text_features(self, embeddings: np.ndarray, metadata: pd.DataFrame) -> torch.Tensor:
        """å¾åµŒå…¥å’Œå…ƒæ•¸æ“šä¸­æå–æ–‡æœ¬ç‰¹å¾µ
        
        Args:
            embeddings: æ–‡æœ¬åµŒå…¥
            metadata: å…ƒæ•¸æ“š
            
        Returns:
            text_features: æ–‡æœ¬ç‰¹å¾µå¼µé‡
        """
        # è¨ˆç®—çµ±è¨ˆç‰¹å¾µ
        batch_size = embeddings.shape[0]
        features_list = []
        
        for i in range(batch_size):
            # åŸºç¤çµ±è¨ˆç‰¹å¾µ
            embedding_mean = np.mean(embeddings[i])
            embedding_std = np.std(embeddings[i])
            embedding_max = np.max(embeddings[i])
            embedding_min = np.min(embeddings[i])
            
            # æ–‡æœ¬é•·åº¦ç‰¹å¾µï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            text_length = len(metadata.iloc[i].get('text', '')) if 'text' in metadata.columns else 100
            text_length_norm = min(text_length / 500.0, 1.0)  # æ­£è¦åŒ–é•·åº¦
            
            # çµ„åˆç‰¹å¾µ
            features = [
                embedding_mean, embedding_std, embedding_max, embedding_min,
                text_length_norm
            ]
            
            # åŠ å…¥åµŒå…¥çš„å‰å¹¾å€‹ç¶­åº¦ä½œç‚ºç‰¹å¾µ
            top_dims = min(10, embeddings.shape[1])
            features.extend(embeddings[i][:top_dims].tolist())
            
            features_list.append(features)
        
        # è½‰æ›ç‚ºå¼µé‡ä¸¦ç¢ºä¿åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
        features_array = np.array(features_list)
        
        try:
            # ç›´æ¥åœ¨ç›®æ¨™è¨­å‚™ä¸Šå‰µå»ºå¼µé‡ä»¥é¿å…CPU-GPUå‚³è¼¸
            if self.device.type == 'cuda':
                text_features = torch.cuda.FloatTensor(features_array)
            else:
                text_features = torch.FloatTensor(features_array).to(self.device)
                
            self.logger.debug(f"   ğŸ“Š æ–‡æœ¬ç‰¹å¾µå¼µé‡: å½¢ç‹€={text_features.shape}, è¨­å‚™={text_features.device}")
            return text_features
            
        except Exception as e:
            self.logger.warning(f"   âš ï¸ GPUå¼µé‡å‰µå»ºå¤±æ•—ï¼Œå›é€€åˆ°CPU: {str(e)}")
            # å›é€€åˆ°CPU
            return torch.FloatTensor(features_array).to(torch.device('cpu'))
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—å‹•æ…‹çµ„åˆæ³¨æ„åŠ›æ¬Šé‡"""
        # ç°¡åŒ–ï¼šåªé¡¯ç¤ºé—œéµä¿¡æ¯
        print(f"ğŸ¯ è¨ˆç®—å‹•æ…‹æ³¨æ„åŠ›æ¬Šé‡ ({embeddings.shape[0]} æ¢æ–‡æœ¬)")
        
        try:
            # åˆå§‹åŒ–èåˆç¶²è·¯
            feature_dim = 5 + min(10, embeddings.shape[1])  # 5å€‹çµ±è¨ˆç‰¹å¾µ + å‰10å€‹åµŒå…¥ç¶­åº¦
            self._initialize_fusion_network(feature_dim)
            
            # æå–æ–‡æœ¬ç‰¹å¾µ
            text_features = self._extract_text_features(embeddings, metadata)
            
            # ç¢ºä¿èåˆç¶²è·¯å’Œæ–‡æœ¬ç‰¹å¾µåœ¨åŒä¸€è¨­å‚™ä¸Š
            if text_features.device != next(self.fusion_network.parameters()).device:
                self.logger.warning(f"   âš ï¸ è¨­å‚™ä¸åŒ¹é…ï¼Œèª¿æ•´æ–‡æœ¬ç‰¹å¾µè¨­å‚™: {text_features.device} -> {next(self.fusion_network.parameters()).device}")
                text_features = text_features.to(next(self.fusion_network.parameters()).device)
            
            # ä½¿ç”¨èåˆç¶²è·¯è¨ˆç®—å‹•æ…‹æ¬Šé‡
            with torch.no_grad():
                # ç¢ºä¿åœ¨è©•ä¼°æ¨¡å¼
                self.fusion_network.eval()
                
                dynamic_weights = self.fusion_network(text_features)
                
                # ç¢ºä¿çµæœåœ¨CPUä¸Šä»¥é¿å…å¾ŒçºŒè™•ç†çš„è¨­å‚™å•é¡Œ
                if dynamic_weights.device.type == 'cuda':
                    dynamic_weights_np = dynamic_weights.cpu().numpy()
                else:
                    dynamic_weights_np = dynamic_weights.numpy()
                    
                # æ¸…ç†GPUè¨˜æ†¶é«”
                if self.device.type == 'cuda':
                    del dynamic_weights  # æ˜ç¢ºåˆªé™¤GPUå¼µé‡
                    torch.cuda.empty_cache()
                    
        except Exception as e:
            self.logger.error(f"   âŒ å‹•æ…‹æ¬Šé‡è¨ˆç®—å¤±æ•—: {str(e)}")
            # æ¸…ç†GPUè¨˜æ†¶é«”
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            raise
        
        # å°‡å‹•æ…‹æ¬Šé‡è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        weights_dict = {}
        for i, mechanism_name in enumerate(self.mechanism_names):
            # å°æ¯å€‹æ¨£æœ¬çš„æ¬Šé‡å–å¹³å‡
            weights_dict[mechanism_name] = float(np.mean(dynamic_weights_np[:, i]))
        
        # ä¿å­˜å­¸ç¿’åˆ°çš„æ¬Šé‡ä¾›å¾ŒçºŒçµ„åˆåˆ†æä½¿ç”¨
        self.learned_weights = weights_dict.copy()
        self.weights_learned = True
        
        # ç°¡åŒ–ï¼šç§»é™¤GPUè¨˜æ†¶é«”ç›£æ§è¼¸å‡º
        
        # ç°¡åŒ–è¼¸å‡ºï¼šåªé¡¯ç¤ºæœ€çµ‚æ¬Šé‡
        weights_str = ", ".join([f"{k}:{v:.3f}" for k, v in weights_dict.items()])
        print(f"   å­¸ç¿’æ¬Šé‡: {weights_str}")
        
        # ä½¿ç”¨å‹•æ…‹æ¬Šé‡è¨ˆç®—æ³¨æ„åŠ›
        weights, indices = self._compute_weighted_attention(embeddings, metadata, weights_dict, **kwargs)
        
        # å°‡å‹•æ…‹æ¬Šé‡ä¿¡æ¯æ·»åŠ åˆ°indicesä¸­ï¼Œä¾›GUIé¡¯ç¤ºä½¿ç”¨
        if isinstance(indices, dict):
            indices['dynamic_weights'] = weights_dict
            indices['is_dynamic'] = True
        else:
            indices = {
                'topic_indices': indices,
                'dynamic_weights': weights_dict,
                'is_dynamic': True
            }
            
        return weights, indices
    
    def _compute_weighted_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, 
                                   weights_dict: Dict[str, float], **kwargs) -> Tuple[np.ndarray, Dict]:
        """ä½¿ç”¨æ¬Šé‡è¨ˆç®—æ³¨æ„åŠ›ï¼ˆèˆ‡åŸCombinedAttentionç›¸åŒçš„é‚è¼¯ï¼‰"""
        # åªä¿ç•™æ¬Šé‡ä¸ç‚º0çš„æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œéæ¿¾æ‰å…ƒæ•¸æ“šéµ
        min_threshold = 1e-6
        active_weights = {k: v for k, v in weights_dict.items() 
                         if not k.startswith('_') and isinstance(v, (int, float)) and v > min_threshold}
        
        if not active_weights:
            print("âš ï¸  æ¬Šé‡ç„¡æ•ˆï¼Œå›é€€åˆ°å‡ç­‰æ¬Šé‡")
            return NoAttention(self.config).compute_attention(embeddings, metadata, **kwargs)
        
        # åˆå§‹åŒ–æ¬Šé‡çŸ©é™£å’Œçµæœ
        weights = None
        topic_indices = None
        
        # æ ¹æ“šæ¬Šé‡è¨ˆç®—å°æ‡‰çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        for mechanism_name, weight in active_weights.items():
            if mechanism_name == 'similarity':
                mechanism = SimilarityAttention(self.config)
            elif mechanism_name == 'keyword':
                mechanism = KeywordGuidedAttention(self.config)
            elif mechanism_name == 'self':
                mechanism = SelfAttention(self.config)
            else:
                self.logger.warning(f"æœªçŸ¥çš„æ³¨æ„åŠ›æ©Ÿåˆ¶: {mechanism_name}ï¼Œå°‡è¢«å¿½ç•¥")
                continue
            
            # è¨ˆç®—è©²æ©Ÿåˆ¶çš„æ³¨æ„åŠ›æ¬Šé‡
            mechanism_weights, indices = mechanism.compute_attention(embeddings, metadata, **kwargs)
            
            # å¦‚æœæ˜¯ç¬¬ä¸€å€‹æ©Ÿåˆ¶ï¼Œåˆå§‹åŒ–çµæœ
            if weights is None:
                weights = weight * mechanism_weights
                topic_indices = indices
            else:
                # ç´¯åŠ åŠ æ¬Šçµæœ
                weights += weight * mechanism_weights
        
        return weights, topic_indices


class CombinedAttention(AttentionMechanism):
    """çµ„åˆå‹æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆåŸæœ‰çš„éœæ…‹ç‰ˆæœ¬ï¼‰"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—çµ„åˆå‹æ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–çµ„åˆæ¬Šé‡
        weights_dict = kwargs.get('weights', {})
        
        # åªä¿ç•™æ¬Šé‡ä¸ç‚º0çš„æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œéæ¿¾æ‰å…ƒæ•¸æ“šéµ
        active_weights = {k: v for k, v in weights_dict.items() 
                         if not k.startswith('_') and isinstance(v, (int, float)) and v > 0}
        
        if not active_weights:
            self.logger.warning("æ²’æœ‰æŒ‡å®šä»»ä½•æœ‰æ•ˆçš„æ³¨æ„åŠ›æ¬Šé‡ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡")
            return NoAttention(self.config).compute_attention(embeddings, metadata, **kwargs)
        
        # å‹•æ…‹æ­£è¦åŒ–æ¬Šé‡ç¢ºä¿ç¸½å’Œç‚º1
        total_weight = sum(active_weights.values())
        if abs(total_weight - 1.0) > 1e-5:
            self.logger.warning(f"æ³¨æ„åŠ›æ¬Šé‡ç¸½å’Œ {total_weight} ä¸ç‚º1ï¼Œå°‡é€²è¡Œæ­¸ä¸€åŒ–")
            active_weights = {k: v/total_weight for k, v in active_weights.items()}
        
        # åˆå§‹åŒ–æ¬Šé‡çŸ©é™£å’Œçµæœ
        weights = None
        topic_indices = None
        
        # æ ¹æ“šæ¬Šé‡è¨ˆç®—å°æ‡‰çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
        for mechanism_name, weight in active_weights.items():
            if mechanism_name == 'similarity':
                mechanism = SimilarityAttention(self.config)
            elif mechanism_name == 'keyword':
                mechanism = KeywordGuidedAttention(self.config)
            elif mechanism_name == 'self':
                mechanism = SelfAttention(self.config)
            else:
                self.logger.warning(f"æœªçŸ¥çš„æ³¨æ„åŠ›æ©Ÿåˆ¶: {mechanism_name}ï¼Œå°‡è¢«å¿½ç•¥")
                continue
            
            # è¨ˆç®—è©²æ©Ÿåˆ¶çš„æ³¨æ„åŠ›æ¬Šé‡
            mechanism_weights, indices = mechanism.compute_attention(embeddings, metadata, **kwargs)
            
            # å¦‚æœæ˜¯ç¬¬ä¸€å€‹æ©Ÿåˆ¶ï¼Œåˆå§‹åŒ–çµæœ
            if weights is None:
                weights = weight * mechanism_weights
                topic_indices = indices
            else:
                # ç´¯åŠ åŠ æ¬Šçµæœ
                weights += weight * mechanism_weights
        
        return weights, topic_indices


def create_attention_mechanism(attention_type: str, config=None) -> AttentionMechanism:
    """å‰µå»ºæŒ‡å®šé¡å‹çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    
    Args:
        attention_type: æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹ï¼Œå¯ä»¥æ˜¯'no', 'similarity', 'keyword', 'self', 'combined'æˆ–'dynamic'
        config: é…ç½®åƒæ•¸
        
    Returns:
        AttentionMechanism: æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ä¾‹
    """
    attention_type = attention_type.lower()
    
    if attention_type == 'no' or attention_type == 'none':
        return NoAttention(config)
    elif attention_type == 'similarity':
        return SimilarityAttention(config)
    elif attention_type == 'keyword':
        return KeywordGuidedAttention(config)
    elif attention_type == 'self':
        return SelfAttention(config)
    elif attention_type == 'combined':
        return CombinedAttention(config)
    elif attention_type == 'dynamic' or attention_type == 'dynamic_combined':
        return DynamicCombinedAttention(config)
    else:
        logger.warning(f"æœªçŸ¥çš„æ³¨æ„åŠ›é¡å‹: {attention_type}ï¼Œä½¿ç”¨ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶")
        return NoAttention(config)


def apply_attention_mechanism(attention_type: str, embeddings: np.ndarray, metadata: pd.DataFrame, 
                            topics_path: Optional[str] = None, weights: Optional[Dict] = None) -> Dict[str, Any]:
    """æ‡‰ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è¨ˆç®—é¢å‘å‘é‡
    
    Args:
        attention_type: æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        embeddings: åµŒå…¥å‘é‡
        metadata: å…ƒæ•¸æ“š
        topics_path: ä¸»é¡Œè©æ–‡ä»¶è·¯å¾‘ï¼Œç”¨æ–¼é—œéµè©æ³¨æ„åŠ›
        weights: çµ„åˆæ³¨æ„åŠ›çš„æ¬Šé‡å­—å…¸
        
    Returns:
        dict: åŒ…å«é¢å‘å‘é‡å’Œè©•ä¼°æŒ‡æ¨™çš„å­—å…¸
    """
    # å‰µå»ºé…ç½®
    config = {}
    
    # é¡¯ç¤ºç•¶å‰è™•ç†çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    logger.debug(f"   ğŸ”„ æ­£åœ¨è¨ˆç®— {attention_type} æ³¨æ„åŠ›æ©Ÿåˆ¶...")
    
    # å‰µå»ºæ³¨æ„åŠ›æ©Ÿåˆ¶
    attention_mech = create_attention_mechanism(attention_type, config)
    
    # æº–å‚™é—œéµå­—åƒæ•¸
    kwargs = {}
    if topics_path:
        kwargs['topics_path'] = topics_path
    if weights:
        kwargs['weights'] = weights
    
    # è¨ˆç®—é¢å‘å‘é‡
    logger.debug(f"   ğŸ“Š è¨ˆç®—é¢å‘å‘é‡...")
    aspect_vectors, attention_data = attention_mech.compute_aspect_vectors(
        embeddings, metadata, **kwargs
    )
    
    # è©•ä¼°é¢å‘å‘é‡
    logger.debug(f"   ğŸ“ è©•ä¼°æŒ‡æ¨™...")
    metrics = attention_mech.evaluate(aspect_vectors, embeddings, metadata)
    
    # é¡¯ç¤ºçµæœ
    logger.info(f"   âœ… {attention_type} æ³¨æ„åŠ›å®Œæˆ - å…§èšåº¦: {metrics['coherence']:.4f}, "
               f"åˆ†é›¢åº¦: {metrics['separation']:.4f}, ç¶œåˆå¾—åˆ†: {metrics['combined_score']:.4f}")
    
    result = {
        'aspect_vectors': aspect_vectors,
        'attention_data': attention_data,
        'metrics': metrics,
        'attention_type': attention_type
    }
    
    # å¦‚æœæœ‰å‹•æ…‹æ¬Šé‡ä¿¡æ¯ï¼Œæ·»åŠ åˆ°çµæœä¸­
    if isinstance(attention_data, dict) and 'dynamic_weights' in attention_data:
        result['dynamic_weights'] = attention_data['dynamic_weights']
        result['is_dynamic'] = attention_data.get('is_dynamic', False)
        logger.debug(f"   ğŸ¯ å‹•æ…‹æ¬Šé‡: {attention_data['dynamic_weights']}")
    
    return result


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒç´šåˆ¥
    logging.basicConfig(level=logging.INFO)
    
    # æ¸¬è©¦æ³¨æ„åŠ›æ©Ÿåˆ¶
    # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
    np.random.seed(42)
    n_docs = 100
    embed_dim = 768  # ä½¿ç”¨BERTçš„æ¨™æº–ç¶­åº¦
    n_topics = 3
    
    # æ¨¡æ“¬åµŒå…¥å‘é‡
    embeddings = np.random.randn(n_docs, embed_dim)
    
    # æ¨¡æ“¬å…ƒæ•¸æ“š
    sentiments = ['positive', 'negative', 'neutral']
    doc_sentiments = np.random.choice(sentiments, size=n_docs)
    metadata = pd.DataFrame({
        'sentiment': doc_sentiments,
        'text': ['This is document ' + str(i) for i in range(n_docs)]
    })
    
    # æ¸¬è©¦æ‰€æœ‰æ³¨æ„åŠ›æ©Ÿåˆ¶
    attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
    
    for attn_type in attention_types:
        logger.info(f"æ¸¬è©¦ {attn_type} æ³¨æ„åŠ›æ©Ÿåˆ¶")
        
        # å‰µå»ºæ³¨æ„åŠ›æ©Ÿåˆ¶
        attention_mech = create_attention_mechanism(attn_type)
        
        # æ¸¬è©¦è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        if attn_type == 'combined':
            # æ¸¬è©¦ä¸åŒçš„çµ„åˆæ¬Šé‡
            test_weights = [
                {'similarity': 1.0},  # å–®ä¸€æ³¨æ„åŠ›
                {'similarity': 0.5, 'keyword': 0.5},  # é›™é‡çµ„åˆ
                {'similarity': 0.33, 'keyword': 0.33, 'self': 0.34}  # ä¸‰é‡çµ„åˆ
            ]
            
            for weights in test_weights:
                logger.info(f"æ¸¬è©¦çµ„åˆæ¬Šé‡: {weights}")
                weights_array, topic_indices = attention_mech.compute_attention(
                    embeddings, metadata, weights=weights
                )
        else:
            weights_array, topic_indices = attention_mech.compute_attention(embeddings, metadata)
        
        # æ¸¬è©¦è¨ˆç®—é¢å‘å‘é‡
        aspect_vectors, _ = attention_mech.compute_aspect_vectors(embeddings, metadata)
        
        # æ¸¬è©¦è©•ä¼°æŒ‡æ¨™
        metrics = attention_mech.evaluate(aspect_vectors, embeddings, metadata)
        
        logger.info(f"å…§èšåº¦: {metrics['coherence']:.4f}")
        logger.info(f"åˆ†é›¢åº¦: {metrics['separation']:.4f}")
        logger.info(f"ç¶œåˆå¾—åˆ†: {metrics['combined_score']:.4f}")
        logger.info("---")
    
    logger.info("æ³¨æ„åŠ›æ©Ÿåˆ¶æ¸¬è©¦å®Œæˆ") 