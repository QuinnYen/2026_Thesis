"""
æ³¨æ„åŠ›æ©Ÿåˆ¶æ¨¡çµ„ - æä¾›å„ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ç¾
åŒ…å«ç›¸ä¼¼åº¦æ³¨æ„åŠ›ã€é—œéµè©æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›å’Œçµ„åˆæ³¨æ„åŠ›
"""

import os
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import json
import logging
import time
import random
import torch
import torch.nn.functional as F
import math
from typing import Dict, List, Tuple, Optional, Any

# å›ºå®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿çµæœå¯é‡ç¾
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# å›ºå®šPyTorchéš¨æ©Ÿç¨®å­
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed(RANDOM_SEED)
    torch.cuda.manual_seed_all(RANDOM_SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class AttentionMechanism:
    """æ³¨æ„åŠ›æ©Ÿåˆ¶åŸºé¡"""
    
    def __init__(self, config=None):
        """åˆå§‹åŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶
        
        Args:
            config: é…ç½®åƒæ•¸å­—å…¸
        """
        self.config = config or {}
        self.logger = logger
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        
        Args:
            embeddings: æ–‡æª”åµŒå…¥å‘é‡ï¼Œå½¢ç‹€ç‚º [n_docs, embed_dim]
            metadata: æ–‡æª”å…ƒæ•¸æ“šï¼ŒåŒ…å«é¢å‘æ¨™ç±¤
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            weights: æ³¨æ„åŠ›æ¬Šé‡ï¼Œå½¢ç‹€ç‚º [n_topics, n_docs]
            topic_indices: æ¯å€‹ä¸»é¡ŒåŒ…å«çš„æ–‡æª”ç´¢å¼•å­—å…¸
        """
        raise NotImplementedError("å­é¡å¿…é ˆå¯¦ç¾æ­¤æ–¹æ³•")
    
    def compute_aspect_vectors(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[Dict, Dict]:
        """è¨ˆç®—é¢å‘å‘é‡
        
        Args:
            embeddings: æ–‡æª”åµŒå…¥å‘é‡
            metadata: æ–‡æª”å…ƒæ•¸æ“š
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            aspect_vectors: é¢å‘å‘é‡å­—å…¸
            attention_data: æ³¨æ„åŠ›æ¬Šé‡å’Œç›¸é—œæ•¸æ“š
        """
        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        weights, topic_indices = self.compute_attention(embeddings, metadata, **kwargs)
        
        # è¨ˆç®—é¢å‘å‘é‡
        topics = list(topic_indices.keys())
        aspect_vectors = {}
        
        for topic_idx, topic in enumerate(topics):
            # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
            doc_indices = topic_indices[topic]
            
            if not doc_indices:
                self.logger.warning(f"ä¸»é¡Œ {topic} æ²’æœ‰æ–‡æª”ï¼Œå°‡ä½¿ç”¨é›¶å‘é‡")
                aspect_vectors[topic] = np.zeros(embeddings.shape[1])
                continue
            
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[doc_indices]
            
            # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”æ¬Šé‡
            topic_weights = weights[topic_idx][doc_indices]
            
            # æ­£è¦åŒ–æ¬Šé‡ç¢ºä¿ç¸½å’Œç‚º1
            if np.sum(topic_weights) > 0:
                topic_weights = topic_weights / np.sum(topic_weights)
            else:
                topic_weights = np.ones(len(doc_indices)) / len(doc_indices)
            
            # è¨ˆç®—åŠ æ¬Šå¹³å‡
            weighted_vector = np.zeros(embeddings.shape[1])
            for i, embed in enumerate(topic_embeddings):
                weighted_vector += topic_weights[i] * embed
                
            aspect_vectors[topic] = weighted_vector
        
        # è¿”å›é¢å‘å‘é‡å’Œæ³¨æ„åŠ›æ•¸æ“š
        attention_data = {
            "weights": weights,
            "topic_indices": topic_indices,
            "topics": topics
        }
        
        return aspect_vectors, attention_data
    
    def evaluate(self, aspect_vectors: Dict, embeddings: np.ndarray, metadata: pd.DataFrame) -> Dict:
        """è©•ä¼°æ³¨æ„åŠ›æ©Ÿåˆ¶çš„æ•ˆèƒ½
        
        Args:
            aspect_vectors: é¢å‘å‘é‡å­—å…¸
            embeddings: æ–‡æª”åµŒå…¥å‘é‡
            metadata: æ–‡æª”å…ƒæ•¸æ“š
            
        Returns:
            metrics: è©•ä¼°æŒ‡æ¨™å­—å…¸
        """
        topics = list(aspect_vectors.keys())
        self.logger.info(f"é–‹å§‹è©•ä¼°ï¼Œå…±æœ‰ {len(topics)} å€‹é¢å‘")
        
        # è¨ˆç®—é¢å‘å…§èšåº¦
        coherence = 0.0
        doc_count = 0
        topic_coherence_dict = {}
        
        self.logger.info("\né–‹å§‹è¨ˆç®—é¢å‘å…§èšåº¦...")
        
        for topic in topics:
            # ç²å–è©²é¢å‘çš„æ–‡æª”
            topic_docs = metadata[metadata['sentiment'] == topic] if 'sentiment' in metadata.columns else metadata[metadata.index.isin([topic])]
            
            if len(topic_docs) == 0:
                self.logger.warning(f"é¢å‘ '{topic}' æ²’æœ‰å°æ‡‰çš„æ–‡æª”")
                topic_coherence_dict[topic] = 0.0
                continue
            
            # ç²å–è©²é¢å‘çš„åµŒå…¥å‘é‡ç´¢å¼•
            doc_indices = topic_docs.index.tolist()
            topic_embeddings = embeddings[doc_indices]
            aspect_vector = aspect_vectors[topic]
            
            # è¨ˆç®—è©²é¢å‘çš„å…§èšåº¦
            topic_coherence = 0.0
            for embed in topic_embeddings:
                similarity = np.dot(aspect_vector, embed) / (np.linalg.norm(aspect_vector) * np.linalg.norm(embed))
                topic_coherence += similarity
            
            if len(topic_embeddings) > 0:
                topic_coherence /= len(topic_embeddings)
                topic_coherence_dict[topic] = float(topic_coherence)
                coherence += topic_coherence * len(topic_embeddings)
                doc_count += len(topic_embeddings)
                
                self.logger.info(f"é¢å‘ '{topic}' çš„å…§èšåº¦: {topic_coherence:.4f} (æ–‡æª”æ•¸: {len(topic_embeddings)})")
            else:
                topic_coherence_dict[topic] = 0.0
        
        if doc_count > 0:
            coherence /= doc_count
            
        self.logger.info(f"\nç¸½é«”å…§èšåº¦: {coherence:.4f}")
        
        # è¨ˆç®—é¢å‘åˆ†é›¢åº¦
        separation = 0.0
        pair_count = 0
        topic_separation_dict = {}
        aspect_separation_values = {topic: [] for topic in topics}
        
        self.logger.info("\né–‹å§‹è¨ˆç®—é¢å‘é–“çš„åˆ†é›¢åº¦...")
        
        for i in range(len(topics)):
            for j in range(i+1, len(topics)):
                topic_i = topics[i]
                topic_j = topics[j]
                vec_i = aspect_vectors[topic_i]
                vec_j = aspect_vectors[topic_j]
                
                # è¨ˆç®—é¤˜å¼¦è·é›¢ (1 - ç›¸ä¼¼åº¦)
                similarity = np.dot(vec_i, vec_j) / (np.linalg.norm(vec_i) * np.linalg.norm(vec_j))
                distance = 1.0 - similarity
                
                separation += distance
                pair_count += 1
                
                # å­˜å„²è©²å°é¢å‘çš„åˆ†é›¢åº¦
                pair_key = f"{topic_i}_{topic_j}"
                topic_separation_dict[pair_key] = float(distance)
                
                # ç‚ºå…©å€‹é¢å‘éƒ½æ·»åŠ é€™å€‹åˆ†é›¢åº¦å€¼
                aspect_separation_values[topic_i].append(distance)
                aspect_separation_values[topic_j].append(distance)
                
                self.logger.info(f"é¢å‘ '{topic_i}' å’Œ '{topic_j}' çš„åˆ†é›¢åº¦: {distance:.4f}")
        
        if pair_count > 0:
            separation /= pair_count
            
        self.logger.info(f"\nç¸½é«”åˆ†é›¢åº¦: {separation:.4f}")
            
        # è¨ˆç®—æ¯å€‹é¢å‘çš„å¹³å‡åˆ†é›¢åº¦
        aspect_avg_separation = {}
        for topic in topics:
            if aspect_separation_values[topic]:
                aspect_avg_separation[topic] = float(np.mean(aspect_separation_values[topic]))
                self.logger.info(f"é¢å‘ '{topic}' çš„å¹³å‡åˆ†é›¢åº¦: {aspect_avg_separation[topic]:.4f}")
            else:
                aspect_avg_separation[topic] = 0.0
            
        # è¨ˆç®—ç¶œåˆå¾—åˆ†
        coherence_weight = self.config.get('coherence_weight', 0.5)
        separation_weight = self.config.get('separation_weight', 0.5)
        combined_score = coherence_weight * coherence + separation_weight * separation
        
        self.logger.info(f"\nç¶œåˆå¾—åˆ†: {combined_score:.4f} (å…§èšåº¦æ¬Šé‡: {coherence_weight}, åˆ†é›¢åº¦æ¬Šé‡: {separation_weight})")
            
        # è¿”å›è©•ä¼°æŒ‡æ¨™
        metrics = {
            # ç¸½é«”æŒ‡æ¨™
            "coherence": float(coherence),
            "separation": float(separation),
            "combined_score": float(combined_score),
            
            # é¢å‘ç‰¹å®šæŒ‡æ¨™
            "topic_coherence": topic_coherence_dict,
            "topic_separation": topic_separation_dict,
            "aspect_separation": aspect_avg_separation,
            
            # è©³ç´°æŒ‡æ¨™
            "metrics_details": {
                "weights": {
                    "coherence_weight": coherence_weight,
                    "separation_weight": separation_weight
                },
                "per_aspect": {
                    "coherence": topic_coherence_dict,
                    "separation": aspect_avg_separation,
                    "pairwise_separation": topic_separation_dict
                }
            }
        }
        
        return metrics


class NoAttention(AttentionMechanism):
    """ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆå¹³å‡ï¼‰"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—å‡ç­‰æ¬Šé‡ï¼ˆç›¸ç•¶æ–¼å¹³å‡ï¼‰"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œ
        if 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å‰µå»ºå‡è¨­çš„æƒ…æ„Ÿæ¨™ç±¤
            topics = ['positive', 'negative', 'neutral']
            topic_column = 'sentiment'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£ï¼ˆæ‰€æœ‰æ¬Šé‡ç›¸ç­‰ï¼‰
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
                doc_indices = metadata.index[metadata[topic_column] == topic].tolist()
            else:
                # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œå¹³å‡åˆ†é…
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            # è¨­ç½®å‡ç­‰æ¬Šé‡
            if doc_indices:
                weights[topic_idx][doc_indices] = 1.0 / len(doc_indices)
        
        return weights, topic_indices


class SimilarityAttention(AttentionMechanism):
    """åŸºæ–¼ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—åŸºæ–¼ç›¸ä¼¼åº¦çš„æ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œ
        if 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            # å‰µå»ºå‡è¨­çš„æƒ…æ„Ÿæ¨™ç±¤
            topics = ['positive', 'negative', 'neutral']
            topic_column = 'sentiment'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                # ç²å–è©²ä¸»é¡Œçš„æ–‡æª”ç´¢å¼•
                doc_indices = metadata.index[metadata[topic_column] == topic].tolist()
            else:
                # å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œå¹³å‡åˆ†é…
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            if not doc_indices:
                continue
                
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[doc_indices]
            
            # è¨ˆç®—ä¸­å¿ƒå‘é‡
            center = np.mean(topic_embeddings, axis=0)
            
            # è¨ˆç®—æ¯å€‹æ–‡æª”èˆ‡ä¸­å¿ƒçš„ç›¸ä¼¼åº¦
            similarities = []
            for embed in topic_embeddings:
                similarity = np.dot(center, embed) / (np.linalg.norm(center) * np.linalg.norm(embed))
                similarities.append(similarity)
            
            # è½‰æ›ç‚º softmax æ¬Šé‡
            similarities = np.array(similarities)
            # é¿å…æ•¸å€¼ä¸ç©©å®š
            similarities = similarities - np.max(similarities)
            exp_sim = np.exp(similarities)
            topic_weights = exp_sim / np.sum(exp_sim)
            
            # å¡«å……æ¬Šé‡çŸ©é™£
            weights[topic_idx][doc_indices] = topic_weights
        
        return weights, topic_indices


class KeywordGuidedAttention(AttentionMechanism):
    """åŸºæ–¼é—œéµè©çš„æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—åŸºæ–¼é—œéµè©çš„æ³¨æ„åŠ›æ¬Šé‡"""
        # æª¢æŸ¥æ˜¯å¦æä¾›äº†é—œéµè©
        topic_keywords = kwargs.get('topic_keywords')
        if topic_keywords is None:
            # å˜—è©¦å¾topics_pathåŠ è¼‰
            topics_path = kwargs.get('topics_path')
            if topics_path and os.path.exists(topics_path):
                try:
                    with open(topics_path, 'r', encoding='utf-8') as f:
                        topic_keywords = json.load(f)
                except Exception as e:
                    self.logger.warning(f"ç„¡æ³•å¾ {topics_path} åŠ è¼‰ä¸»é¡Œé—œéµè©: {str(e)}")
        
        if topic_keywords is None:
            # ä½¿ç”¨é è¨­é—œéµè©
            topic_keywords = {
                'positive': ['good', 'great', 'excellent', 'amazing', 'wonderful', 'å¥½', 'æ£’', 'å„ªç§€'],
                'negative': ['bad', 'terrible', 'awful', 'horrible', 'worst', 'å£', 'ç³Ÿç³•', 'å¯æ€•'],
                'neutral': ['okay', 'average', 'normal', 'fine', 'é‚„å¯ä»¥', 'æ™®é€š', 'ä¸€èˆ¬']
            }
            self.logger.info("ä½¿ç”¨é è¨­é—œéµè©é€²è¡Œæ³¨æ„åŠ›è¨ˆç®—")
        
        # ç²å–æ‰€æœ‰ä¸»é¡Œ
        if 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            topics = list(topic_keywords.keys())
            topic_column = 'sentiment'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        # ç²å–åŸå§‹æ–‡æœ¬
        text_column = None
        for col in ['processed_text', 'clean_text', 'text', 'review']:
            if col in metadata.columns:
                text_column = col
                break
        
        if text_column is None:
            self.logger.warning("æœªæ‰¾åˆ°æ–‡æœ¬æ¬„ä½ï¼Œå°‡ä½¿ç”¨ç›¸ä¼¼åº¦æ³¨æ„åŠ›")
            similarity_attn = SimilarityAttention(self.config)
            return similarity_attn.compute_attention(embeddings, metadata, **kwargs)
        
        texts = metadata[text_column].fillna('').tolist()
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                doc_indices = metadata.index[metadata[topic_column] == topic].tolist()
            else:
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            if not doc_indices:
                continue
            
            # ç²å–è©²ä¸»é¡Œçš„é—œéµè©
            keywords = topic_keywords.get(topic, [])
            if not keywords:
                # å¦‚æœæ²’æœ‰é—œéµè©ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
                weights[topic_idx][doc_indices] = 1.0 / len(doc_indices)
                continue
            
            # è¨ˆç®—é—œéµè©åˆ†æ•¸
            keyword_scores = []
            
            for idx in doc_indices:
                text = texts[idx].lower()
                score = 0
                
                # è¨ˆç®—é—œéµè©å‡ºç¾æ¬¡æ•¸çš„åŠ æ¬Šå’Œ
                for i, keyword in enumerate(keywords):
                    # é—œéµè©æ¬Šé‡éš¨æ’åéæ¸›
                    keyword_weight = 1.0 / (i + 1)
                    count = text.count(keyword.lower())
                    score += count * keyword_weight
                
                keyword_scores.append(score)
            
            # å¦‚æœæ‰€æœ‰åˆ†æ•¸ç‚ºé›¶ï¼Œä½¿ç”¨å¹³å‡æ¬Šé‡
            if sum(keyword_scores) == 0:
                weights[topic_idx][doc_indices] = 1.0 / len(doc_indices)
            else:
                # æ­£è¦åŒ–åˆ†æ•¸
                keyword_scores = np.array(keyword_scores)
                topic_weights = keyword_scores / np.sum(keyword_scores)
                weights[topic_idx][doc_indices] = topic_weights
        
        return weights, topic_indices


class SelfAttention(AttentionMechanism):
    """è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def __init__(self, config=None):
        super().__init__(config)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—è‡ªæ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–æ‰€æœ‰ä¸»é¡Œ
        if 'sentiment' in metadata.columns:
            topics = metadata['sentiment'].unique()
            topic_column = 'sentiment'
        elif 'main_topic' in metadata.columns:
            topics = metadata['main_topic'].unique()
            topic_column = 'main_topic'
        else:
            topics = ['positive', 'negative', 'neutral']
            topic_column = 'sentiment'
        
        # å‰µå»ºæ¬Šé‡çŸ©é™£
        weights = np.zeros((len(topics), len(embeddings)))
        topic_indices = {}
        
        for topic_idx, topic in enumerate(topics):
            if topic_column in metadata.columns:
                doc_indices = metadata.index[metadata[topic_column] == topic].tolist()
            else:
                doc_indices = list(range(topic_idx * len(embeddings) // len(topics),
                                       (topic_idx + 1) * len(embeddings) // len(topics)))
            
            topic_indices[topic] = doc_indices
            
            if not doc_indices or len(doc_indices) < 2:
                # å¦‚æœæ–‡æª”æ•¸å°‘æ–¼2ï¼Œä½¿ç”¨å‡ç­‰æ¬Šé‡
                if doc_indices:
                    weights[topic_idx][doc_indices] = 1.0 / len(doc_indices)
                continue
                
            # ç²å–è©²ä¸»é¡Œçš„åµŒå…¥å‘é‡
            topic_embeddings = embeddings[doc_indices]
            
            # è½‰æ›ç‚ºå¼µé‡
            topic_embeddings_tensor = torch.tensor(topic_embeddings, dtype=torch.float32).to(self.device)
            
            # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
            # ä½¿ç”¨ç¸®æ”¾é»ç©æ³¨æ„åŠ›
            d_k = topic_embeddings_tensor.size(-1)
            scores = torch.matmul(topic_embeddings_tensor, topic_embeddings_tensor.transpose(-2, -1)) / math.sqrt(d_k)
            
            # ä½¿ç”¨softmaxè½‰æ›ç‚ºæ¬Šé‡
            attn_weights = F.softmax(scores, dim=-1)
            
            # è¨ˆç®—æ¯å€‹æ–‡æª”çš„å¹³å‡æ³¨æ„åŠ›å¾—åˆ†
            avg_weights = torch.mean(attn_weights, dim=-1).cpu().numpy()
            
            # å¡«å……æ¬Šé‡çŸ©é™£
            weights[topic_idx][doc_indices] = avg_weights
        
        return weights, topic_indices


class CombinedAttention(AttentionMechanism):
    """çµ„åˆå‹æ³¨æ„åŠ›æ©Ÿåˆ¶"""
    
    def compute_attention(self, embeddings: np.ndarray, metadata: pd.DataFrame, **kwargs) -> Tuple[np.ndarray, Dict]:
        """è¨ˆç®—çµ„åˆå‹æ³¨æ„åŠ›æ¬Šé‡"""
        # ç²å–çµ„åˆæ¬Šé‡
        weights_dict = kwargs.get('weights', {})
        similarity_weight = weights_dict.get('similarity', 0.33)
        keyword_weight = weights_dict.get('keyword', 0.33)
        self_weight = weights_dict.get('self', 0.34)
        
        # ç¢ºä¿æ¬Šé‡ç¸½å’Œç‚º1
        total = similarity_weight + keyword_weight + self_weight
        if abs(total - 1.0) > 1e-5:
            self.logger.warning(f"æ³¨æ„åŠ›æ¬Šé‡ç¸½å’Œ {total} ä¸ç‚º1ï¼Œå°‡é€²è¡Œæ­¸ä¸€åŒ–")
            similarity_weight /= total
            keyword_weight /= total
            self_weight /= total
        
        # åˆå§‹åŒ–å„ç¨®æ³¨æ„åŠ›æ©Ÿåˆ¶
        similarity_attn = SimilarityAttention(self.config)
        keyword_attn = KeywordGuidedAttention(self.config)
        self_attn = SelfAttention(self.config)
        
        # è¨ˆç®—å„é¡æ³¨æ„åŠ›æ¬Šé‡
        similarity_weights, topic_indices = similarity_attn.compute_attention(embeddings, metadata, **kwargs)
        keyword_weights, _ = keyword_attn.compute_attention(embeddings, metadata, **kwargs)
        self_weights, _ = self_attn.compute_attention(embeddings, metadata, **kwargs)
        
        # è¨ˆç®—åŠ æ¬Šçµ„åˆ
        weights = (
            similarity_weight * similarity_weights +
            keyword_weight * keyword_weights +
            self_weight * self_weights
        )
        
        return weights, topic_indices


def create_attention_mechanism(attention_type: str, config=None) -> AttentionMechanism:
    """å‰µå»ºæŒ‡å®šé¡å‹çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    
    Args:
        attention_type: æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹ï¼Œå¯ä»¥æ˜¯'no', 'similarity', 'keyword', 'self'æˆ–'combined'
        config: é…ç½®åƒæ•¸
        
    Returns:
        AttentionMechanism: æ³¨æ„åŠ›æ©Ÿåˆ¶å¯¦ä¾‹
    """
    attention_type = attention_type.lower()
    
    if attention_type == 'no' or attention_type == 'none':
        return NoAttention(config)
    elif attention_type == 'similarity':
        return SimilarityAttention(config)
    elif attention_type == 'keyword':
        return KeywordGuidedAttention(config)
    elif attention_type == 'self':
        return SelfAttention(config)
    elif attention_type == 'combined':
        return CombinedAttention(config)
    else:
        logger.warning(f"æœªçŸ¥çš„æ³¨æ„åŠ›é¡å‹: {attention_type}ï¼Œä½¿ç”¨ç„¡æ³¨æ„åŠ›æ©Ÿåˆ¶")
        return NoAttention(config)


def apply_attention_mechanism(attention_type: str, embeddings: np.ndarray, metadata: pd.DataFrame, 
                            topics_path: Optional[str] = None, weights: Optional[Dict] = None) -> Dict[str, Any]:
    """æ‡‰ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è¨ˆç®—é¢å‘å‘é‡
    
    Args:
        attention_type: æ³¨æ„åŠ›æ©Ÿåˆ¶é¡å‹
        embeddings: åµŒå…¥å‘é‡
        metadata: å…ƒæ•¸æ“š
        topics_path: ä¸»é¡Œè©æ–‡ä»¶è·¯å¾‘ï¼Œç”¨æ–¼é—œéµè©æ³¨æ„åŠ›
        weights: çµ„åˆæ³¨æ„åŠ›çš„æ¬Šé‡å­—å…¸
        
    Returns:
        dict: åŒ…å«é¢å‘å‘é‡å’Œè©•ä¼°æŒ‡æ¨™çš„å­—å…¸
    """
    # å‰µå»ºé…ç½®
    config = {}
    
    # é¡¯ç¤ºç•¶å‰è™•ç†çš„æ³¨æ„åŠ›æ©Ÿåˆ¶
    logger.info(f"   ğŸ”„ æ­£åœ¨è¨ˆç®— {attention_type} æ³¨æ„åŠ›æ©Ÿåˆ¶...")
    
    # å‰µå»ºæ³¨æ„åŠ›æ©Ÿåˆ¶
    attention_mech = create_attention_mechanism(attention_type, config)
    
    # æº–å‚™é—œéµå­—åƒæ•¸
    kwargs = {}
    if topics_path:
        kwargs['topics_path'] = topics_path
    if weights:
        kwargs['weights'] = weights
    
    # è¨ˆç®—é¢å‘å‘é‡
    logger.info(f"   ğŸ“Š è¨ˆç®—é¢å‘å‘é‡...")
    aspect_vectors, attention_data = attention_mech.compute_aspect_vectors(
        embeddings, metadata, **kwargs
    )
    
    # è©•ä¼°é¢å‘å‘é‡
    logger.info(f"   ğŸ“ è©•ä¼°æŒ‡æ¨™...")
    metrics = attention_mech.evaluate(aspect_vectors, embeddings, metadata)
    
    # é¡¯ç¤ºçµæœ
    logger.info(f"   âœ… {attention_type} æ³¨æ„åŠ›å®Œæˆ - å…§èšåº¦: {metrics['coherence']:.4f}, "
               f"åˆ†é›¢åº¦: {metrics['separation']:.4f}, ç¶œåˆå¾—åˆ†: {metrics['combined_score']:.4f}")
    
    return {
        'aspect_vectors': aspect_vectors,
        'attention_data': attention_data,
        'metrics': metrics,
        'attention_type': attention_type
    }


# æ¸¬è©¦ä»£ç¢¼
if __name__ == "__main__":
    # é…ç½®æ—¥èªŒç´šåˆ¥
    logging.basicConfig(level=logging.INFO)
    
    # æ¸¬è©¦æ³¨æ„åŠ›æ©Ÿåˆ¶
    # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
    np.random.seed(42)
    n_docs = 100
    embed_dim = 768  # ä½¿ç”¨BERTçš„æ¨™æº–ç¶­åº¦
    n_topics = 3
    
    # æ¨¡æ“¬åµŒå…¥å‘é‡
    embeddings = np.random.randn(n_docs, embed_dim)
    
    # æ¨¡æ“¬å…ƒæ•¸æ“š
    sentiments = ['positive', 'negative', 'neutral']
    doc_sentiments = np.random.choice(sentiments, size=n_docs)
    metadata = pd.DataFrame({
        'sentiment': doc_sentiments,
        'text': ['This is document ' + str(i) for i in range(n_docs)]
    })
    
    # æ¸¬è©¦æ‰€æœ‰æ³¨æ„åŠ›æ©Ÿåˆ¶
    attention_types = ['no', 'similarity', 'keyword', 'self', 'combined']
    
    for attn_type in attention_types:
        logger.info(f"æ¸¬è©¦ {attn_type} æ³¨æ„åŠ›æ©Ÿåˆ¶")
        
        # å‰µå»ºæ³¨æ„åŠ›æ©Ÿåˆ¶
        attention_mech = create_attention_mechanism(attn_type)
        
        # æ¸¬è©¦è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        weights, topic_indices = attention_mech.compute_attention(embeddings, metadata)
        
        # æ¸¬è©¦è¨ˆç®—é¢å‘å‘é‡
        aspect_vectors, _ = attention_mech.compute_aspect_vectors(embeddings, metadata)
        
        # æ¸¬è©¦è©•ä¼°æŒ‡æ¨™
        metrics = attention_mech.evaluate(aspect_vectors, embeddings, metadata)
        
        logger.info(f"å…§èšåº¦: {metrics['coherence']:.4f}")
        logger.info(f"åˆ†é›¢åº¦: {metrics['separation']:.4f}")
        logger.info(f"ç¶œåˆå¾—åˆ†: {metrics['combined_score']:.4f}")
        logger.info("---")
    
    logger.info("æ³¨æ„åŠ›æ©Ÿåˆ¶æ¸¬è©¦å®Œæˆ") 